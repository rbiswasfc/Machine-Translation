{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Encoder_Decoder_w_Attention.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SgOORIz0kUlJ","colab_type":"text"},"source":["### Objective\n","Build a machine translation model (Spainish --> English) : LSTM Encoder-Decoder + Attention Mechanism"]},{"cell_type":"code","metadata":{"id":"fqkV84rywSQ-","colab_type":"code","outputId":"fe337264-d727-4afa-ce91-5c78f242fac3","executionInfo":{"status":"ok","timestamp":1563882067670,"user_tz":-480,"elapsed":19745,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# connect to google drive\n","import os\n","import numpy as np\n","\n","# mount google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n5cBcqr7wWs0","colab_type":"code","colab":{}},"source":["root_dir = \"/content/gdrive/My Drive/NLP/MT_ENSP\"\n","os.chdir(root_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UC9vp7Ef3cqf","colab_type":"text"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"X74ct1ElwWxv","colab_type":"code","colab":{}},"source":["# basic packages\n","import sys\n","import os\n","import math\n","import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","\n","from collections import Counter, namedtuple\n","from docopt import docopt\n","from itertools import chain\n","import json\n","from typing import List, Tuple, Dict, Set, Union\n","from docopt import docopt\n","\n","#pytorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.utils\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","\n","\n","\n","#others\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","from tqdm import tqdm\n","from IPython.core.debugger import set_trace"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Galc-JYk3Xxp","colab_type":"text"},"source":["### Utility Functions"]},{"cell_type":"code","metadata":{"id":"vq13M4PjwW0B","colab_type":"code","colab":{}},"source":["# post-padding for source/target sequences\n","\n","def pad_sents(sents, pad_token):\n","    \n","    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n","    @param sents (list[list[str]]): list of sentences, where each sentence\n","                                    is represented as a list of words\n","    @param pad_token (str): padding token\n","    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n","        than the max length sentence are padded out with the pad_token, such that\n","        each sentences in the batch now has equal length.\n","    \"\"\"\n","    sents_padded = []\n","\n","    max_len = max([len(sent) for sent in sents])\n","    for sent in sents:\n","        sent_len = len(sent)\n","        sents_padded.append(sent + (max_len - sent_len) * [pad_token])\n","\n","    return sents_padded"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zrhfN0JsxfQO","colab_type":"code","outputId":"4c9d7150-8f5b-4893-f584-384be2cb590b","executionInfo":{"status":"ok","timestamp":1563882081253,"user_tz":-480,"elapsed":826,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# verify padding\n","sents = [['a','clear','day'],['it','is','not','raining','today']]\n","pad_sents(sents,'<Pad>')"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['a', 'clear', 'day', '<Pad>', '<Pad>'],\n"," ['it', 'is', 'not', 'raining', 'today']]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"bowvCZdQwW2k","colab_type":"code","colab":{}},"source":["# read from corpus: vocab building\n","\n","def read_corpus(file_path, source):\n","    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n","    @param file_path (str): path to file containing corpus\n","    @param source (str): \"tgt\" or \"src\" indicating whether text\n","        is of the source language or target language\n","    \"\"\"\n","    data = []\n","    for line in open(file_path):\n","        sent = line.strip().split(' ')\n","        #sent = line.split(' ')\n","        # only append <s> and </s> to the target sentence\n","        if source == 'tgt':\n","            sent = ['<s>'] + sent + ['</s>']\n","        data.append(sent)\n","\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"to7xNtAQyLlT","colab_type":"code","outputId":"add7a4ca-d3dd-4e43-9f2c-b8af35573e06","executionInfo":{"status":"ok","timestamp":1563882085806,"user_tz":-480,"elapsed":778,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["# verify if read_corpus is working\n","file_path = 'en_es_data/dev.en'\n","data = read_corpus(file_path, 'src')\n","data[1]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['But',\n"," 'this',\n"," 'understates',\n"," 'the',\n"," 'seriousness',\n"," 'of',\n"," 'this',\n"," 'particular',\n"," 'problem',\n"," '',\n"," 'because',\n"," 'it',\n"," \"doesn't\",\n"," 'show',\n"," 'the',\n"," 'thickness',\n"," 'of',\n"," 'the',\n"," 'ice.']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"WGuqqY1xwW5J","colab_type":"code","colab":{}},"source":["# generate batches for taining\n","\n","def batch_iter(data, batch_size, shuffle=False):\n","    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n","    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (int): batch size\n","    @param shuffle (boolean): whether to randomly shuffle the dataset\n","    \"\"\"\n","    batch_num = math.ceil(len(data) / batch_size)\n","    index_array = list(range(len(data)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        examples = [data[idx] for idx in indices]\n","\n","        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n","        src_sents = [e[0] for e in examples]\n","        tgt_sents = [e[1] for e in examples]\n","\n","        yield src_sents, tgt_sents"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H62BeCkswW8E","colab_type":"code","outputId":"b98ce1c1-cde5-4605-b786-cd68b465ed58","executionInfo":{"status":"ok","timestamp":1563882092633,"user_tz":-480,"elapsed":1003,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# check batch_iter\n","es_path = 'en_es_data/dev.es'\n","en_path = 'en_es_data/dev.en'\n","\n","es_data = read_corpus(es_path, source = 'src')\n","en_data = read_corpus(en_path, source = 'tgt')\n","\n","data = list(zip(es_data, en_data))\n","\n","for src, tgt in batch_iter(data[:4],2):\n","  print(src, tgt)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[['El', 'ao', 'pasado', 'proyect', 'estas', 'dos', 'diapositivas', 'para', 'demostrar', 'que', 'la', 'capa', 'de', 'hielo', 'rtico,', 'que', 'durante', 'los', 'ltimos', 'tres', 'millones', 'de', 'aos', 'ha', 'sido', 'del', 'tamao', 'de', 'los', '48', 'estados,', 'se', 'ha', 'reducido', 'en', 'un', '40', 'por', 'ciento.'], ['Pero', 'esto', 'minimiza', 'la', 'seriedad', 'de', 'este', 'problema', 'concreto', 'porque', 'no', 'muestra', 'el', 'grosor', 'del', 'hielo.']] [['<s>', 'Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.', '</s>'], ['<s>', 'But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', \"doesn't\", 'show', 'the', 'thickness', 'of', 'the', 'ice.', '</s>']]\n","[['La', 'capa', 'de', 'hielo', 'rtico', 'es,', 'en', 'cierta', 'forma,', 'el', 'corazn', 'palpitante', 'del', 'sistema', 'climtico', 'global.'], ['Se', 'expande', 'en', 'invierno', 'y', 'se', 'contrae', 'en', 'verano.']] [['<s>', 'The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.', '</s>'], ['<s>', 'It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.', '</s>']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZvQsfbG03TKx","colab_type":"text"},"source":["### Build Vocab"]},{"cell_type":"code","metadata":{"id":"2E2oAQGXwXBW","colab_type":"code","colab":{}},"source":["class VocabEntry(object):\n","    \"\"\" Vocabulary Entry, i.e. structure containing either\n","    src or tgt language terms.\n","    \"\"\"\n","    def __init__(self, word2id=None):\n","        \"\"\" Init VocabEntry Instance.\n","        @param word2id (dict): dictionary mapping words 2 indices\n","        \"\"\"\n","        if word2id:\n","            self.word2id = word2id\n","        else:\n","            self.word2id = dict()\n","            self.word2id['<pad>'] = 0   # Pad Token\n","            self.word2id['<s>'] = 1 # Start Token\n","            self.word2id['</s>'] = 2    # End Token\n","            self.word2id['<unk>'] = 3   # Unknown Token\n","        self.unk_id = self.word2id['<unk>']\n","        self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","    def __getitem__(self, word):\n","        \"\"\" Retrieve word's index. Return the index for the unk\n","        token if the word is out of vocabulary.\n","        @param word (str): word to look up.\n","        @returns index (int): index of word \n","        \"\"\"\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        \"\"\" Check if word is captured by VocabEntry.\n","        @param word (str): word to look up\n","        @returns contains (bool): whether word is contained    \n","        \"\"\"\n","        return word in self.word2id\n","\n","    def __setitem__(self, key, value):\n","        \"\"\" Raise error, if one tries to edit the VocabEntry.\n","        \"\"\"\n","        raise ValueError('vocabulary is readonly')\n","\n","    def __len__(self):\n","        \"\"\" Compute number of words in VocabEntry.\n","        @returns len (int): number of words in VocabEntry\n","        \"\"\"\n","        return len(self.word2id)\n","\n","    def __repr__(self):\n","        \"\"\" Representation of VocabEntry to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocabulary[size=%d]' % len(self)\n","\n","    def id2word(self, wid):\n","        \"\"\" Return mapping of index to word.\n","        @param wid (int): word index\n","        @returns word (str): word corresponding to index\n","        \"\"\"\n","        return self.id2word[wid]\n","\n","    def add(self, word):\n","        \"\"\" Add word to VocabEntry, if it is previously unseen.\n","        @param word (str): word to add to VocabEntry\n","        @return index (int): index that the word has been assigned\n","        \"\"\"\n","        if word not in self:\n","            wid = self.word2id[word] = len(self)\n","            self.id2word[wid] = word\n","            return wid\n","        else:\n","            return self[word]\n","\n","    def words2indices(self, sents):\n","        \"\"\" Convert list of words or list of sentences of words\n","        into list or list of list of indices.\n","        @param sents (list[str] or list[list[str]]): sentence(s) in words\n","        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n","        \"\"\"\n","        if type(sents[0]) == list:\n","            return [[self[w] for w in s] for s in sents]\n","        else:\n","            return [self[w] for w in sents]\n","\n","    def indices2words(self, word_ids):\n","        \"\"\" Convert list of indices into words.\n","        @param word_ids (list[int]): list of word ids\n","        @return sents (list[str]): list of words\n","        \"\"\"\n","        return [self.id2word[w_id] for w_id in word_ids]\n","\n","    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n","        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n","        shorter sentences.\n","\n","        @param sents (List[List[str]]): list of sentences (words)\n","        @param device: device on which to load the tesnor, i.e. CPU or GPU\n","\n","        @returns sents_var: tensor of (max_sentence_length, batch_size)\n","        \"\"\"\n","        word_ids = self.words2indices(sents)\n","        sents_t = pad_sents(word_ids, self['<pad>'])\n","        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n","        return torch.t(sents_var)\n","\n","    @staticmethod\n","    def from_corpus(corpus, size, freq_cutoff=2):\n","        \"\"\" Given a corpus construct a Vocab Entry.\n","        @param corpus (list[str]): corpus of text produced by read_corpus function\n","        @param size (int): # of words in vocabulary\n","        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n","        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n","        \"\"\"\n","        vocab_entry = VocabEntry()\n","        word_freq = Counter(chain(*corpus))\n","        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n","        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n","              .format(len(word_freq), freq_cutoff, len(valid_words)))\n","        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n","        for word in top_k_words:\n","            vocab_entry.add(word)\n","        return vocab_entry"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRxSESD94TVd","colab_type":"code","outputId":"4cdb2d6d-f51b-4413-c2fa-22aa4ba1219d","executionInfo":{"status":"ok","timestamp":1563882105952,"user_tz":-480,"elapsed":7441,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["# Check Vocab Entry Object\n","lang = VocabEntry()\n","#\n","print('Check if <pad> token is inside vocab:')\n","print('<pad>' in lang) #__contains__#\n","print('Length of vocab:{}'.format(len(lang))) # __len__\n","print('Adding a new word')\n","lang.add('new') # add new entry 'add' method\n","print(lang) #__repr__\n","print('The index of \"new\" is:{}'.format(lang.word2id['new']))\n","## \n","print('Generate a vocab with the from_corpus static method:')\n","en_vocab = VocabEntry.from_corpus(en_data, 100)\n","print('The token for the word \"the\" is:{}'.format(en_vocab.word2id['the']))\n","# \n","print('check to_input_tensor method:')\n","# set device name\n","device = torch.tensor(1).cuda().device\n","\n","temp = en_vocab.to_input_tensor(sents,device)\n","print(temp)\n","# Note: output tensor shape--> (max_len, batch)\n","# len(en_vocab)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Check if <pad> token is inside vocab:\n","True\n","Length of vocab:4\n","Adding a new word\n","Vocabulary[size=5]\n","The index of \"new\" is:4\n","Generate a vocab with the from_corpus static method:\n","number of word types: 3955, number of word types w/ frequency >= 2: 1339\n","The token for the word \"the\" is:5\n","check to_input_tensor method:\n","tensor([[10, 18],\n","        [ 3, 11],\n","        [ 3, 37],\n","        [ 0,  3],\n","        [ 0,  3]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WlZkM2Xm4Nsl","colab_type":"code","colab":{}},"source":["class Vocab(object):\n","    \"\"\" Vocab encapsulating src and target langauges.\n","    \"\"\"\n","    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n","        \"\"\" Init Vocab.\n","        @param src_vocab (VocabEntry): VocabEntry for source language\n","        @param tgt_vocab (VocabEntry): VocabEntry for target language\n","        \"\"\"\n","        self.src = src_vocab\n","        self.tgt = tgt_vocab\n","\n","    @staticmethod\n","    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n","        \"\"\" Build Vocabulary.\n","        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n","        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n","        @param vocab_size (int): Size of vocabulary for both source and target languages\n","        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n","        \"\"\"\n","        assert len(src_sents) == len(tgt_sents)\n","\n","        print('initialize source vocabulary ..')\n","        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n","\n","        print('initialize target vocabulary ..')\n","        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n","\n","        return Vocab(src, tgt)\n","\n","    def save(self, file_path):\n","        \"\"\" Save Vocab to file as JSON dump.\n","        @param file_path (str): file path to vocab file\n","        \"\"\"\n","        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n","\n","    @staticmethod\n","    def load(file_path):\n","        \"\"\" Load vocabulary from JSON dump.\n","        @param file_path (str): file path to vocab file\n","        @returns Vocab object loaded from JSON dump\n","        \"\"\"\n","        entry = json.load(open(file_path, 'r'))\n","        src_word2id = entry['src_word2id']\n","        tgt_word2id = entry['tgt_word2id']\n","\n","        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n","\n","    def __repr__(self):\n","        \"\"\" Representation of Vocab to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFaikyYkAXtT","colab_type":"code","outputId":"f1d32ee1-ce06-4987-d681-86b89b7b795e","executionInfo":{"status":"ok","timestamp":1563882117539,"user_tz":-480,"elapsed":6695,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# Now build source [Spanish] and target [English] vocab\n","\n","train_es = 'en_es_data/train.es'\n","train_en = 'en_es_data/train.en'\n","vocab_file = 'en_es_data/vocab.json'\n","\n","src_sents = read_corpus(train_es, source='src')\n","tgt_sents = read_corpus(train_en, source='tgt')\n","\n","size = 50000\n","freq_cutoff= 2\n","\n","vocab = Vocab.build(src_sents, tgt_sents, size, freq_cutoff)\n","print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n","\n","vocab.save(vocab_file)\n","print('vocabulary saved to %s' % vocab_file)\n","\n","#  \n","print('Note that the <s> and </s> tokens are added while vocab initialization.\\n These tokens are also present in target top frequent words. \\nThat is why vocab size for target language is lesser by 2.')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["initialize source vocabulary ..\n","number of word types: 172418, number of word types w/ frequency >= 2: 80623\n","initialize target vocabulary ..\n","number of word types: 128873, number of word types w/ frequency >= 2: 64215\n","generated vocabulary, source 50004 words, target 50002 words\n","vocabulary saved to en_es_data/vocab.json\n","Note that the <s> and </s> tokens are added while vocab initialization.\n"," These tokens are also present in target top frequent words. \n","That is why vocab size for target language is lesser by 2.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JmW9mJBHlQyy","colab_type":"text"},"source":["### Load Data and EDA"]},{"cell_type":"code","metadata":{"id":"DroJFpmllTt1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":612},"outputId":"b7bb8757-79d1-46e7-a9ab-cca30dad1958","executionInfo":{"status":"ok","timestamp":1563882123475,"user_tz":-480,"elapsed":5403,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["# load data\n","train_es = 'en_es_data/train.es'\n","train_en = 'en_es_data/train.en'\n","\n","dev_es = 'en_es_data/dev.es'\n","dev_en = 'en_es_data/dev.en'\n","\n","test_es = 'en_es_data/test.es'\n","test_en = 'en_es_data/test.en'\n","\n","\n","train_data_src = read_corpus(train_es, source='src')\n","train_data_tgt = read_corpus(train_en, source='tgt')\n","\n","dev_data_src = read_corpus(dev_es, source='src')\n","dev_data_tgt = read_corpus(dev_en, source='tgt')\n","\n","test_data_src = read_corpus(test_es, source='src')\n","test_data_tgt = read_corpus(test_en, source='tgt')\n","\n","train_data = list(zip(train_data_src,train_data_tgt))\n","dev_data = list(zip(dev_data_src,dev_data_tgt))\n","test_data = list(zip(test_data_src,test_data_tgt))\n","\n","#\n","print(\"==\"*40)\n","print(\"Number of examples in train: {}\".format(len(train_data)))\n","print(\"Number of examples in valid: {}\".format(len(dev_data)))\n","print(\"Number of examples in test: {}\".format(len(test_data)))\n","#\n","print(\"==\"*40)\n","print(\"Spanish --> English\")\n","es, en = next(iter(dev_data))\n","print(\"Sp: {}\".format(' '.join(es)))\n","print(\"En: {}\".format(' '.join(en)))\n","print(\"==\"*40)\n","\n","\n","## Build Vocab\n","# Build Vocab with train set\n","\n","size = 50000\n","freq_cutoff= 2\n","vocab_file = 'en_es_data/vocab.json'\n","\n","vocab = Vocab.build(train_data_src, train_data_tgt, size, freq_cutoff)\n","print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n","\n","vocab.save(vocab_file)\n","print('vocabulary saved to %s' % vocab_file)\n","\n","#\n","print(\"==\"*40)\n","print('Note that the <s> and </s> tokens are added while vocab\\\n","      initialization.\\nThese tokens are also present in target\\\n","      top frequent words. \\nThat is why vocab size for target language is lesser by 2.')\n","print(\"==\"*40)\n","\n","\n","# Check tokenization process\n","print(\"==\"*40)\n","sents = [['I', 'asgjsssd', 'will', 'be', 'there', 'for', 'you.'], ['This', 'is', 'spartaaaaaaaa.']]\n","print(\"Tokenize:\\n {} \\n {}\\n\".format(' '.join(sents[0]), ' '.join(sents[1])))\n","\n","print(vocab.tgt.to_input_tensor(sents, \"cpu\"))\n","#\n","print(\"==\"*40)\n","print(\"Note that 3 and 0  are <unk> and <pad> tokens!\")\n","print(\"==\"*40)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["================================================================================\n","Number of examples in train: 216617\n","Number of examples in valid: 851\n","Number of examples in test: 8064\n","================================================================================\n","Spanish --> English\n","Sp: El ao pasado proyect estas dos diapositivas para demostrar que la capa de hielo rtico, que durante los ltimos tres millones de aos ha sido del tamao de los 48 estados, se ha reducido en un 40 por ciento.\n","En: <s> Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent. </s>\n","================================================================================\n","initialize source vocabulary ..\n","number of word types: 172418, number of word types w/ frequency >= 2: 80623\n","initialize target vocabulary ..\n","number of word types: 128873, number of word types w/ frequency >= 2: 64215\n","generated vocabulary, source 50004 words, target 50002 words\n","vocabulary saved to en_es_data/vocab.json\n","================================================================================\n","Note that the <s> and </s> tokens are added while vocab      initialization.\n","These tokens are also present in target      top frequent words. \n","That is why vocab size for target language is lesser by 2.\n","================================================================================\n","================================================================================\n","Tokenize:\n"," I asgjsssd will be there for you. \n"," This is spartaaaaaaaa.\n","\n","tensor([[ 11,  76],\n","        [  3,  12],\n","        [ 88,   3],\n","        [ 29,   0],\n","        [ 67,   0],\n","        [ 19,   0],\n","        [165,   0]])\n","================================================================================\n","Note that 3 and 0  are <unk> and <pad> tokens!\n","================================================================================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ojs5sb5QFg_Q","colab_type":"text"},"source":["### Embedding\n","Initialize embedding matrix for words in source and target languages"]},{"cell_type":"code","metadata":{"id":"PqpaQFz_CTOA","colab_type":"code","colab":{}},"source":["class ModelEmbeddings(nn.Module): \n","    \"\"\"\n","    Class that converts input words to their embeddings.\n","    \"\"\"\n","    def __init__(self, embed_size, vocab):\n","        \"\"\"\n","        Init the Embedding layers.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        \"\"\"\n","        super(ModelEmbeddings, self).__init__()\n","        self.embed_size = embed_size\n","\n","        # default values\n","        self.source = None\n","        self.target = None\n","\n","        src_pad_token_idx = vocab.src['<pad>']\n","        tgt_pad_token_idx = vocab.tgt['<pad>']\n","\n","        \n","        self.source = nn.Embedding(len(vocab.src),embed_size,padding_idx=src_pad_token_idx)\n","        self.target = nn.Embedding(len(vocab.tgt),embed_size,padding_idx=tgt_pad_token_idx)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cPIYL6RxF4-v","colab_type":"text"},"source":["## Encoder-Decoder model\n","With scaler dot-product attention mechanism "]},{"cell_type":"code","metadata":{"id":"sJAf3QnCwXGW","colab_type":"code","colab":{}},"source":["## Hypethesis for Beam search to be used later\n","Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n","\n","### Neural Machine Translation model\n","class NMT(nn.Module):\n","    \"\"\" Simple Neural Machine Translation Model:\n","        - Bidrectional LSTM Encoder\n","        - Unidirection LSTM Decoder\n","        - Global Attention Model (Luong, et al. 2015)\n","    \"\"\"\n","    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n","        \"\"\" Init NMT Model.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param hidden_size (int): Hidden Size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        @param dropout_rate (float): Dropout probability, for attention\n","        \"\"\"\n","        super(NMT, self).__init__()\n","        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n","        self.hidden_size = hidden_size\n","        self.dropout_rate = dropout_rate\n","        self.vocab = vocab\n","\n","        # default values\n","        self.encoder = None\n","        self.decoder = None\n","        self.h_projection = None\n","        self.c_projection = None\n","        self.att_projection = None\n","        self.combined_output_projection = None\n","        self.target_vocab_projection = None\n","        self.dropout = None\n","\n","        # different layers        \n","        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n","        self.decoder = nn.LSTMCell(embed_size+hidden_size, hidden_size)\n","        self.h_projection= nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        self.c_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        self.att_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        self.combined_output_projection = nn.Linear(3*hidden_size, hidden_size, bias=False)\n","        self.target_vocab_projection = nn.Linear(hidden_size,len(vocab.tgt),bias=False)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","\n","\n","    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n","        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n","        target sentences under the language models learned by the NMT system.\n","\n","        @param source (List[List[str]]): list of source sentence tokens\n","        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n","\n","        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n","                                    log-likelihood of generating the gold-standard target sentence for\n","                                    each example in the input batch. Here b = batch size.\n","        \"\"\"\n","        # Compute sentence lengths\n","        source_lengths = [len(s) for s in source]\n","\n","        # Convert list of lists into tensors\n","        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n","        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n","\n","        ###     Run the network forward:\n","\n","        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n","        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n","        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n","        #print(combined_outputs.size())\n","        #temp = self.target_vocab_projection(combined_outputs)\n","        #print(temp.size())\n","        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n","\n","        # Zero out, probabilities for which we have nothing in the target text\n","        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n","        \n","        # Compute log probability of generating true target words\n","        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n","        scores = target_gold_words_log_prob.sum(dim=0)\n","        return scores\n","\n","\n","    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n","            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n","\n","        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n","                                        b = batch_size, src_len = maximum source sentence length. Note that \n","                                       these have already been sorted in order of longest to shortest sentence.\n","        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n","        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n","                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n","                                                hidden state and cell.\n","        \"\"\"\n","        enc_hiddens, dec_init_state = None, None\n","\n","        \n","        X = self.model_embeddings.source(source_padded) #(src_len, b, embed_size)\n","        X = pack_padded_sequence(X, source_lengths)\n","        enc_hiddens, (last_hidden,last_cell) = self.encoder(X) #(h0,c0) defaults to zero\n","        enc_hiddens, _ = pad_packed_sequence(enc_hiddens, batch_first=True)\n","        last_hidden = torch.cat((last_hidden[0,:],last_hidden[1,:]),1)\n","        last_cell = torch.cat((last_cell[0,:],last_cell[1,:]),1)\n","        init_decoder_hidden = self.h_projection(last_hidden)\n","        init_decoder_cell = self.c_projection(last_cell)\n","        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n","\n","\n","        return enc_hiddens, dec_init_state\n","\n","\n","    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n","                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Compute combined output vectors for a batch.\n","\n","        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n","                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n","                                     b = batch size, src_len = maximum source sentence length.\n","        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n","        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n","                                       tgt_len = maximum target sentence length, b = batch size. \n","\n","        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n","                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n","        \"\"\"\n","        # Chop of the <END> token for max length sentences.\n","        target_padded = target_padded[:-1]\n","\n","        # Initialize the decoder state (hidden and cell)\n","        dec_state = dec_init_state\n","\n","        # Initialize previous combined output vector o_{t-1} as zero\n","        batch_size = enc_hiddens.size(0)\n","        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n","\n","        # Initialize a list we will use to collect the combined output o_t on each step\n","        combined_outputs = []\n","\n","\n","        enc_hiddens_proj = self.att_projection(enc_hiddens)\n","        Y = self.model_embeddings.target(target_padded) #(tgt_len,b,e)\n","        Y_splited = torch.split(Y,1, dim=0)\n","        tgt_len = target_padded.size(0)\n","        for i in range(tgt_len):\n","            Y_t = Y_splited[i]\n","            Y_t = torch.squeeze(Y_t,dim =0) #(b,e) --> after removal of time dim\n","            Ybar_t = torch.cat((Y_t,o_prev),dim=1) #(b,e+h)\n","            dec_state, o_prev, e_t = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n","            combined_outputs.append(o_prev)\n","        combined_outputs = torch.stack(combined_outputs)\n","        ### END YOUR CODE\n","\n","        return combined_outputs\n","\n","\n","    def step(self, Ybar_t: torch.Tensor,\n","            dec_state: Tuple[torch.Tensor, torch.Tensor],\n","            enc_hiddens: torch.Tensor,\n","            enc_hiddens_proj: torch.Tensor,\n","            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n","        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n","\n","        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n","                                where b = batch size, e = embedding size, h = hidden size.\n","        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n","                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n","        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n","                                    src_len = maximum source length, h = hidden size.\n","        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n","                                    where b = batch size, src_len = maximum source length, h = hidden size.\n","        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n","                                    where b = batch size, src_len is maximum source length. \n","\n","        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n","                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n","        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n","        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n","                                Note: You will not use this outside of this function.\n","                                      We are simply returning this value so that we can sanity check\n","                                      your implementation.\n","        \"\"\"\n","\n","        combined_output = None\n","\n","        dec_state = self.decoder(Ybar_t, dec_state)\n","        dec_hidden, dec_cell = dec_state\n","        aug_dec_hidden = torch.unsqueeze(dec_hidden, dim=2) #(b,hidden_size,1)\n"," \n","        e_t = torch.bmm(enc_hiddens_proj,aug_dec_hidden) # (b,max_len, hidden_size) * (b,hidden_size,1) --> (b, max_len, 1)\n","        e_t = torch.squeeze(e_t, dim=2) #(b, max_len)\n","\n","        ### END YOUR CODE\n","\n","        # Set e_t to -inf where enc_masks has 1\n","        if enc_masks is not None:\n","            e_t.data.masked_fill_(enc_masks.byte(), -float('inf'))\n","\n","        alpha_t = F.softmax(e_t, dim=1) #(b, src_len)\n","        # enc_hiddens --> (b, src_len, hidden_size*2) #\n","        aug_att = torch.unsqueeze(alpha_t,2) #(b, src_len, 1)\n","        tr_hiddens = enc_hiddens.transpose(1,2) #(b,hidden_size*2, src_len)\n","        a_t = torch.bmm(tr_hiddens,aug_att) #(b,2*hidden_size,1)\n","        a_t = torch.squeeze(a_t,dim=2) #(b,2*hidden_size)\n","        #print(a_t.size(),dec_hidden.size())\n","        \n","        U_t = torch.cat((a_t,dec_hidden), dim=1) #(b,3*hidden_size)\n","        V_t = self.combined_output_projection(U_t) #(b,hidden_size)\n","        O_t = self.dropout(torch.tanh(V_t))\n","        \n","\n","        combined_output = O_t\n","        return dec_state, combined_output, e_t\n","\n","    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n","        \"\"\" Generate sentence masks for encoder hidden states.\n","\n","        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n","                                     src_len = max source length, h = hidden size. \n","        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n","        \n","        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n","                                    where src_len = max source length, h = hidden size.\n","        \"\"\"\n","        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n","        for e_id, src_len in enumerate(source_lengths):\n","            enc_masks[e_id, src_len:] = 1\n","        return enc_masks.to(self.device)\n","\n","\n","    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n","        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n","        @param src_sent (List[str]): a single source sentence (words)\n","        @param beam_size (int): beam size\n","        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n","        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n","                value: List[str]: the decoded target sentence, represented as a list of words\n","                score: float: the log-likelihood of the target sentence\n","        \"\"\"\n","        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n","\n","        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n","        src_encodings_att_linear = self.att_projection(src_encodings)\n","\n","        h_tm1 = dec_init_vec\n","        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n","\n","        eos_id = self.vocab.tgt['</s>']\n","\n","        hypotheses = [['<s>']]\n","        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n","        completed_hypotheses = []\n","\n","        t = 0\n","        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n","            t += 1\n","            hyp_num = len(hypotheses)\n","\n","            exp_src_encodings = src_encodings.expand(hyp_num,\n","                                                     src_encodings.size(1),\n","                                                     src_encodings.size(2))\n","\n","            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n","                                                                           src_encodings_att_linear.size(1),\n","                                                                           src_encodings_att_linear.size(2))\n","\n","            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n","            y_t_embed = self.model_embeddings.target(y_tm1)\n","\n","            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n","\n","            (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n","                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n","\n","            # log probabilities over target words\n","            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n","\n","            live_hyp_num = beam_size - len(completed_hypotheses)\n","            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n","            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n","\n","            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n","            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n","\n","            new_hypotheses = []\n","            live_hyp_ids = []\n","            new_hyp_scores = []\n","\n","            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n","                prev_hyp_id = prev_hyp_id.item()\n","                hyp_word_id = hyp_word_id.item()\n","                cand_new_hyp_score = cand_new_hyp_score.item()\n","\n","                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n","                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n","                if hyp_word == '</s>':\n","                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n","                                                           score=cand_new_hyp_score))\n","                else:\n","                    new_hypotheses.append(new_hyp_sent)\n","                    live_hyp_ids.append(prev_hyp_id)\n","                    new_hyp_scores.append(cand_new_hyp_score)\n","\n","            if len(completed_hypotheses) == beam_size:\n","                break\n","\n","            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n","            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n","            att_tm1 = att_t[live_hyp_ids]\n","\n","            hypotheses = new_hypotheses\n","            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n","\n","        if len(completed_hypotheses) == 0:\n","            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n","                                                   score=hyp_scores[0].item()))\n","\n","        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n","\n","        return completed_hypotheses\n","\n","    @property\n","    def device(self) -> torch.device:\n","        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","        \"\"\"\n","        return self.model_embeddings.source.weight.device\n","\n","    @staticmethod\n","    def load(model_path: str):\n","        \"\"\" Load the model from a file.\n","        @param model_path (str): path to model\n","        \"\"\"\n","        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","        args = params['args']\n","        model = NMT(vocab=params['vocab'], **args)\n","        model.load_state_dict(params['state_dict'])\n","\n","        return model\n","\n","    def save(self, path: str):\n","        \"\"\" Save the odel to a file.\n","        @param path (str): path to the model\n","        \"\"\"\n","        print('save model parameters to [%s]' % path, file=sys.stderr)\n","\n","        params = {\n","            'args': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n","            'vocab': self.vocab,\n","            'state_dict': self.state_dict()\n","        }\n","\n","        torch.save(params, path)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TAZvZPOtHLD5","colab_type":"text"},"source":["### Build Model"]},{"cell_type":"code","metadata":{"id":"bC1f-TK6msPg","colab_type":"code","colab":{}},"source":["model = NMT(embed_size= 256, hidden_size=256, dropout_rate=0.3, vocab=vocab)\n","## Model in training mode\n","model.train();"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5EN1mb5my1K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"8750794f-fd46-4a7c-e46a-babc5fc633c8","executionInfo":{"status":"ok","timestamp":1563882595092,"user_tz":-480,"elapsed":1536,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["## Parameter Initialization\n","uniform_init = 0.1\n","\n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -uniform_init, uniform_init)\n","        \n","model.apply(init_weights);\n","# Count total parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')\n","\n","# Use Adam Optimizaer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# transfer model to cuda if available\n","device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n","print('use device: %s' % device)\n","model = model.to(device)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["The model has 40,833,024 trainable parameters\n","use device: cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h_LOkZ82nS1Y","colab_type":"text"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"NHjwCeDynwB4","colab_type":"text"},"source":["#### Perplexity (PPL)"]},{"cell_type":"code","metadata":{"id":"4knYOx9bnzXp","colab_type":"code","colab":{}},"source":["## Compute Perplexity to keep track of training\n","\n","def evaluate_ppl(model, dev_data, batch_size=32):\n","    \"\"\" Evaluate perplexity on dev sentences\n","    @param model (NMT): NMT Model\n","    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (batch size)\n","    @returns ppl (perplixty on dev sentences)\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    cum_loss = 0.\n","    cum_tgt_words = 0.\n","\n","    # no_grad() signals backend to throw away all gradients\n","    with torch.no_grad():\n","        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n","            loss = -model(src_sents, tgt_sents).sum()\n","\n","            cum_loss += loss.item()\n","            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","            cum_tgt_words += tgt_word_num_to_predict\n","\n","        ppl = np.exp(cum_loss / cum_tgt_words)\n","\n","    if was_training:\n","        model.train()\n","\n","    return ppl"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42JTj05fn8ih","colab_type":"text"},"source":["#### Model Training function"]},{"cell_type":"code","metadata":{"id":"ymPQ67-Jn_ej","colab_type":"code","colab":{}},"source":["######## Train Model ########\n","\n","model_save_path = 'NMT_LSTM_seq2seq_one_layer'\n","\n","##\n","def train_model(model, optimizer, clip_grad =5.0, max_epoch =30, max_patience = 3, max_trial = 3, lr_decay = 0.5, train_batch_size = 128, log_every = 100, valid_niter = 1000):\n","  \n","  \n","  print('Training begins...')\n","  ## Temp variables\n","  num_trial = 0\n","  train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","  cum_examples = report_examples  = valid_num = 0\n","  hist_valid_scores = []\n","  train_time = begin_time = time.time()\n","  \n","  # put the model in training mode\n","  model.train()\n","  \n","  \n","  # iterate over the epochs\n","  for epoch in range(max_epoch):\n","    for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n","        \n","        train_iter += 1\n","        optimizer.zero_grad()\n","        batch_size = len(src_sents)\n","        \n","        example_losses = -model(src_sents, tgt_sents)\n","        batch_loss = example_losses.sum()\n","        loss = batch_loss/batch_size\n","        loss.backward() # autograd\n","        \n","        # Clip gradient\n","        grad_norn = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n","        optimizer.step() # update parameters\n","        \n","        batch_losses_val = batch_loss.item()\n","        report_loss += batch_losses_val\n","        cum_loss += batch_losses_val\n","        \n","        tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","        report_tgt_words += tgt_words_num_to_predict\n","        cum_tgt_words += tgt_words_num_to_predict\n","        report_examples += batch_size\n","        cum_examples += batch_size\n","        \n","        # print interim report about training\n","        \n","        if train_iter % log_every == 0:\n","            #set_trace()\n","            print('| Epoch %d, Iter %d| Avg Loss = %.2f| Avg. ppl = %.2f| Speed %.2f words/sec| Time %.2f min|' % (epoch+1, train_iter, report_loss / report_examples, math.exp(report_loss / report_tgt_words),\n","                                                                                     report_tgt_words / (time.time() - train_time), (time.time() - begin_time)/60.0))\n","\n","            train_time = time.time()\n","            report_loss = report_tgt_words = report_examples = 0.\n","        \n","        # validation\n","        if train_iter % valid_niter == 0:\n","            \n","            print('| <Train Summary> | Epoch %d, Iter %d| Cum. loss = %.2f| Cum. ppl = %.2f|' % (epoch+1, train_iter, cum_loss / cum_examples, np.exp(cum_loss / cum_tgt_words)))\n","\n","            cum_loss = cum_examples = cum_tgt_words = 0.\n","            valid_num += 1\n","\n","            print('Report on validation set:', file=sys.stderr)\n","\n","            # compute dev. ppl and bleu\n","            dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n","            valid_metric = -dev_ppl\n","\n","            print('Validation:  Dev. ppl = %f' % (dev_ppl), file=sys.stderr)\n","\n","            \n","            # learning rate scheduling\n","            \n","            is_better = (len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores))\n","            hist_valid_scores.append(valid_metric)\n","\n","            if is_better:\n","                patience = 0\n","                print('Save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n","                model.save(model_save_path)\n","\n","                # also save the optimizers' state\n","                torch.save(optimizer.state_dict(), model_save_path + '.optim')\n","                \n","            elif patience < int(max_patience):\n","                patience += 1\n","                print('Hit patience %d' % patience, file=sys.stderr)\n","\n","                if patience == int(max_patience):\n","                    num_trial += 1\n","                    print('Hit #%d trial' % num_trial, file=sys.stderr)\n","                    \n","                    if num_trial == int(max_trial):\n","                        print('early stop!', file=sys.stderr)\n","                        return\n","\n","                    # decay lr, and restore from previously best checkpoint\n","                    lr = optimizer.param_groups[0]['lr'] * float(lr_decay)\n","                    print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n","\n","                    # load model\n","                    params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n","                    model.load_state_dict(params['state_dict'])\n","                    model = model.to(device)\n","\n","                    print('restore parameters of the optimizers', file=sys.stderr)\n","                    optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n","\n","                    # set new lr\n","                    for param_group in optimizer.param_groups:\n","                        param_group['lr'] = lr\n","\n","                    # reset patience\n","                    patience = 0\n","\n","            if epoch +1 == int(max_epoch):\n","                print('Training stopped <-> Reached maximum number of epochs!', file=sys.stderr)\n","                return"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zmiIXZTRoEd6","colab_type":"code","colab":{}},"source":["# mask pad tokens while computing attention\n","vocab_mask = torch.ones(len(vocab.tgt))\n","vocab_mask[vocab.tgt['<pad>']] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1xlXlIxmlZE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6e06498a-e6b4-4927-9f81-d1472828a139","executionInfo":{"status":"ok","timestamp":1563887825476,"user_tz":-480,"elapsed":4850736,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["# train parameters\n","max_epoch =30\n","train_batch_size = 64\n","\n","# train the model\n","train_model(model, optimizer, max_epoch =max_epoch, train_batch_size = train_batch_size)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Training begins...\n","| Epoch 1, Iter 100| Avg Loss = 131.76| Avg. ppl = 1663.93| Speed 6365.70 words/sec| Time 0.30 min|\n","| Epoch 1, Iter 200| Avg Loss = 116.58| Avg. ppl = 719.71| Speed 6294.99 words/sec| Time 0.60 min|\n","| Epoch 1, Iter 300| Avg Loss = 108.04| Avg. ppl = 477.81| Speed 6449.24 words/sec| Time 0.89 min|\n","| Epoch 1, Iter 400| Avg Loss = 105.62| Avg. ppl = 382.42| Speed 6371.60 words/sec| Time 1.18 min|\n","| Epoch 1, Iter 500| Avg Loss = 103.62| Avg. ppl = 333.05| Speed 6440.80 words/sec| Time 1.48 min|\n","| Epoch 1, Iter 600| Avg Loss = 98.07| Avg. ppl = 266.42| Speed 6339.53 words/sec| Time 1.78 min|\n","| Epoch 1, Iter 700| Avg Loss = 94.69| Avg. ppl = 216.89| Speed 6358.89 words/sec| Time 2.07 min|\n","| Epoch 1, Iter 800| Avg Loss = 91.02| Avg. ppl = 179.22| Speed 6270.09 words/sec| Time 2.37 min|\n","| Epoch 1, Iter 900| Avg Loss = 88.90| Avg. ppl = 155.06| Speed 6418.97 words/sec| Time 2.66 min|\n","| Epoch 1, Iter 1000| Avg Loss = 85.37| Avg. ppl = 131.82| Speed 6432.22 words/sec| Time 2.95 min|\n","| <Train Summary> | Epoch 1, Iter 1000| Cum. loss = 102.37| Cum. ppl = 331.13|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 131.942455\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 1, Iter 1100| Avg Loss = 83.70| Avg. ppl = 113.51| Speed 5175.07 words/sec| Time 3.32 min|\n","| Epoch 1, Iter 1200| Avg Loss = 81.31| Avg. ppl = 103.84| Speed 6339.46 words/sec| Time 3.61 min|\n","| Epoch 1, Iter 1300| Avg Loss = 81.51| Avg. ppl = 94.29| Speed 6522.23 words/sec| Time 3.90 min|\n","| Epoch 1, Iter 1400| Avg Loss = 78.87| Avg. ppl = 84.22| Speed 6412.45 words/sec| Time 4.20 min|\n","| Epoch 1, Iter 1500| Avg Loss = 76.81| Avg. ppl = 78.91| Speed 6318.65 words/sec| Time 4.50 min|\n","| Epoch 1, Iter 1600| Avg Loss = 75.55| Avg. ppl = 70.58| Speed 6502.78 words/sec| Time 4.79 min|\n","| Epoch 1, Iter 1700| Avg Loss = 74.88| Avg. ppl = 66.83| Speed 6335.77 words/sec| Time 5.09 min|\n","| Epoch 1, Iter 1800| Avg Loss = 72.58| Avg. ppl = 62.97| Speed 6316.53 words/sec| Time 5.38 min|\n","| Epoch 1, Iter 1900| Avg Loss = 71.52| Avg. ppl = 57.56| Speed 6368.53 words/sec| Time 5.68 min|\n","| Epoch 1, Iter 2000| Avg Loss = 71.52| Avg. ppl = 55.33| Speed 6500.88 words/sec| Time 5.97 min|\n","| <Train Summary> | Epoch 1, Iter 2000| Cum. loss = 76.82| Cum. ppl = 76.63|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 59.153163\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 1, Iter 2100| Avg Loss = 69.56| Avg. ppl = 52.27| Speed 5285.53 words/sec| Time 6.33 min|\n","| Epoch 1, Iter 2200| Avg Loss = 70.29| Avg. ppl = 50.81| Speed 6409.16 words/sec| Time 6.63 min|\n","| Epoch 1, Iter 2300| Avg Loss = 68.46| Avg. ppl = 47.32| Speed 6339.45 words/sec| Time 6.92 min|\n","| Epoch 1, Iter 2400| Avg Loss = 66.56| Avg. ppl = 44.61| Speed 6403.40 words/sec| Time 7.22 min|\n","| Epoch 1, Iter 2500| Avg Loss = 65.47| Avg. ppl = 42.04| Speed 6496.91 words/sec| Time 7.50 min|\n","| Epoch 1, Iter 2600| Avg Loss = 65.48| Avg. ppl = 40.42| Speed 6389.41 words/sec| Time 7.80 min|\n","| Epoch 1, Iter 2700| Avg Loss = 65.47| Avg. ppl = 40.19| Speed 6355.93 words/sec| Time 8.10 min|\n","| Epoch 1, Iter 2800| Avg Loss = 64.94| Avg. ppl = 38.89| Speed 6529.01 words/sec| Time 8.39 min|\n","| Epoch 1, Iter 2900| Avg Loss = 63.58| Avg. ppl = 36.92| Speed 6413.32 words/sec| Time 8.68 min|\n","| Epoch 1, Iter 3000| Avg Loss = 63.06| Avg. ppl = 35.34| Speed 6373.76 words/sec| Time 8.98 min|\n","| <Train Summary> | Epoch 1, Iter 3000| Cum. loss = 66.29| Cum. ppl = 42.55|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 36.348401\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 1, Iter 3100| Avg Loss = 63.21| Avg. ppl = 35.34| Speed 5317.87 words/sec| Time 9.33 min|\n","| Epoch 1, Iter 3200| Avg Loss = 61.40| Avg. ppl = 32.78| Speed 6405.25 words/sec| Time 9.62 min|\n","| Epoch 1, Iter 3300| Avg Loss = 61.50| Avg. ppl = 32.57| Speed 6471.59 words/sec| Time 9.91 min|\n","| Epoch 2, Iter 3400| Avg Loss = 59.83| Avg. ppl = 29.69| Speed 6172.92 words/sec| Time 10.22 min|\n","| Epoch 2, Iter 3500| Avg Loss = 56.77| Avg. ppl = 24.77| Speed 6446.86 words/sec| Time 10.51 min|\n","| Epoch 2, Iter 3600| Avg Loss = 55.88| Avg. ppl = 23.55| Speed 6341.96 words/sec| Time 10.81 min|\n","| Epoch 2, Iter 3700| Avg Loss = 55.82| Avg. ppl = 23.71| Speed 6367.27 words/sec| Time 11.10 min|\n","| Epoch 2, Iter 3800| Avg Loss = 55.73| Avg. ppl = 23.58| Speed 6322.57 words/sec| Time 11.40 min|\n","| Epoch 2, Iter 3900| Avg Loss = 55.22| Avg. ppl = 23.01| Speed 6329.86 words/sec| Time 11.70 min|\n","| Epoch 2, Iter 4000| Avg Loss = 54.88| Avg. ppl = 22.76| Speed 6361.24 words/sec| Time 11.99 min|\n","| <Train Summary> | Epoch 2, Iter 4000| Cum. loss = 58.02| Cum. ppl = 26.81|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 30.443960\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 2, Iter 4100| Avg Loss = 54.36| Avg. ppl = 21.71| Speed 5258.12 words/sec| Time 12.35 min|\n","| Epoch 2, Iter 4200| Avg Loss = 54.69| Avg. ppl = 21.94| Speed 6226.70 words/sec| Time 12.65 min|\n","| Epoch 2, Iter 4300| Avg Loss = 54.72| Avg. ppl = 21.66| Speed 6309.30 words/sec| Time 12.96 min|\n","| Epoch 2, Iter 4400| Avg Loss = 53.64| Avg. ppl = 21.21| Speed 6354.49 words/sec| Time 13.25 min|\n","| Epoch 2, Iter 4500| Avg Loss = 55.03| Avg. ppl = 21.50| Speed 6369.87 words/sec| Time 13.55 min|\n","| Epoch 2, Iter 4600| Avg Loss = 53.57| Avg. ppl = 21.06| Speed 6355.45 words/sec| Time 13.85 min|\n","| Epoch 2, Iter 4700| Avg Loss = 52.57| Avg. ppl = 19.77| Speed 6408.50 words/sec| Time 14.14 min|\n","| Epoch 2, Iter 4800| Avg Loss = 53.16| Avg. ppl = 20.37| Speed 6388.99 words/sec| Time 14.43 min|\n","| Epoch 2, Iter 4900| Avg Loss = 53.51| Avg. ppl = 19.95| Speed 6403.84 words/sec| Time 14.73 min|\n","| Epoch 2, Iter 5000| Avg Loss = 53.17| Avg. ppl = 19.98| Speed 6340.88 words/sec| Time 15.03 min|\n","| <Train Summary> | Epoch 2, Iter 5000| Cum. loss = 53.84| Cum. ppl = 20.90|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 25.710404\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 2, Iter 5100| Avg Loss = 52.10| Avg. ppl = 19.93| Speed 5325.16 words/sec| Time 15.38 min|\n","| Epoch 2, Iter 5200| Avg Loss = 51.89| Avg. ppl = 19.48| Speed 6375.90 words/sec| Time 15.67 min|\n","| Epoch 2, Iter 5300| Avg Loss = 51.48| Avg. ppl = 18.76| Speed 6345.78 words/sec| Time 15.97 min|\n","| Epoch 2, Iter 5400| Avg Loss = 52.58| Avg. ppl = 19.07| Speed 6404.67 words/sec| Time 16.26 min|\n","| Epoch 2, Iter 5500| Avg Loss = 51.87| Avg. ppl = 18.56| Speed 6465.69 words/sec| Time 16.56 min|\n","| Epoch 2, Iter 5600| Avg Loss = 51.64| Avg. ppl = 18.63| Speed 6355.96 words/sec| Time 16.85 min|\n","| Epoch 2, Iter 5700| Avg Loss = 51.22| Avg. ppl = 18.38| Speed 6479.85 words/sec| Time 17.14 min|\n","| Epoch 2, Iter 5800| Avg Loss = 51.11| Avg. ppl = 18.06| Speed 6276.02 words/sec| Time 17.44 min|\n","| Epoch 2, Iter 5900| Avg Loss = 51.10| Avg. ppl = 18.58| Speed 6428.42 words/sec| Time 17.73 min|\n","| Epoch 2, Iter 6000| Avg Loss = 51.31| Avg. ppl = 17.67| Speed 6357.69 words/sec| Time 18.03 min|\n","| <Train Summary> | Epoch 2, Iter 6000| Cum. loss = 51.63| Cum. ppl = 18.70|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 23.047655\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 2, Iter 6100| Avg Loss = 51.35| Avg. ppl = 17.79| Speed 5416.21 words/sec| Time 18.38 min|\n","| Epoch 2, Iter 6200| Avg Loss = 50.67| Avg. ppl = 17.60| Speed 6348.74 words/sec| Time 18.68 min|\n","| Epoch 2, Iter 6300| Avg Loss = 49.99| Avg. ppl = 17.28| Speed 6149.87 words/sec| Time 18.98 min|\n","| Epoch 2, Iter 6400| Avg Loss = 50.59| Avg. ppl = 17.41| Speed 6411.18 words/sec| Time 19.28 min|\n","| Epoch 2, Iter 6500| Avg Loss = 50.64| Avg. ppl = 17.25| Speed 6423.20 words/sec| Time 19.57 min|\n","| Epoch 2, Iter 6600| Avg Loss = 49.98| Avg. ppl = 16.99| Speed 6450.06 words/sec| Time 19.87 min|\n","| Epoch 2, Iter 6700| Avg Loss = 50.15| Avg. ppl = 16.84| Speed 6443.41 words/sec| Time 20.16 min|\n","| Epoch 3, Iter 6800| Avg Loss = 47.50| Avg. ppl = 14.97| Speed 6311.32 words/sec| Time 20.46 min|\n","| Epoch 3, Iter 6900| Avg Loss = 44.60| Avg. ppl = 12.37| Speed 6472.18 words/sec| Time 20.75 min|\n","| Epoch 3, Iter 7000| Avg Loss = 44.89| Avg. ppl = 12.32| Speed 6480.36 words/sec| Time 21.04 min|\n","| <Train Summary> | Epoch 3, Iter 7000| Cum. loss = 49.04| Cum. ppl = 15.94|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 21.919477\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 3, Iter 7100| Avg Loss = 44.14| Avg. ppl = 12.27| Speed 5260.15 words/sec| Time 21.40 min|\n","| Epoch 3, Iter 7200| Avg Loss = 44.80| Avg. ppl = 12.66| Speed 6271.03 words/sec| Time 21.70 min|\n","| Epoch 3, Iter 7300| Avg Loss = 44.87| Avg. ppl = 12.48| Speed 6381.03 words/sec| Time 22.00 min|\n","| Epoch 3, Iter 7400| Avg Loss = 44.73| Avg. ppl = 12.61| Speed 6326.54 words/sec| Time 22.29 min|\n","| Epoch 3, Iter 7500| Avg Loss = 44.64| Avg. ppl = 12.39| Speed 6357.76 words/sec| Time 22.59 min|\n","| Epoch 3, Iter 7600| Avg Loss = 43.87| Avg. ppl = 12.29| Speed 6425.00 words/sec| Time 22.88 min|\n","| Epoch 3, Iter 7700| Avg Loss = 44.76| Avg. ppl = 12.29| Speed 6437.82 words/sec| Time 23.18 min|\n","| Epoch 3, Iter 7800| Avg Loss = 43.90| Avg. ppl = 12.15| Speed 6340.73 words/sec| Time 23.47 min|\n","| Epoch 3, Iter 7900| Avg Loss = 44.21| Avg. ppl = 12.31| Speed 6216.72 words/sec| Time 23.78 min|\n","| Epoch 3, Iter 8000| Avg Loss = 44.11| Avg. ppl = 12.30| Speed 6325.54 words/sec| Time 24.07 min|\n","| <Train Summary> | Epoch 3, Iter 8000| Cum. loss = 44.40| Cum. ppl = 12.37|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 20.620818\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 3, Iter 8100| Avg Loss = 44.29| Avg. ppl = 12.29| Speed 5294.59 words/sec| Time 24.43 min|\n","| Epoch 3, Iter 8200| Avg Loss = 44.66| Avg. ppl = 12.56| Speed 6214.30 words/sec| Time 24.73 min|\n","| Epoch 3, Iter 8300| Avg Loss = 44.39| Avg. ppl = 12.42| Speed 6138.13 words/sec| Time 25.04 min|\n","| Epoch 3, Iter 8400| Avg Loss = 44.57| Avg. ppl = 12.31| Speed 6414.55 words/sec| Time 25.33 min|\n","| Epoch 3, Iter 8500| Avg Loss = 44.71| Avg. ppl = 12.46| Speed 6243.35 words/sec| Time 25.64 min|\n","| Epoch 3, Iter 8600| Avg Loss = 43.48| Avg. ppl = 12.09| Speed 6373.77 words/sec| Time 25.93 min|\n","| Epoch 3, Iter 8700| Avg Loss = 44.29| Avg. ppl = 12.21| Speed 6316.10 words/sec| Time 26.23 min|\n","| Epoch 3, Iter 8800| Avg Loss = 44.57| Avg. ppl = 12.23| Speed 6476.40 words/sec| Time 26.52 min|\n","| Epoch 3, Iter 8900| Avg Loss = 45.04| Avg. ppl = 12.47| Speed 6268.86 words/sec| Time 26.82 min|\n","| Epoch 3, Iter 9000| Avg Loss = 44.10| Avg. ppl = 12.19| Speed 6516.32 words/sec| Time 27.11 min|\n","| <Train Summary> | Epoch 3, Iter 9000| Cum. loss = 44.41| Cum. ppl = 12.32|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 19.123942\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 3, Iter 9100| Avg Loss = 44.30| Avg. ppl = 12.42| Speed 5325.18 words/sec| Time 27.46 min|\n","| Epoch 3, Iter 9200| Avg Loss = 44.09| Avg. ppl = 12.26| Speed 6274.90 words/sec| Time 27.76 min|\n","| Epoch 3, Iter 9300| Avg Loss = 44.49| Avg. ppl = 12.18| Speed 6343.04 words/sec| Time 28.06 min|\n","| Epoch 3, Iter 9400| Avg Loss = 44.35| Avg. ppl = 12.25| Speed 6404.62 words/sec| Time 28.36 min|\n","| Epoch 3, Iter 9500| Avg Loss = 43.80| Avg. ppl = 11.83| Speed 6326.38 words/sec| Time 28.66 min|\n","| Epoch 3, Iter 9600| Avg Loss = 44.39| Avg. ppl = 12.08| Speed 6293.19 words/sec| Time 28.96 min|\n","| Epoch 3, Iter 9700| Avg Loss = 43.82| Avg. ppl = 12.17| Speed 6250.46 words/sec| Time 29.26 min|\n","| Epoch 3, Iter 9800| Avg Loss = 43.96| Avg. ppl = 12.14| Speed 6349.68 words/sec| Time 29.55 min|\n","| Epoch 3, Iter 9900| Avg Loss = 43.39| Avg. ppl = 12.01| Speed 6277.00 words/sec| Time 29.85 min|\n","| Epoch 3, Iter 10000| Avg Loss = 43.49| Avg. ppl = 12.16| Speed 6295.43 words/sec| Time 30.14 min|\n","| <Train Summary> | Epoch 3, Iter 10000| Cum. loss = 44.01| Cum. ppl = 12.15|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 18.360140\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 3, Iter 10100| Avg Loss = 44.49| Avg. ppl = 11.87| Speed 5361.26 words/sec| Time 30.50 min|\n","| Epoch 4, Iter 10200| Avg Loss = 41.46| Avg. ppl = 10.31| Speed 6364.47 words/sec| Time 30.80 min|\n","| Epoch 4, Iter 10300| Avg Loss = 38.02| Avg. ppl = 8.73| Speed 6307.30 words/sec| Time 31.10 min|\n","| Epoch 4, Iter 10400| Avg Loss = 38.94| Avg. ppl = 9.05| Speed 6385.54 words/sec| Time 31.39 min|\n","| Epoch 4, Iter 10500| Avg Loss = 39.17| Avg. ppl = 8.95| Speed 6326.52 words/sec| Time 31.69 min|\n","| Epoch 4, Iter 10600| Avg Loss = 39.03| Avg. ppl = 8.97| Speed 6463.83 words/sec| Time 31.99 min|\n","| Epoch 4, Iter 10700| Avg Loss = 38.83| Avg. ppl = 9.19| Speed 6295.59 words/sec| Time 32.28 min|\n","| Epoch 4, Iter 10800| Avg Loss = 39.22| Avg. ppl = 9.17| Speed 6378.88 words/sec| Time 32.58 min|\n","| Epoch 4, Iter 10900| Avg Loss = 38.80| Avg. ppl = 9.15| Speed 6369.83 words/sec| Time 32.87 min|\n","| Epoch 4, Iter 11000| Avg Loss = 39.69| Avg. ppl = 9.17| Speed 6393.88 words/sec| Time 33.17 min|\n","| <Train Summary> | Epoch 4, Iter 11000| Cum. loss = 39.77| Cum. ppl = 9.42|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 18.333865\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 4, Iter 11100| Avg Loss = 39.10| Avg. ppl = 9.18| Speed 5290.14 words/sec| Time 33.53 min|\n","| Epoch 4, Iter 11200| Avg Loss = 40.04| Avg. ppl = 9.49| Speed 6360.48 words/sec| Time 33.83 min|\n","| Epoch 4, Iter 11300| Avg Loss = 39.54| Avg. ppl = 9.43| Speed 6355.68 words/sec| Time 34.12 min|\n","| Epoch 4, Iter 11400| Avg Loss = 40.70| Avg. ppl = 9.59| Speed 6418.22 words/sec| Time 34.42 min|\n","| Epoch 4, Iter 11500| Avg Loss = 39.72| Avg. ppl = 9.47| Speed 6436.89 words/sec| Time 34.71 min|\n","| Epoch 4, Iter 11600| Avg Loss = 39.53| Avg. ppl = 9.54| Speed 6230.33 words/sec| Time 35.01 min|\n","| Epoch 4, Iter 11700| Avg Loss = 39.68| Avg. ppl = 9.52| Speed 6467.96 words/sec| Time 35.30 min|\n","| Epoch 4, Iter 11800| Avg Loss = 39.89| Avg. ppl = 9.57| Speed 6445.10 words/sec| Time 35.60 min|\n","| Epoch 4, Iter 11900| Avg Loss = 38.83| Avg. ppl = 9.35| Speed 6216.95 words/sec| Time 35.89 min|\n","| Epoch 4, Iter 12000| Avg Loss = 39.65| Avg. ppl = 9.33| Speed 6450.52 words/sec| Time 36.19 min|\n","| <Train Summary> | Epoch 4, Iter 12000| Cum. loss = 39.67| Cum. ppl = 9.45|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 18.013487\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 4, Iter 12100| Avg Loss = 39.68| Avg. ppl = 9.68| Speed 5313.52 words/sec| Time 36.54 min|\n","| Epoch 4, Iter 12200| Avg Loss = 39.54| Avg. ppl = 9.42| Speed 6424.47 words/sec| Time 36.83 min|\n","| Epoch 4, Iter 12300| Avg Loss = 40.37| Avg. ppl = 9.56| Speed 6415.48 words/sec| Time 37.13 min|\n","| Epoch 4, Iter 12400| Avg Loss = 39.63| Avg. ppl = 9.58| Speed 6271.83 words/sec| Time 37.43 min|\n","| Epoch 4, Iter 12500| Avg Loss = 40.32| Avg. ppl = 9.62| Speed 6238.13 words/sec| Time 37.73 min|\n","| Epoch 4, Iter 12600| Avg Loss = 40.07| Avg. ppl = 9.60| Speed 6373.05 words/sec| Time 38.03 min|\n","| Epoch 4, Iter 12700| Avg Loss = 39.99| Avg. ppl = 9.62| Speed 6406.52 words/sec| Time 38.32 min|\n","| Epoch 4, Iter 12800| Avg Loss = 39.35| Avg. ppl = 9.61| Speed 6401.37 words/sec| Time 38.61 min|\n","| Epoch 4, Iter 12900| Avg Loss = 39.62| Avg. ppl = 9.54| Speed 6330.96 words/sec| Time 38.91 min|\n","| Epoch 4, Iter 13000| Avg Loss = 39.85| Avg. ppl = 9.63| Speed 6361.35 words/sec| Time 39.20 min|\n","| <Train Summary> | Epoch 4, Iter 13000| Cum. loss = 39.84| Cum. ppl = 9.58|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 18.120077\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 4, Iter 13100| Avg Loss = 39.70| Avg. ppl = 9.73| Speed 6124.99 words/sec| Time 39.51 min|\n","| Epoch 4, Iter 13200| Avg Loss = 39.66| Avg. ppl = 9.50| Speed 6372.54 words/sec| Time 39.80 min|\n","| Epoch 4, Iter 13300| Avg Loss = 40.78| Avg. ppl = 9.61| Speed 6480.29 words/sec| Time 40.10 min|\n","| Epoch 4, Iter 13400| Avg Loss = 39.97| Avg. ppl = 9.60| Speed 6304.14 words/sec| Time 40.40 min|\n","| Epoch 4, Iter 13500| Avg Loss = 41.20| Avg. ppl = 9.93| Speed 6399.75 words/sec| Time 40.70 min|\n","| Epoch 5, Iter 13600| Avg Loss = 36.36| Avg. ppl = 8.00| Speed 6295.38 words/sec| Time 40.99 min|\n","| Epoch 5, Iter 13700| Avg Loss = 34.61| Avg. ppl = 7.14| Speed 6263.54 words/sec| Time 41.29 min|\n","| Epoch 5, Iter 13800| Avg Loss = 35.15| Avg. ppl = 7.36| Speed 6390.94 words/sec| Time 41.59 min|\n","| Epoch 5, Iter 13900| Avg Loss = 35.18| Avg. ppl = 7.34| Speed 6368.68 words/sec| Time 41.88 min|\n","| Epoch 5, Iter 14000| Avg Loss = 35.80| Avg. ppl = 7.46| Speed 6404.76 words/sec| Time 42.18 min|\n","| <Train Summary> | Epoch 5, Iter 14000| Cum. loss = 37.84| Cum. ppl = 8.50|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.857683\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 5, Iter 14100| Avg Loss = 36.16| Avg. ppl = 7.58| Speed 5407.03 words/sec| Time 42.53 min|\n","| Epoch 5, Iter 14200| Avg Loss = 35.29| Avg. ppl = 7.56| Speed 6251.25 words/sec| Time 42.83 min|\n","| Epoch 5, Iter 14300| Avg Loss = 35.59| Avg. ppl = 7.50| Speed 6415.14 words/sec| Time 43.12 min|\n","| Epoch 5, Iter 14400| Avg Loss = 35.91| Avg. ppl = 7.67| Speed 6315.81 words/sec| Time 43.42 min|\n","| Epoch 5, Iter 14500| Avg Loss = 36.04| Avg. ppl = 7.78| Speed 6338.66 words/sec| Time 43.71 min|\n","| Epoch 5, Iter 14600| Avg Loss = 36.56| Avg. ppl = 7.65| Speed 6528.91 words/sec| Time 44.01 min|\n","| Epoch 5, Iter 14700| Avg Loss = 36.36| Avg. ppl = 7.72| Speed 6474.60 words/sec| Time 44.30 min|\n","| Epoch 5, Iter 14800| Avg Loss = 35.81| Avg. ppl = 7.68| Speed 6389.50 words/sec| Time 44.59 min|\n","| Epoch 5, Iter 14900| Avg Loss = 35.58| Avg. ppl = 7.68| Speed 6297.61 words/sec| Time 44.89 min|\n","| Epoch 5, Iter 15000| Avg Loss = 35.97| Avg. ppl = 7.79| Speed 6377.94 words/sec| Time 45.18 min|\n","| <Train Summary> | Epoch 5, Iter 15000| Cum. loss = 35.93| Cum. ppl = 7.66|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.946360\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 5, Iter 15100| Avg Loss = 36.62| Avg. ppl = 7.96| Speed 6092.25 words/sec| Time 45.49 min|\n","| Epoch 5, Iter 15200| Avg Loss = 36.51| Avg. ppl = 8.04| Speed 6367.73 words/sec| Time 45.79 min|\n","| Epoch 5, Iter 15300| Avg Loss = 36.87| Avg. ppl = 7.93| Speed 6393.76 words/sec| Time 46.08 min|\n","| Epoch 5, Iter 15400| Avg Loss = 36.35| Avg. ppl = 7.91| Speed 6420.52 words/sec| Time 46.37 min|\n","| Epoch 5, Iter 15500| Avg Loss = 36.20| Avg. ppl = 7.88| Speed 6344.45 words/sec| Time 46.67 min|\n","| Epoch 5, Iter 15600| Avg Loss = 36.95| Avg. ppl = 8.06| Speed 6318.29 words/sec| Time 46.97 min|\n","| Epoch 5, Iter 15700| Avg Loss = 36.85| Avg. ppl = 8.05| Speed 6304.39 words/sec| Time 47.27 min|\n","| Epoch 5, Iter 15800| Avg Loss = 36.70| Avg. ppl = 8.02| Speed 6312.49 words/sec| Time 47.57 min|\n","| Epoch 5, Iter 15900| Avg Loss = 37.13| Avg. ppl = 8.00| Speed 6359.80 words/sec| Time 47.86 min|\n","| Epoch 5, Iter 16000| Avg Loss = 37.13| Avg. ppl = 8.13| Speed 6375.43 words/sec| Time 48.16 min|\n","| <Train Summary> | Epoch 5, Iter 16000| Cum. loss = 36.73| Cum. ppl = 8.00|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.396996\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 5, Iter 16100| Avg Loss = 36.89| Avg. ppl = 8.01| Speed 5347.40 words/sec| Time 48.51 min|\n","| Epoch 5, Iter 16200| Avg Loss = 37.12| Avg. ppl = 8.18| Speed 6335.74 words/sec| Time 48.81 min|\n","| Epoch 5, Iter 16300| Avg Loss = 37.28| Avg. ppl = 8.18| Speed 6424.82 words/sec| Time 49.11 min|\n","| Epoch 5, Iter 16400| Avg Loss = 37.81| Avg. ppl = 8.22| Speed 6392.22 words/sec| Time 49.41 min|\n","| Epoch 5, Iter 16500| Avg Loss = 37.24| Avg. ppl = 8.11| Speed 6371.99 words/sec| Time 49.70 min|\n","| Epoch 5, Iter 16600| Avg Loss = 37.15| Avg. ppl = 8.20| Speed 6390.37 words/sec| Time 50.00 min|\n","| Epoch 5, Iter 16700| Avg Loss = 37.57| Avg. ppl = 8.25| Speed 6257.02 words/sec| Time 50.30 min|\n","| Epoch 5, Iter 16800| Avg Loss = 36.78| Avg. ppl = 8.29| Speed 6377.29 words/sec| Time 50.59 min|\n","| Epoch 5, Iter 16900| Avg Loss = 37.39| Avg. ppl = 8.28| Speed 6367.68 words/sec| Time 50.89 min|\n","| Epoch 6, Iter 17000| Avg Loss = 33.47| Avg. ppl = 6.67| Speed 6353.89 words/sec| Time 51.18 min|\n","| <Train Summary> | Epoch 6, Iter 17000| Cum. loss = 36.87| Cum. ppl = 8.03|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.833943\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 6, Iter 17100| Avg Loss = 32.41| Avg. ppl = 6.25| Speed 6161.82 words/sec| Time 51.49 min|\n","| Epoch 6, Iter 17200| Avg Loss = 32.28| Avg. ppl = 6.22| Speed 6352.89 words/sec| Time 51.79 min|\n","| Epoch 6, Iter 17300| Avg Loss = 32.86| Avg. ppl = 6.36| Speed 6492.31 words/sec| Time 52.08 min|\n","| Epoch 6, Iter 17400| Avg Loss = 33.05| Avg. ppl = 6.41| Speed 6358.62 words/sec| Time 52.38 min|\n","| Epoch 6, Iter 17500| Avg Loss = 33.24| Avg. ppl = 6.44| Speed 6344.51 words/sec| Time 52.68 min|\n","| Epoch 6, Iter 17600| Avg Loss = 32.75| Avg. ppl = 6.45| Speed 6326.18 words/sec| Time 52.97 min|\n","| Epoch 6, Iter 17700| Avg Loss = 32.68| Avg. ppl = 6.49| Speed 6176.67 words/sec| Time 53.28 min|\n","| Epoch 6, Iter 17800| Avg Loss = 33.23| Avg. ppl = 6.56| Speed 6470.75 words/sec| Time 53.57 min|\n","| Epoch 6, Iter 17900| Avg Loss = 33.67| Avg. ppl = 6.60| Speed 6324.61 words/sec| Time 53.87 min|\n","| Epoch 6, Iter 18000| Avg Loss = 33.78| Avg. ppl = 6.71| Speed 6411.28 words/sec| Time 54.16 min|\n","| <Train Summary> | Epoch 6, Iter 18000| Cum. loss = 33.00| Cum. ppl = 6.45|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.812225\n","Hit patience 2\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 6, Iter 18100| Avg Loss = 33.47| Avg. ppl = 6.68| Speed 6085.35 words/sec| Time 54.47 min|\n","| Epoch 6, Iter 18200| Avg Loss = 33.71| Avg. ppl = 6.63| Speed 6315.42 words/sec| Time 54.77 min|\n","| Epoch 6, Iter 18300| Avg Loss = 34.25| Avg. ppl = 6.82| Speed 6260.93 words/sec| Time 55.08 min|\n","| Epoch 6, Iter 18400| Avg Loss = 34.47| Avg. ppl = 6.83| Speed 6489.56 words/sec| Time 55.37 min|\n","| Epoch 6, Iter 18500| Avg Loss = 33.94| Avg. ppl = 6.77| Speed 6416.90 words/sec| Time 55.67 min|\n","| Epoch 6, Iter 18600| Avg Loss = 34.01| Avg. ppl = 6.98| Speed 6345.21 words/sec| Time 55.96 min|\n","| Epoch 6, Iter 18700| Avg Loss = 33.74| Avg. ppl = 6.81| Speed 6304.29 words/sec| Time 56.26 min|\n","| Epoch 6, Iter 18800| Avg Loss = 34.52| Avg. ppl = 6.93| Speed 6345.06 words/sec| Time 56.56 min|\n","| Epoch 6, Iter 18900| Avg Loss = 33.98| Avg. ppl = 6.96| Speed 6313.45 words/sec| Time 56.85 min|\n","| Epoch 6, Iter 19000| Avg Loss = 33.55| Avg. ppl = 6.82| Speed 6308.69 words/sec| Time 57.15 min|\n","| <Train Summary> | Epoch 6, Iter 19000| Cum. loss = 33.96| Cum. ppl = 6.82|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.880647\n","Hit patience 3\n","Hit #1 trial\n","load previously best model and decay learning rate to 0.000500\n","restore parameters of the optimizers\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 6, Iter 19100| Avg Loss = 32.94| Avg. ppl = 6.36| Speed 5893.62 words/sec| Time 57.47 min|\n","| Epoch 6, Iter 19200| Avg Loss = 32.01| Avg. ppl = 6.25| Speed 6347.00 words/sec| Time 57.77 min|\n","| Epoch 6, Iter 19300| Avg Loss = 31.91| Avg. ppl = 6.20| Speed 6293.20 words/sec| Time 58.06 min|\n","| Epoch 6, Iter 19400| Avg Loss = 32.38| Avg. ppl = 6.24| Speed 6317.56 words/sec| Time 58.36 min|\n","| Epoch 6, Iter 19500| Avg Loss = 32.50| Avg. ppl = 6.37| Speed 6334.93 words/sec| Time 58.66 min|\n","| Epoch 6, Iter 19600| Avg Loss = 32.19| Avg. ppl = 6.24| Speed 6270.38 words/sec| Time 58.96 min|\n","| Epoch 6, Iter 19700| Avg Loss = 32.78| Avg. ppl = 6.29| Speed 6368.50 words/sec| Time 59.25 min|\n","| Epoch 6, Iter 19800| Avg Loss = 32.21| Avg. ppl = 6.15| Speed 6366.16 words/sec| Time 59.55 min|\n","| Epoch 6, Iter 19900| Avg Loss = 32.01| Avg. ppl = 6.18| Speed 6388.01 words/sec| Time 59.84 min|\n","| Epoch 6, Iter 20000| Avg Loss = 32.57| Avg. ppl = 6.32| Speed 6447.76 words/sec| Time 60.14 min|\n","| <Train Summary> | Epoch 6, Iter 20000| Cum. loss = 32.35| Cum. ppl = 6.26|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.379159\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 6, Iter 20100| Avg Loss = 32.29| Avg. ppl = 6.24| Speed 5207.45 words/sec| Time 60.50 min|\n","| Epoch 6, Iter 20200| Avg Loss = 32.45| Avg. ppl = 6.27| Speed 6464.47 words/sec| Time 60.79 min|\n","| Epoch 6, Iter 20300| Avg Loss = 32.15| Avg. ppl = 6.22| Speed 6441.13 words/sec| Time 61.08 min|\n","| Epoch 7, Iter 20400| Avg Loss = 30.99| Avg. ppl = 5.71| Speed 6287.20 words/sec| Time 61.38 min|\n","| Epoch 7, Iter 20500| Avg Loss = 30.91| Avg. ppl = 5.81| Speed 6423.14 words/sec| Time 61.67 min|\n","| Epoch 7, Iter 20600| Avg Loss = 30.89| Avg. ppl = 5.70| Speed 6476.13 words/sec| Time 61.97 min|\n","| Epoch 7, Iter 20700| Avg Loss = 31.14| Avg. ppl = 5.82| Speed 6383.91 words/sec| Time 62.26 min|\n","| Epoch 7, Iter 20800| Avg Loss = 31.02| Avg. ppl = 5.86| Speed 6406.89 words/sec| Time 62.55 min|\n","| Epoch 7, Iter 20900| Avg Loss = 31.91| Avg. ppl = 5.99| Speed 6420.00 words/sec| Time 62.85 min|\n","| Epoch 7, Iter 21000| Avg Loss = 31.27| Avg. ppl = 5.92| Speed 6303.12 words/sec| Time 63.15 min|\n","| <Train Summary> | Epoch 7, Iter 21000| Cum. loss = 31.50| Cum. ppl = 5.95|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.775141\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 7, Iter 21100| Avg Loss = 31.80| Avg. ppl = 5.99| Speed 6068.86 words/sec| Time 63.46 min|\n","| Epoch 7, Iter 21200| Avg Loss = 31.16| Avg. ppl = 5.88| Speed 6310.27 words/sec| Time 63.76 min|\n","| Epoch 7, Iter 21300| Avg Loss = 31.64| Avg. ppl = 5.89| Speed 6369.22 words/sec| Time 64.06 min|\n","| Epoch 7, Iter 21400| Avg Loss = 31.12| Avg. ppl = 5.86| Speed 6319.26 words/sec| Time 64.35 min|\n","| Epoch 7, Iter 21500| Avg Loss = 31.36| Avg. ppl = 5.87| Speed 6320.21 words/sec| Time 64.65 min|\n","| Epoch 7, Iter 21600| Avg Loss = 31.57| Avg. ppl = 6.04| Speed 6298.78 words/sec| Time 64.95 min|\n","| Epoch 7, Iter 21700| Avg Loss = 31.90| Avg. ppl = 6.08| Speed 6493.07 words/sec| Time 65.24 min|\n","| Epoch 7, Iter 21800| Avg Loss = 32.01| Avg. ppl = 6.07| Speed 6341.20 words/sec| Time 65.54 min|\n","| Epoch 7, Iter 21900| Avg Loss = 31.72| Avg. ppl = 6.00| Speed 6341.76 words/sec| Time 65.84 min|\n","| Epoch 7, Iter 22000| Avg Loss = 32.08| Avg. ppl = 6.05| Speed 6451.92 words/sec| Time 66.13 min|\n","| <Train Summary> | Epoch 7, Iter 22000| Cum. loss = 31.64| Cum. ppl = 5.97|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.731136\n","Hit patience 2\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 7, Iter 22100| Avg Loss = 31.53| Avg. ppl = 5.96| Speed 6159.05 words/sec| Time 66.44 min|\n","| Epoch 7, Iter 22200| Avg Loss = 31.94| Avg. ppl = 6.18| Speed 6332.37 words/sec| Time 66.73 min|\n","| Epoch 7, Iter 22300| Avg Loss = 31.89| Avg. ppl = 6.07| Speed 6311.93 words/sec| Time 67.03 min|\n","| Epoch 7, Iter 22400| Avg Loss = 32.04| Avg. ppl = 6.14| Speed 6389.47 words/sec| Time 67.32 min|\n","| Epoch 7, Iter 22500| Avg Loss = 31.95| Avg. ppl = 6.17| Speed 6365.63 words/sec| Time 67.62 min|\n","| Epoch 7, Iter 22600| Avg Loss = 31.94| Avg. ppl = 6.16| Speed 6230.21 words/sec| Time 67.92 min|\n","| Epoch 7, Iter 22700| Avg Loss = 32.07| Avg. ppl = 6.10| Speed 6386.30 words/sec| Time 68.22 min|\n","| Epoch 7, Iter 22800| Avg Loss = 32.11| Avg. ppl = 6.14| Speed 6434.30 words/sec| Time 68.51 min|\n","| Epoch 7, Iter 22900| Avg Loss = 32.35| Avg. ppl = 6.18| Speed 6494.78 words/sec| Time 68.80 min|\n","| Epoch 7, Iter 23000| Avg Loss = 31.60| Avg. ppl = 6.11| Speed 6253.05 words/sec| Time 69.10 min|\n","| <Train Summary> | Epoch 7, Iter 23000| Cum. loss = 31.94| Cum. ppl = 6.12|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.158697\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 7, Iter 23100| Avg Loss = 32.43| Avg. ppl = 6.20| Speed 5199.81 words/sec| Time 69.46 min|\n","| Epoch 7, Iter 23200| Avg Loss = 32.03| Avg. ppl = 6.12| Speed 6246.19 words/sec| Time 69.77 min|\n","| Epoch 7, Iter 23300| Avg Loss = 32.85| Avg. ppl = 6.35| Speed 6381.77 words/sec| Time 70.06 min|\n","| Epoch 7, Iter 23400| Avg Loss = 31.80| Avg. ppl = 6.16| Speed 6342.50 words/sec| Time 70.36 min|\n","| Epoch 7, Iter 23500| Avg Loss = 32.42| Avg. ppl = 6.27| Speed 6379.11 words/sec| Time 70.65 min|\n","| Epoch 7, Iter 23600| Avg Loss = 32.21| Avg. ppl = 6.21| Speed 6377.29 words/sec| Time 70.95 min|\n","| Epoch 8, Iter 23700| Avg Loss = 32.16| Avg. ppl = 6.14| Speed 6387.10 words/sec| Time 71.24 min|\n","| Epoch 8, Iter 23800| Avg Loss = 28.36| Avg. ppl = 4.97| Speed 6127.04 words/sec| Time 71.55 min|\n","| Epoch 8, Iter 23900| Avg Loss = 28.81| Avg. ppl = 5.06| Speed 6394.88 words/sec| Time 71.85 min|\n","| Epoch 8, Iter 24000| Avg Loss = 28.85| Avg. ppl = 5.12| Speed 6350.42 words/sec| Time 72.14 min|\n","| <Train Summary> | Epoch 8, Iter 24000| Cum. loss = 31.19| Cum. ppl = 5.83|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.783496\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 8, Iter 24100| Avg Loss = 28.67| Avg. ppl = 5.10| Speed 6124.08 words/sec| Time 72.45 min|\n","| Epoch 8, Iter 24200| Avg Loss = 29.22| Avg. ppl = 5.22| Speed 6284.25 words/sec| Time 72.75 min|\n","| Epoch 8, Iter 24300| Avg Loss = 29.35| Avg. ppl = 5.30| Speed 6328.17 words/sec| Time 73.05 min|\n","| Epoch 8, Iter 24400| Avg Loss = 29.26| Avg. ppl = 5.24| Speed 6312.44 words/sec| Time 73.34 min|\n","| Epoch 8, Iter 24500| Avg Loss = 29.13| Avg. ppl = 5.22| Speed 6354.35 words/sec| Time 73.64 min|\n","| Epoch 8, Iter 24600| Avg Loss = 29.92| Avg. ppl = 5.37| Speed 6359.65 words/sec| Time 73.94 min|\n","| Epoch 8, Iter 24700| Avg Loss = 29.43| Avg. ppl = 5.30| Speed 6336.34 words/sec| Time 74.24 min|\n","| Epoch 8, Iter 24800| Avg Loss = 29.38| Avg. ppl = 5.24| Speed 6405.39 words/sec| Time 74.53 min|\n","| Epoch 8, Iter 24900| Avg Loss = 29.64| Avg. ppl = 5.39| Speed 6362.59 words/sec| Time 74.83 min|\n","| Epoch 8, Iter 25000| Avg Loss = 30.34| Avg. ppl = 5.48| Speed 6437.89 words/sec| Time 75.12 min|\n","| <Train Summary> | Epoch 8, Iter 25000| Cum. loss = 29.43| Cum. ppl = 5.29|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 18.061083\n","Hit patience 2\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 8, Iter 25100| Avg Loss = 30.26| Avg. ppl = 5.46| Speed 6192.69 words/sec| Time 75.43 min|\n","| Epoch 8, Iter 25200| Avg Loss = 29.69| Avg. ppl = 5.45| Speed 6358.62 words/sec| Time 75.72 min|\n","| Epoch 8, Iter 25300| Avg Loss = 29.48| Avg. ppl = 5.35| Speed 6266.80 words/sec| Time 76.02 min|\n","| Epoch 8, Iter 25400| Avg Loss = 29.89| Avg. ppl = 5.37| Speed 6424.49 words/sec| Time 76.32 min|\n","| Epoch 8, Iter 25500| Avg Loss = 29.47| Avg. ppl = 5.47| Speed 6188.92 words/sec| Time 76.62 min|\n","| Epoch 8, Iter 25600| Avg Loss = 30.06| Avg. ppl = 5.47| Speed 6204.17 words/sec| Time 76.92 min|\n","| Epoch 8, Iter 25700| Avg Loss = 30.40| Avg. ppl = 5.49| Speed 6341.51 words/sec| Time 77.22 min|\n","| Epoch 8, Iter 25800| Avg Loss = 29.93| Avg. ppl = 5.49| Speed 6391.33 words/sec| Time 77.51 min|\n","| Epoch 8, Iter 25900| Avg Loss = 30.05| Avg. ppl = 5.53| Speed 6206.72 words/sec| Time 77.82 min|\n","| Epoch 8, Iter 26000| Avg Loss = 30.44| Avg. ppl = 5.54| Speed 6371.79 words/sec| Time 78.11 min|\n","| <Train Summary> | Epoch 8, Iter 26000| Cum. loss = 29.97| Cum. ppl = 5.46|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 18.005420\n","Hit patience 3\n","Hit #2 trial\n","load previously best model and decay learning rate to 0.000250\n","restore parameters of the optimizers\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 8, Iter 26100| Avg Loss = 29.12| Avg. ppl = 5.16| Speed 5858.17 words/sec| Time 78.44 min|\n","| Epoch 8, Iter 26200| Avg Loss = 29.02| Avg. ppl = 5.14| Speed 6370.67 words/sec| Time 78.73 min|\n","| Epoch 8, Iter 26300| Avg Loss = 28.85| Avg. ppl = 5.15| Speed 6336.25 words/sec| Time 79.03 min|\n","| Epoch 8, Iter 26400| Avg Loss = 28.93| Avg. ppl = 5.17| Speed 6402.48 words/sec| Time 79.32 min|\n","| Epoch 8, Iter 26500| Avg Loss = 28.71| Avg. ppl = 5.11| Speed 6264.21 words/sec| Time 79.62 min|\n","| Epoch 8, Iter 26600| Avg Loss = 28.82| Avg. ppl = 5.12| Speed 6368.22 words/sec| Time 79.92 min|\n","| Epoch 8, Iter 26700| Avg Loss = 29.09| Avg. ppl = 5.17| Speed 6362.36 words/sec| Time 80.22 min|\n","| Epoch 8, Iter 26800| Avg Loss = 28.86| Avg. ppl = 5.20| Speed 6305.41 words/sec| Time 80.51 min|\n","| Epoch 8, Iter 26900| Avg Loss = 29.30| Avg. ppl = 5.21| Speed 6336.68 words/sec| Time 80.81 min|\n","| Epoch 8, Iter 27000| Avg Loss = 29.09| Avg. ppl = 5.16| Speed 6408.95 words/sec| Time 81.11 min|\n","| <Train Summary> | Epoch 8, Iter 27000| Cum. loss = 28.98| Cum. ppl = 5.16|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.412656\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 9, Iter 27100| Avg Loss = 28.90| Avg. ppl = 5.06| Speed 6205.27 words/sec| Time 81.41 min|\n","| Epoch 9, Iter 27200| Avg Loss = 28.26| Avg. ppl = 4.94| Speed 6360.70 words/sec| Time 81.71 min|\n","| Epoch 9, Iter 27300| Avg Loss = 28.71| Avg. ppl = 4.99| Speed 6452.14 words/sec| Time 82.00 min|\n","| Epoch 9, Iter 27400| Avg Loss = 28.48| Avg. ppl = 5.08| Speed 6314.70 words/sec| Time 82.30 min|\n","| Epoch 9, Iter 27500| Avg Loss = 28.93| Avg. ppl = 5.11| Speed 6417.10 words/sec| Time 82.59 min|\n","| Epoch 9, Iter 27600| Avg Loss = 28.71| Avg. ppl = 5.02| Speed 6391.18 words/sec| Time 82.89 min|\n","| Epoch 9, Iter 27700| Avg Loss = 28.49| Avg. ppl = 5.01| Speed 6424.83 words/sec| Time 83.18 min|\n","| Epoch 9, Iter 27800| Avg Loss = 27.98| Avg. ppl = 5.01| Speed 6324.06 words/sec| Time 83.48 min|\n","| Epoch 9, Iter 27900| Avg Loss = 28.63| Avg. ppl = 5.07| Speed 6420.47 words/sec| Time 83.77 min|\n","| Epoch 9, Iter 28000| Avg Loss = 28.73| Avg. ppl = 5.03| Speed 6248.30 words/sec| Time 84.07 min|\n","| <Train Summary> | Epoch 9, Iter 28000| Cum. loss = 28.58| Cum. ppl = 5.03|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.605742\n","Hit patience 2\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 9, Iter 28100| Avg Loss = 28.89| Avg. ppl = 5.09| Speed 6099.48 words/sec| Time 84.38 min|\n","| Epoch 9, Iter 28200| Avg Loss = 28.73| Avg. ppl = 5.12| Speed 6273.67 words/sec| Time 84.68 min|\n","| Epoch 9, Iter 28300| Avg Loss = 28.63| Avg. ppl = 5.06| Speed 6266.13 words/sec| Time 84.98 min|\n","| Epoch 9, Iter 28400| Avg Loss = 28.71| Avg. ppl = 5.07| Speed 6372.52 words/sec| Time 85.28 min|\n","| Epoch 9, Iter 28500| Avg Loss = 28.25| Avg. ppl = 5.02| Speed 6385.47 words/sec| Time 85.57 min|\n","| Epoch 9, Iter 28600| Avg Loss = 28.84| Avg. ppl = 5.21| Speed 6440.26 words/sec| Time 85.86 min|\n","| Epoch 9, Iter 28700| Avg Loss = 29.10| Avg. ppl = 5.12| Speed 6329.76 words/sec| Time 86.16 min|\n","| Epoch 9, Iter 28800| Avg Loss = 28.78| Avg. ppl = 5.13| Speed 6261.38 words/sec| Time 86.46 min|\n","| Epoch 9, Iter 28900| Avg Loss = 28.80| Avg. ppl = 5.16| Speed 6404.84 words/sec| Time 86.75 min|\n","| Epoch 9, Iter 29000| Avg Loss = 29.20| Avg. ppl = 5.17| Speed 6289.36 words/sec| Time 87.05 min|\n","| <Train Summary> | Epoch 9, Iter 29000| Cum. loss = 28.79| Cum. ppl = 5.12|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.636673\n","Hit patience 3\n","Hit #3 trial\n","early stop!\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"M6SoJjGPsher","colab_type":"text"},"source":["### Evaluation"]},{"cell_type":"markdown","metadata":{"id":"ISzFTuLdp_2X","colab_type":"text"},"source":["#### Beam search"]},{"cell_type":"code","metadata":{"id":"LTvAYbLFuqUd","colab_type":"code","colab":{}},"source":["def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n","    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n","    @param model (NMT): NMT Model\n","    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n","    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n","    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n","    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    hypotheses = []\n","    with torch.no_grad():\n","        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n","            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n","\n","            hypotheses.append(example_hyps)\n","\n","    if was_training: model.train(was_training)\n","\n","    return hypotheses"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGJbEn1Ew-S-","colab_type":"code","colab":{}},"source":["def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n","    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n","    @param references (List[List[str]]): a list of gold-standard reference target sentences\n","    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n","    @returns bleu_score: corpus-level BLEU score\n","    \"\"\"\n","    if references[0][0] == '<s>':\n","        references = [ref[1:-1] for ref in references]\n","    bleu_score = corpus_bleu([[ref] for ref in references],\n","                             [hyp.value for hyp in hypotheses])\n","    return bleu_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eW9oOGYXVdIa","colab_type":"code","colab":{}},"source":["def decode():\n","    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n","    If the target gold-standard sentences are given, the function also computes\n","    corpus-level BLEU score.\n","    @param args (Dict): args from cmd line\n","    \"\"\"\n","\n","    print(\"load test source sentences\", file=sys.stderr)\n","    test_data_src = read_corpus(test_es, source='src')\n","    \n","    \n","    print(\"load test target sentences\", file=sys.stderr)\n","    test_data_tgt = read_corpus(test_en, source='tgt')\n","\n","    print(\"load trained model\", file=sys.stderr)\n","    model = NMT.load(model_save_path)\n","\n","    #device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n","    if torch.cuda.device_count()>0:\n","        print(\"Transfer to cuda!!\")\n","        model = model.to(torch.device(\"cuda:0\"))\n","\n","    hypotheses = beam_search(model, test_data_src,\n","                             beam_size=5,\n","                             max_decoding_time_step=70)\n","\n","\n","    top_hypotheses = [hyps[0] for hyps in hypotheses]\n","    bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n","    print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n","\n","    with open('test_output.txt', 'w') as f:\n","        for src_sent, hyps in zip(test_data_src, hypotheses):\n","            top_hyp = hyps[0]\n","            hyp_sent = ' '.join(top_hyp.value)\n","            f.write(hyp_sent + '\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xexMtTi-Vcwg","colab_type":"code","outputId":"650fe0b6-2891-4464-a88f-5e9ff94bd183","executionInfo":{"status":"ok","timestamp":1563888444043,"user_tz":-480,"elapsed":328808,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["decode()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["load test source sentences\n","load test target sentences\n","load trained model\n"],"name":"stderr"},{"output_type":"stream","text":["Transfer to cuda!!\n","Decoding: 100%|██████████| 8064/8064 [05:25<00:00, 24.77it/s]\n"],"name":"stdout"},{"output_type":"stream","text":["Corpus BLEU: 22.35384594841707\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3XnkzmxaAo31","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OJcNc2IXsLed","colab_type":"text"},"source":["### Inference"]},{"cell_type":"code","metadata":{"id":"9ge7ElVDwXeP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"36c9945a-109b-4887-bf61-2fc3b955224d","executionInfo":{"status":"ok","timestamp":1563891957106,"user_tz":-480,"elapsed":1791,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["####\n","src_sent =['También', 'tenemos', 'que', 'tener', 'cuidado', 'con', 'el', 'hielo', 'se', 'resbala', 'fácilmente', 'en', 'él.']\n","en_ref = ['We', 'also', 'have', 'to', 'be', 'careful', 'with', 'the', 'ice,','it', 'slides', 'easily', 'on', 'it.']\n","en_hat = model.beam_search(src_sent,5,70)\n","print(\"==\"*40)\n","print(\"Model Translation:\\n\")\n","print('{}'.format(' '.join(en_hat[0].value)))\n","print(\"\\n\")\n","print(\"Human Reference:\\n\")\n","print('{}'.format(' '.join(en_ref)))\n","print(\"==\"*40)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["================================================================================\n","Model Translation:\n","\n","We have to be careful with the ice <unk> <unk> <unk>\n","\n","\n","Human Reference:\n","\n","We also have to be careful with the ice, it slides easily on it.\n","================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pE6O6avwwXg-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DebG7DtyO8MF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}