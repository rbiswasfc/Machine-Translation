{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Machine_Translation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fqkV84rywSQ-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e5f507b9-3892-4cef-a7b0-ce45c7a72f6c","executionInfo":{"status":"ok","timestamp":1558447484793,"user_tz":-480,"elapsed":3169,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["# connect to google drive\n","import os\n","import numpy as np\n","\n","# mount google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n5cBcqr7wWs0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"4a891cac-e942-4d3f-9449-d6c237a32f92","executionInfo":{"status":"ok","timestamp":1558447492023,"user_tz":-480,"elapsed":3540,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["root_dir = \"/content/gdrive/My Drive/NLP/MT_ENSP\"\n","os.chdir(root_dir)\n","!ls"],"execution_count":6,"outputs":[{"output_type":"stream","text":["collect_submission.sh\t   model_embeddings.py\trun.sh\n","Debugging.ipynb\t\t   NMT_model\t\tsanity_check_en_es_data\n","en_es_data\t\t   NMT_model.optim\tsanity_check.py\n","gpu_requirements.txt\t   nmt_model.py\t\tutils.py\n","__init__.py\t\t   __pycache__\t\tvocab.json\n","local_env.yml\t\t   README.md\t\tvocab.py\n","Machine_Translation.ipynb  run.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UC9vp7Ef3cqf","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"X74ct1ElwWxv","colab_type":"code","colab":{}},"source":["import math\n","import sys\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.utils\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","\n","from collections import Counter, namedtuple\n","from docopt import docopt\n","from itertools import chain\n","import json\n","from typing import List, Tuple, Dict, Set, Union\n","\n","from docopt import docopt\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Galc-JYk3Xxp","colab_type":"text"},"source":["## Utility Functions"]},{"cell_type":"code","metadata":{"id":"vq13M4PjwW0B","colab_type":"code","colab":{}},"source":["# post-padding for source/target sequences\n","\n","def pad_sents(sents, pad_token):\n","    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n","    @param sents (list[list[str]]): list of sentences, where each sentence\n","                                    is represented as a list of words\n","    @param pad_token (str): padding token\n","    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n","        than the max length sentence are padded out with the pad_token, such that\n","        each sentences in the batch now has equal length.\n","    \"\"\"\n","    sents_padded = []\n","\n","    ### YOUR CODE HERE (~6 Lines)\n","    max_len = max([len(sent) for sent in sents])\n","    for sent in sents:\n","        sent_len = len(sent)\n","        sents_padded.append(sent + (max_len - sent_len) * [pad_token])\n","\n","    ### END YOUR CODE\n","\n","    return sents_padded"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zrhfN0JsxfQO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"2283b66e-1b7d-4ba3-8554-25433e76811d","executionInfo":{"status":"ok","timestamp":1558447501051,"user_tz":-480,"elapsed":943,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["# verify padding\n","sents = [['a','clear','day'],['it','is','not','raining','today']]\n","pad_sents(sents,'<Pad>')"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['a', 'clear', 'day', '<Pad>', '<Pad>'],\n"," ['it', 'is', 'not', 'raining', 'today']]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"bowvCZdQwW2k","colab_type":"code","colab":{}},"source":["# read from corpus: vocab building\n","\n","def read_corpus(file_path, source):\n","    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n","    @param file_path (str): path to file containing corpus\n","    @param source (str): \"tgt\" or \"src\" indicating whether text\n","        is of the source language or target language\n","    \"\"\"\n","    data = []\n","    for line in open(file_path):\n","        sent = line.strip().split(' ')\n","        #sent = line.split(' ')\n","        # only append <s> and </s> to the target sentence\n","        if source == 'tgt':\n","            sent = ['<s>'] + sent + ['</s>']\n","        data.append(sent)\n","\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"to7xNtAQyLlT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"04ce34e2-9951-4ea3-a1f0-6cbab0e232ab","executionInfo":{"status":"ok","timestamp":1558447506813,"user_tz":-480,"elapsed":846,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["# verify if read_corpus is working\n","file_path = 'en_es_data/dev.en'\n","data = read_corpus(file_path, 'src')\n","data[1]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['But',\n"," 'this',\n"," 'understates',\n"," 'the',\n"," 'seriousness',\n"," 'of',\n"," 'this',\n"," 'particular',\n"," 'problem',\n"," '',\n"," 'because',\n"," 'it',\n"," \"doesn't\",\n"," 'show',\n"," 'the',\n"," 'thickness',\n"," 'of',\n"," 'the',\n"," 'ice.']"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"WGuqqY1xwW5J","colab_type":"code","colab":{}},"source":["# generate batches for taining\n","\n","def batch_iter(data, batch_size, shuffle=False):\n","    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n","    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (int): batch size\n","    @param shuffle (boolean): whether to randomly shuffle the dataset\n","    \"\"\"\n","    batch_num = math.ceil(len(data) / batch_size)\n","    index_array = list(range(len(data)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        examples = [data[idx] for idx in indices]\n","\n","        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n","        src_sents = [e[0] for e in examples]\n","        tgt_sents = [e[1] for e in examples]\n","\n","        yield src_sents, tgt_sents"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H62BeCkswW8E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"d7ff1a5d-f4bc-40d2-e539-26493d9967a6","executionInfo":{"status":"ok","timestamp":1558447513642,"user_tz":-480,"elapsed":898,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["# check batch_iter\n","es_path = 'en_es_data/dev.es'\n","en_path = 'en_es_data/dev.en'\n","\n","es_data = read_corpus(es_path, source = 'src')\n","en_data = read_corpus(en_path, source = 'tgt')\n","\n","data = list(zip(es_data, en_data))\n","\n","for src, tgt in batch_iter(data[:4],2):\n","  print(src, tgt)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[['El', 'ao', 'pasado', 'proyect', 'estas', 'dos', 'diapositivas', 'para', 'demostrar', 'que', 'la', 'capa', 'de', 'hielo', 'rtico,', 'que', 'durante', 'los', 'ltimos', 'tres', 'millones', 'de', 'aos', 'ha', 'sido', 'del', 'tamao', 'de', 'los', '48', 'estados,', 'se', 'ha', 'reducido', 'en', 'un', '40', 'por', 'ciento.'], ['Pero', 'esto', 'minimiza', 'la', 'seriedad', 'de', 'este', 'problema', 'concreto', 'porque', 'no', 'muestra', 'el', 'grosor', 'del', 'hielo.']] [['<s>', 'Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.', '</s>'], ['<s>', 'But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', \"doesn't\", 'show', 'the', 'thickness', 'of', 'the', 'ice.', '</s>']]\n","[['La', 'capa', 'de', 'hielo', 'rtico', 'es,', 'en', 'cierta', 'forma,', 'el', 'corazn', 'palpitante', 'del', 'sistema', 'climtico', 'global.'], ['Se', 'expande', 'en', 'invierno', 'y', 'se', 'contrae', 'en', 'verano.']] [['<s>', 'The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.', '</s>'], ['<s>', 'It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.', '</s>']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZvQsfbG03TKx","colab_type":"text"},"source":["## Build Vocab"]},{"cell_type":"code","metadata":{"id":"2E2oAQGXwXBW","colab_type":"code","colab":{}},"source":["class VocabEntry(object):\n","    \"\"\" Vocabulary Entry, i.e. structure containing either\n","    src or tgt language terms.\n","    \"\"\"\n","    def __init__(self, word2id=None):\n","        \"\"\" Init VocabEntry Instance.\n","        @param word2id (dict): dictionary mapping words 2 indices\n","        \"\"\"\n","        if word2id:\n","            self.word2id = word2id\n","        else:\n","            self.word2id = dict()\n","            self.word2id['<pad>'] = 0   # Pad Token\n","            self.word2id['<s>'] = 1 # Start Token\n","            self.word2id['</s>'] = 2    # End Token\n","            self.word2id['<unk>'] = 3   # Unknown Token\n","        self.unk_id = self.word2id['<unk>']\n","        self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","    def __getitem__(self, word):\n","        \"\"\" Retrieve word's index. Return the index for the unk\n","        token if the word is out of vocabulary.\n","        @param word (str): word to look up.\n","        @returns index (int): index of word \n","        \"\"\"\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        \"\"\" Check if word is captured by VocabEntry.\n","        @param word (str): word to look up\n","        @returns contains (bool): whether word is contained    \n","        \"\"\"\n","        return word in self.word2id\n","\n","    def __setitem__(self, key, value):\n","        \"\"\" Raise error, if one tries to edit the VocabEntry.\n","        \"\"\"\n","        raise ValueError('vocabulary is readonly')\n","\n","    def __len__(self):\n","        \"\"\" Compute number of words in VocabEntry.\n","        @returns len (int): number of words in VocabEntry\n","        \"\"\"\n","        return len(self.word2id)\n","\n","    def __repr__(self):\n","        \"\"\" Representation of VocabEntry to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocabulary[size=%d]' % len(self)\n","\n","    def id2word(self, wid):\n","        \"\"\" Return mapping of index to word.\n","        @param wid (int): word index\n","        @returns word (str): word corresponding to index\n","        \"\"\"\n","        return self.id2word[wid]\n","\n","    def add(self, word):\n","        \"\"\" Add word to VocabEntry, if it is previously unseen.\n","        @param word (str): word to add to VocabEntry\n","        @return index (int): index that the word has been assigned\n","        \"\"\"\n","        if word not in self:\n","            wid = self.word2id[word] = len(self)\n","            self.id2word[wid] = word\n","            return wid\n","        else:\n","            return self[word]\n","\n","    def words2indices(self, sents):\n","        \"\"\" Convert list of words or list of sentences of words\n","        into list or list of list of indices.\n","        @param sents (list[str] or list[list[str]]): sentence(s) in words\n","        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n","        \"\"\"\n","        if type(sents[0]) == list:\n","            return [[self[w] for w in s] for s in sents]\n","        else:\n","            return [self[w] for w in sents]\n","\n","    def indices2words(self, word_ids):\n","        \"\"\" Convert list of indices into words.\n","        @param word_ids (list[int]): list of word ids\n","        @return sents (list[str]): list of words\n","        \"\"\"\n","        return [self.id2word[w_id] for w_id in word_ids]\n","\n","    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n","        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n","        shorter sentences.\n","\n","        @param sents (List[List[str]]): list of sentences (words)\n","        @param device: device on which to load the tesnor, i.e. CPU or GPU\n","\n","        @returns sents_var: tensor of (max_sentence_length, batch_size)\n","        \"\"\"\n","        word_ids = self.words2indices(sents)\n","        sents_t = pad_sents(word_ids, self['<pad>'])\n","        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n","        return torch.t(sents_var)\n","\n","    @staticmethod\n","    def from_corpus(corpus, size, freq_cutoff=2):\n","        \"\"\" Given a corpus construct a Vocab Entry.\n","        @param corpus (list[str]): corpus of text produced by read_corpus function\n","        @param size (int): # of words in vocabulary\n","        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n","        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n","        \"\"\"\n","        vocab_entry = VocabEntry()\n","        word_freq = Counter(chain(*corpus))\n","        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n","        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n","              .format(len(word_freq), freq_cutoff, len(valid_words)))\n","        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n","        for word in top_k_words:\n","            vocab_entry.add(word)\n","        return vocab_entry"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRxSESD94TVd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"39919195-f849-43a6-c5e5-bca5d82e1544","executionInfo":{"status":"ok","timestamp":1558447525948,"user_tz":-480,"elapsed":5388,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["# Check Vocab Entry Object\n","lang = VocabEntry()\n","#\n","print('Check if <pad> token is inside vocab:')\n","print('<pad>' in lang) #__contains__#\n","print('Length of vocab:{}'.format(len(lang))) # __len__\n","print('Adding a new word')\n","lang.add('new') # add new entry 'add' method\n","print(lang) #__repr__\n","print('The index of \"new\" is:{}'.format(lang.word2id['new']))\n","## \n","print('Generate a vocab with the from_corpus static method:')\n","en_vocab = VocabEntry.from_corpus(en_data, 100)\n","print('The token for the word \"the\" is:{}'.format(en_vocab.word2id['the']))\n","# \n","print('check to_input_tensor method:')\n","# set device name\n","device = torch.tensor(1).cuda().device\n","\n","temp = en_vocab.to_input_tensor(sents,device)\n","print(temp)\n","# Note: output tensor shape--> (max_len, batch)\n","# len(en_vocab)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Check if <pad> token is inside vocab:\n","True\n","Length of vocab:4\n","Adding a new word\n","Vocabulary[size=5]\n","The index of \"new\" is:4\n","Generate a vocab with the from_corpus static method:\n","number of word types: 3955, number of word types w/ frequency >= 2: 1339\n","The token for the word \"the\" is:5\n","check to_input_tensor method:\n","tensor([[10, 18],\n","        [ 3, 11],\n","        [ 3, 37],\n","        [ 0,  3],\n","        [ 0,  3]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WlZkM2Xm4Nsl","colab_type":"code","colab":{}},"source":["class Vocab(object):\n","    \"\"\" Vocab encapsulating src and target langauges.\n","    \"\"\"\n","    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n","        \"\"\" Init Vocab.\n","        @param src_vocab (VocabEntry): VocabEntry for source language\n","        @param tgt_vocab (VocabEntry): VocabEntry for target language\n","        \"\"\"\n","        self.src = src_vocab\n","        self.tgt = tgt_vocab\n","\n","    @staticmethod\n","    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n","        \"\"\" Build Vocabulary.\n","        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n","        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n","        @param vocab_size (int): Size of vocabulary for both source and target languages\n","        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n","        \"\"\"\n","        assert len(src_sents) == len(tgt_sents)\n","\n","        print('initialize source vocabulary ..')\n","        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n","\n","        print('initialize target vocabulary ..')\n","        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n","\n","        return Vocab(src, tgt)\n","\n","    def save(self, file_path):\n","        \"\"\" Save Vocab to file as JSON dump.\n","        @param file_path (str): file path to vocab file\n","        \"\"\"\n","        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n","\n","    @staticmethod\n","    def load(file_path):\n","        \"\"\" Load vocabulary from JSON dump.\n","        @param file_path (str): file path to vocab file\n","        @returns Vocab object loaded from JSON dump\n","        \"\"\"\n","        entry = json.load(open(file_path, 'r'))\n","        src_word2id = entry['src_word2id']\n","        tgt_word2id = entry['tgt_word2id']\n","\n","        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n","\n","    def __repr__(self):\n","        \"\"\" Representation of Vocab to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFaikyYkAXtT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"47fa52c4-4ab8-4801-a78f-717d79ba73a7","executionInfo":{"status":"ok","timestamp":1558447541129,"user_tz":-480,"elapsed":4747,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["# Now build source [Spanish] and target [English] vocab\n","\n","train_es = 'en_es_data/train.es'\n","train_en = 'en_es_data/train.en'\n","vocab_file = 'en_es_data/vocab.json'\n","\n","src_sents = read_corpus(train_es, source='src')\n","tgt_sents = read_corpus(train_en, source='tgt')\n","\n","size = 50000\n","freq_cutoff= 2\n","\n","vocab = Vocab.build(src_sents, tgt_sents, size, freq_cutoff)\n","print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n","\n","vocab.save(vocab_file)\n","print('vocabulary saved to %s' % vocab_file)\n","\n","#  \n","print('Note that the <s> and </s> tokens are added while vocab initialization.\\n These tokens are also present in target top frequent words. \\nThat is why vocab size for target language is lesser by 2.')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["initialize source vocabulary ..\n","number of word types: 172418, number of word types w/ frequency >= 2: 80623\n","initialize target vocabulary ..\n","number of word types: 128873, number of word types w/ frequency >= 2: 64215\n","generated vocabulary, source 50004 words, target 50002 words\n","vocabulary saved to en_es_data/vocab.json\n","Note that the <s> and </s> tokens are added while vocab initialization.\n"," These tokens are also present in target top frequent words. \n","That is why vocab size for target language is lesser by 2.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e-95CL6vFfp9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ojs5sb5QFg_Q","colab_type":"text"},"source":["## Embedding"]},{"cell_type":"code","metadata":{"id":"PqpaQFz_CTOA","colab_type":"code","colab":{}},"source":["class ModelEmbeddings(nn.Module): \n","    \"\"\"\n","    Class that converts input words to their embeddings.\n","    \"\"\"\n","    def __init__(self, embed_size, vocab):\n","        \"\"\"\n","        Init the Embedding layers.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        \"\"\"\n","        super(ModelEmbeddings, self).__init__()\n","        self.embed_size = embed_size\n","\n","        # default values\n","        self.source = None\n","        self.target = None\n","\n","        src_pad_token_idx = vocab.src['<pad>']\n","        tgt_pad_token_idx = vocab.tgt['<pad>']\n","\n","        \n","        self.source = nn.Embedding(len(vocab.src),embed_size,padding_idx=src_pad_token_idx)\n","        self.target = nn.Embedding(len(vocab.tgt),embed_size,padding_idx=tgt_pad_token_idx)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cPIYL6RxF4-v","colab_type":"text"},"source":["## Encoder-Decoder model"]},{"cell_type":"code","metadata":{"id":"O0Dgrx1awXD9","colab_type":"code","colab":{}},"source":["Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJAf3QnCwXGW","colab_type":"code","colab":{}},"source":["class NMT(nn.Module):\n","    \"\"\" Simple Neural Machine Translation Model:\n","        - Bidrectional LSTM Encoder\n","        - Unidirection LSTM Decoder\n","        - Global Attention Model (Luong, et al. 2015)\n","    \"\"\"\n","    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n","        \"\"\" Init NMT Model.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param hidden_size (int): Hidden Size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        @param dropout_rate (float): Dropout probability, for attention\n","        \"\"\"\n","        super(NMT, self).__init__()\n","        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n","        self.hidden_size = hidden_size\n","        self.dropout_rate = dropout_rate\n","        self.vocab = vocab\n","\n","        # default values\n","        self.encoder = None\n","        self.decoder = None\n","        self.h_projection = None\n","        self.c_projection = None\n","        self.att_projection = None\n","        self.combined_output_projection = None\n","        self.target_vocab_projection = None\n","        self.dropout = None\n","\n","        # different layers        \n","        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n","        self.decoder = nn.LSTMCell(embed_size+hidden_size, hidden_size)\n","        self.h_projection= nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        self.c_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        self.att_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        self.combined_output_projection = nn.Linear(3*hidden_size, hidden_size, bias=False)\n","        self.target_vocab_projection = nn.Linear(hidden_size,len(vocab.tgt),bias=False)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","\n","\n","    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n","        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n","        target sentences under the language models learned by the NMT system.\n","\n","        @param source (List[List[str]]): list of source sentence tokens\n","        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n","\n","        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n","                                    log-likelihood of generating the gold-standard target sentence for\n","                                    each example in the input batch. Here b = batch size.\n","        \"\"\"\n","        # Compute sentence lengths\n","        source_lengths = [len(s) for s in source]\n","\n","        # Convert list of lists into tensors\n","        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n","        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n","\n","        ###     Run the network forward:\n","\n","        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n","        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n","        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n","        #print(combined_outputs.size())\n","        #temp = self.target_vocab_projection(combined_outputs)\n","        #print(temp.size())\n","        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n","\n","        # Zero out, probabilities for which we have nothing in the target text\n","        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n","        \n","        # Compute log probability of generating true target words\n","        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n","        scores = target_gold_words_log_prob.sum(dim=0)\n","        return scores\n","\n","\n","    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n","            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n","\n","        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n","                                        b = batch_size, src_len = maximum source sentence length. Note that \n","                                       these have already been sorted in order of longest to shortest sentence.\n","        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n","        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n","                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n","                                                hidden state and cell.\n","        \"\"\"\n","        enc_hiddens, dec_init_state = None, None\n","\n","        \n","        X = self.model_embeddings.source(source_padded) #(src_len, b, embed_size)\n","        X = pack_padded_sequence(X, source_lengths)\n","        enc_hiddens, (last_hidden,last_cell) = self.encoder(X) #(h0,c0) defaults to zero\n","        enc_hiddens, _ = pad_packed_sequence(enc_hiddens, batch_first=True)\n","        last_hidden = torch.cat((last_hidden[0,:],last_hidden[1,:]),1)\n","        last_cell = torch.cat((last_cell[0,:],last_cell[1,:]),1)\n","        init_decoder_hidden = self.h_projection(last_hidden)\n","        init_decoder_cell = self.c_projection(last_cell)\n","        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n","\n","\n","        return enc_hiddens, dec_init_state\n","\n","\n","    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n","                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Compute combined output vectors for a batch.\n","\n","        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n","                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n","                                     b = batch size, src_len = maximum source sentence length.\n","        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n","        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n","                                       tgt_len = maximum target sentence length, b = batch size. \n","\n","        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n","                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n","        \"\"\"\n","        # Chop of the <END> token for max length sentences.\n","        target_padded = target_padded[:-1]\n","\n","        # Initialize the decoder state (hidden and cell)\n","        dec_state = dec_init_state\n","\n","        # Initialize previous combined output vector o_{t-1} as zero\n","        batch_size = enc_hiddens.size(0)\n","        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n","\n","        # Initialize a list we will use to collect the combined output o_t on each step\n","        combined_outputs = []\n","\n","\n","        enc_hiddens_proj = self.att_projection(enc_hiddens)\n","        Y = self.model_embeddings.target(target_padded) #(tgt_len,b,e)\n","        Y_splited = torch.split(Y,1, dim=0)\n","        tgt_len = target_padded.size(0)\n","        for i in range(tgt_len):\n","            Y_t = Y_splited[i]\n","            Y_t = torch.squeeze(Y_t,dim =0) #(b,e) --> after removal of time dim\n","            Ybar_t = torch.cat((Y_t,o_prev),dim=1) #(b,e+h)\n","            dec_state, o_prev, e_t = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n","            combined_outputs.append(o_prev)\n","        combined_outputs = torch.stack(combined_outputs)\n","        ### END YOUR CODE\n","\n","        return combined_outputs\n","\n","\n","    def step(self, Ybar_t: torch.Tensor,\n","            dec_state: Tuple[torch.Tensor, torch.Tensor],\n","            enc_hiddens: torch.Tensor,\n","            enc_hiddens_proj: torch.Tensor,\n","            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n","        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n","\n","        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n","                                where b = batch size, e = embedding size, h = hidden size.\n","        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n","                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n","        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n","                                    src_len = maximum source length, h = hidden size.\n","        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n","                                    where b = batch size, src_len = maximum source length, h = hidden size.\n","        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n","                                    where b = batch size, src_len is maximum source length. \n","\n","        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n","                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n","        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n","        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n","                                Note: You will not use this outside of this function.\n","                                      We are simply returning this value so that we can sanity check\n","                                      your implementation.\n","        \"\"\"\n","\n","        combined_output = None\n","\n","        dec_state = self.decoder(Ybar_t, dec_state)\n","        dec_hidden, dec_cell = dec_state\n","        aug_dec_hidden = torch.unsqueeze(dec_hidden, dim=2) #(b,hidden_size,1)\n"," \n","        e_t = torch.bmm(enc_hiddens_proj,aug_dec_hidden) # (b,max_len, hidden_size) * (b,hidden_size,1) --> (b, max_len, 1)\n","        e_t = torch.squeeze(e_t, dim=2) #(b, max_len)\n","\n","        ### END YOUR CODE\n","\n","        # Set e_t to -inf where enc_masks has 1\n","        if enc_masks is not None:\n","            e_t.data.masked_fill_(enc_masks.byte(), -float('inf'))\n","\n","        alpha_t = F.softmax(e_t, dim=1) #(b, src_len)\n","        # enc_hiddens --> (b, src_len, hidden_size*2) #\n","        aug_att = torch.unsqueeze(alpha_t,2) #(b, src_len, 1)\n","        tr_hiddens = enc_hiddens.transpose(1,2) #(b,hidden_size*2, src_len)\n","        a_t = torch.bmm(tr_hiddens,aug_att) #(b,2*hidden_size,1)\n","        a_t = torch.squeeze(a_t,dim=2) #(b,2*hidden_size)\n","        #print(a_t.size(),dec_hidden.size())\n","        \n","        U_t = torch.cat((a_t,dec_hidden), dim=1) #(b,3*hidden_size)\n","        V_t = self.combined_output_projection(U_t) #(b,hidden_size)\n","        O_t = self.dropout(torch.tanh(V_t))\n","        \n","\n","        combined_output = O_t\n","        return dec_state, combined_output, e_t\n","\n","    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n","        \"\"\" Generate sentence masks for encoder hidden states.\n","\n","        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n","                                     src_len = max source length, h = hidden size. \n","        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n","        \n","        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n","                                    where src_len = max source length, h = hidden size.\n","        \"\"\"\n","        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n","        for e_id, src_len in enumerate(source_lengths):\n","            enc_masks[e_id, src_len:] = 1\n","        return enc_masks.to(self.device)\n","\n","\n","    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n","        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n","        @param src_sent (List[str]): a single source sentence (words)\n","        @param beam_size (int): beam size\n","        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n","        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n","                value: List[str]: the decoded target sentence, represented as a list of words\n","                score: float: the log-likelihood of the target sentence\n","        \"\"\"\n","        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n","\n","        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n","        src_encodings_att_linear = self.att_projection(src_encodings)\n","\n","        h_tm1 = dec_init_vec\n","        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n","\n","        eos_id = self.vocab.tgt['</s>']\n","\n","        hypotheses = [['<s>']]\n","        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n","        completed_hypotheses = []\n","\n","        t = 0\n","        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n","            t += 1\n","            hyp_num = len(hypotheses)\n","\n","            exp_src_encodings = src_encodings.expand(hyp_num,\n","                                                     src_encodings.size(1),\n","                                                     src_encodings.size(2))\n","\n","            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n","                                                                           src_encodings_att_linear.size(1),\n","                                                                           src_encodings_att_linear.size(2))\n","\n","            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n","            y_t_embed = self.model_embeddings.target(y_tm1)\n","\n","            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n","\n","            (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n","                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n","\n","            # log probabilities over target words\n","            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n","\n","            live_hyp_num = beam_size - len(completed_hypotheses)\n","            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n","            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n","\n","            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n","            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n","\n","            new_hypotheses = []\n","            live_hyp_ids = []\n","            new_hyp_scores = []\n","\n","            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n","                prev_hyp_id = prev_hyp_id.item()\n","                hyp_word_id = hyp_word_id.item()\n","                cand_new_hyp_score = cand_new_hyp_score.item()\n","\n","                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n","                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n","                if hyp_word == '</s>':\n","                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n","                                                           score=cand_new_hyp_score))\n","                else:\n","                    new_hypotheses.append(new_hyp_sent)\n","                    live_hyp_ids.append(prev_hyp_id)\n","                    new_hyp_scores.append(cand_new_hyp_score)\n","\n","            if len(completed_hypotheses) == beam_size:\n","                break\n","\n","            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n","            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n","            att_tm1 = att_t[live_hyp_ids]\n","\n","            hypotheses = new_hypotheses\n","            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n","\n","        if len(completed_hypotheses) == 0:\n","            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n","                                                   score=hyp_scores[0].item()))\n","\n","        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n","\n","        return completed_hypotheses\n","\n","    @property\n","    def device(self) -> torch.device:\n","        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","        \"\"\"\n","        return self.model_embeddings.source.weight.device\n","\n","    @staticmethod\n","    def load(model_path: str):\n","        \"\"\" Load the model from a file.\n","        @param model_path (str): path to model\n","        \"\"\"\n","        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","        args = params['args']\n","        model = NMT(vocab=params['vocab'], **args)\n","        model.load_state_dict(params['state_dict'])\n","\n","        return model\n","\n","    def save(self, path: str):\n","        \"\"\" Save the odel to a file.\n","        @param path (str): path to the model\n","        \"\"\"\n","        print('save model parameters to [%s]' % path, file=sys.stderr)\n","\n","        params = {\n","            'args': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n","            'vocab': self.vocab,\n","            'state_dict': self.state_dict()\n","        }\n","\n","        torch.save(params, path)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TAZvZPOtHLD5","colab_type":"text"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"_etPbHxUwXIq","colab_type":"code","colab":{}},"source":["# load data\n","train_es = 'en_es_data/train.es'\n","train_en = 'en_es_data/train.en'\n","\n","dev_es = 'en_es_data/dev.es'\n","dev_en = 'en_es_data/dev.en'\n","\n","test_es = 'en_es_data/test.es'\n","test_en = 'en_es_data/test.en'\n","\n","vocab_file = 'en_es_data/vocab.json'\n","\n","\n","train_data_src = read_corpus(train_es, source='src')\n","train_data_tgt = read_corpus(train_en, source='tgt')\n","\n","dev_data_src = read_corpus(dev_es, source='src')\n","dev_data_tgt = read_corpus(dev_en, source='tgt')\n","\n","test_data_src = read_corpus(test_es, source='src')\n","test_data_tgt = read_corpus(test_en, source='tgt')\n","\n","train_data = list(zip(train_data_src,train_data_tgt))\n","dev_data = list(zip(dev_data_src,dev_data_tgt))\n","test_data = list(zip(test_data_src,test_data_tgt))\n","\n","vocab = Vocab.load(vocab_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktso8XG2wXLC","colab_type":"code","colab":{}},"source":["# We can set these parameters\n","train_batch_size = 128\n","clip_grad = 5.0\n","#valid_niter = int(args['--valid-niter'])\n","valid_niter = 2000\n","\n","log_every = 100\n","model_save_path = 'NMT_model'\n","max_epoch = 30\n","max_patience = 5\n","max_trial = 5\n","lr_decay = 0.5\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmAzDGIgwXNW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"outputId":"a8f00b8d-8f8c-43f3-dc48-9e76ab2fcbe7","executionInfo":{"status":"ok","timestamp":1558422032938,"user_tz":-480,"elapsed":1054,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["model = NMT(embed_size= 256, hidden_size=256, dropout_rate=0.3, vocab=vocab)\n","model.train()"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NMT(\n","  (model_embeddings): ModelEmbeddings(\n","    (source): Embedding(50004, 256, padding_idx=0)\n","    (target): Embedding(50002, 256, padding_idx=0)\n","  )\n","  (encoder): LSTM(256, 256, bidirectional=True)\n","  (decoder): LSTMCell(512, 256)\n","  (h_projection): Linear(in_features=512, out_features=256, bias=False)\n","  (c_projection): Linear(in_features=512, out_features=256, bias=False)\n","  (att_projection): Linear(in_features=512, out_features=256, bias=False)\n","  (combined_output_projection): Linear(in_features=768, out_features=256, bias=False)\n","  (target_vocab_projection): Linear(in_features=256, out_features=50002, bias=False)\n","  (dropout): Dropout(p=0.3)\n",")"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"XRB6pKvcwXPy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f94b144e-c2f1-4ee4-9770-de23adbeab37","executionInfo":{"status":"ok","timestamp":1558422134443,"user_tz":-480,"elapsed":780,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["uniform_init = 0.1\n","if np.abs(uniform_init) > 0.:\n","  print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n","  for p in model.parameters():\n","    p.data.uniform_(-uniform_init, uniform_init)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["uniformly initialize parameters [-0.100000, +0.100000]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hATdhBJowXSD","colab_type":"code","colab":{}},"source":["# mask pad tokens\n","vocab_mask = torch.ones(len(vocab.tgt))\n","vocab_mask[vocab.tgt['<pad>']] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7Ax9a3awXUd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"24660bb7-491e-46d4-d8c4-ee398d309495","executionInfo":{"status":"ok","timestamp":1558422330373,"user_tz":-480,"elapsed":1024,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n","print('use device: %s' % device)\n","model = model.to(device)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["use device: cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9hibQuGaNTgc","colab_type":"code","colab":{}},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFIUK2VLwXXE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e3e8d3b1-1f6e-48dd-ad1a-7802a97b39cf","executionInfo":{"status":"ok","timestamp":1558422452253,"user_tz":-480,"elapsed":761,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["import time\n","num_trial = 0\n","train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","cum_examples = report_examples = epoch = valid_num = 0\n","hist_valid_scores = []\n","train_time = begin_time = time.time()\n","\n","print('Start Maximum Likelihood training:')"],"execution_count":49,"outputs":[{"output_type":"stream","text":["Start Maximum Likelihood training:\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fAiNNGGewXZb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":15997},"outputId":"6616cb0e-1705-44b7-9a7f-c3da8c3cde3f","executionInfo":{"status":"error","timestamp":1558447217974,"user_tz":-480,"elapsed":23753892,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["while True:\n","    epoch += 1\n","\n","    for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n","        train_iter += 1\n","\n","        optimizer.zero_grad()\n","\n","        batch_size = len(src_sents)\n","        #\n","        #print(batch_size)\n","        example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n","        batch_loss = example_losses.sum()\n","        loss = batch_loss / batch_size\n","\n","        loss.backward()\n","\n","        # clip gradient\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n","\n","        optimizer.step()\n","\n","        batch_losses_val = batch_loss.item()\n","        report_loss += batch_losses_val\n","        cum_loss += batch_losses_val\n","\n","        tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","        report_tgt_words += tgt_words_num_to_predict\n","        cum_tgt_words += tgt_words_num_to_predict\n","        report_examples += batch_size\n","        cum_examples += batch_size\n","\n","        if train_iter % log_every == 0:\n","            print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n","                  'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n","                                                                                     report_loss / report_examples,\n","                                                                                     math.exp(report_loss / report_tgt_words),\n","                                                                                     cum_examples,\n","                                                                                     report_tgt_words / (time.time() - train_time),\n","                                                                                     time.time() - begin_time), file=sys.stderr)\n","\n","            train_time = time.time()\n","            report_loss = report_tgt_words = report_examples = 0.\n","\n","        # perform validation\n","        if train_iter % valid_niter == 0:\n","            print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n","                                                                                     cum_loss / cum_examples,\n","                                                                                     np.exp(cum_loss / cum_tgt_words),\n","                                                                                     cum_examples), file=sys.stderr)\n","\n","            cum_loss = cum_examples = cum_tgt_words = 0.\n","            valid_num += 1\n","\n","            print('begin validation ...', file=sys.stderr)\n","\n","            # compute dev. ppl and bleu\n","            dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n","            valid_metric = -dev_ppl\n","\n","            print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n","\n","            is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n","            hist_valid_scores.append(valid_metric)\n","\n","            if is_better:\n","                patience = 0\n","                print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n","                model.save(model_save_path)\n","\n","                # also save the optimizers' state\n","                torch.save(optimizer.state_dict(), model_save_path + '.optim')\n","            elif patience < int(max_patience):\n","                patience += 1\n","                print('hit patience %d' % patience, file=sys.stderr)\n","\n","                if patience == int(max_patience):\n","                    num_trial += 1\n","                    print('hit #%d trial' % num_trial, file=sys.stderr)\n","                    if num_trial == int(max_trial):\n","                        print('early stop!', file=sys.stderr)\n","                        exit(0)\n","\n","                    # decay lr, and restore from previously best checkpoint\n","                    lr = optimizer.param_groups[0]['lr'] * float(lr_decay)\n","                    print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n","\n","                    # load model\n","                    params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n","                    model.load_state_dict(params['state_dict'])\n","                    model = model.to(device)\n","\n","                    print('restore parameters of the optimizers', file=sys.stderr)\n","                    optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n","\n","                    # set new lr\n","                    for param_group in optimizer.param_groups:\n","                        param_group['lr'] = lr\n","\n","                    # reset patience\n","                    patience = 0\n","\n","            if epoch == int(max_epoch):\n","                print('reached maximum number of epochs!', file=sys.stderr)\n","                break"],"execution_count":60,"outputs":[{"output_type":"stream","text":["epoch 3, iter 100, avg. loss 126.17, avg. ppl 1177.05 cum. examples 5824, speed 347.45 words/sec, time elapsed 1021.30 sec\n","epoch 3, iter 200, avg. loss 116.63, avg. ppl 724.67 cum. examples 18624, speed 7187.88 words/sec, time elapsed 1052.84 sec\n","epoch 3, iter 300, avg. loss 107.30, avg. ppl 434.38 cum. examples 31424, speed 7120.48 words/sec, time elapsed 1084.60 sec\n","epoch 3, iter 400, avg. loss 100.24, avg. ppl 305.09 cum. examples 44224, speed 7125.74 words/sec, time elapsed 1116.07 sec\n","epoch 3, iter 500, avg. loss 95.02, avg. ppl 216.17 cum. examples 57024, speed 7128.57 words/sec, time elapsed 1147.81 sec\n","epoch 3, iter 600, avg. loss 89.95, avg. ppl 164.66 cum. examples 69824, speed 7057.68 words/sec, time elapsed 1179.77 sec\n","epoch 3, iter 700, avg. loss 86.48, avg. ppl 132.19 cum. examples 82624, speed 7203.58 words/sec, time elapsed 1211.24 sec\n","epoch 3, iter 800, avg. loss 83.00, avg. ppl 109.43 cum. examples 95424, speed 7181.22 words/sec, time elapsed 1242.74 sec\n","epoch 3, iter 900, avg. loss 80.05, avg. ppl 93.25 cum. examples 108224, speed 7090.89 words/sec, time elapsed 1274.61 sec\n","epoch 3, iter 1000, avg. loss 77.40, avg. ppl 81.45 cum. examples 121024, speed 7113.91 words/sec, time elapsed 1306.26 sec\n","epoch 3, iter 1100, avg. loss 76.13, avg. ppl 73.02 cum. examples 133824, speed 7142.12 words/sec, time elapsed 1338.06 sec\n","epoch 3, iter 1200, avg. loss 73.61, avg. ppl 64.93 cum. examples 146624, speed 7106.04 words/sec, time elapsed 1369.83 sec\n","epoch 3, iter 1300, avg. loss 72.26, avg. ppl 59.11 cum. examples 159424, speed 7150.35 words/sec, time elapsed 1401.54 sec\n","epoch 3, iter 1400, avg. loss 70.29, avg. ppl 53.86 cum. examples 172224, speed 7073.84 words/sec, time elapsed 1433.45 sec\n","epoch 3, iter 1500, avg. loss 68.46, avg. ppl 48.75 cum. examples 185024, speed 7150.52 words/sec, time elapsed 1464.98 sec\n","epoch 3, iter 1600, avg. loss 67.42, avg. ppl 45.02 cum. examples 197824, speed 7010.53 words/sec, time elapsed 1497.31 sec\n","epoch 3, iter 1700, avg. loss 66.21, avg. ppl 42.51 cum. examples 210624, speed 7119.13 words/sec, time elapsed 1529.05 sec\n","epoch 4, iter 1800, avg. loss 64.17, avg. ppl 36.63 cum. examples 223337, speed 7210.64 words/sec, time elapsed 1560.47 sec\n","epoch 4, iter 1900, avg. loss 60.25, avg. ppl 30.52 cum. examples 236137, speed 7115.24 words/sec, time elapsed 1592.18 sec\n","epoch 4, iter 2000, avg. loss 59.12, avg. ppl 29.16 cum. examples 248937, speed 7060.63 words/sec, time elapsed 1623.96 sec\n","epoch 4, iter 2000, cum. loss 80.89, cum. ppl 97.51 cum. examples 248937\n","begin validation ...\n","validation: iter 2000, dev. ppl 40.269371\n","save currently the best model to [NMT_model]\n","save model parameters to [NMT_model]\n","epoch 4, iter 2100, avg. loss 59.97, avg. ppl 28.88 cum. examples 12800, speed 6479.29 words/sec, time elapsed 1659.19 sec\n","epoch 4, iter 2200, avg. loss 58.82, avg. ppl 27.40 cum. examples 25600, speed 7072.12 words/sec, time elapsed 1691.34 sec\n","epoch 4, iter 2300, avg. loss 58.47, avg. ppl 26.57 cum. examples 38400, speed 7149.61 words/sec, time elapsed 1723.26 sec\n","epoch 4, iter 2400, avg. loss 56.93, avg. ppl 25.28 cum. examples 51200, speed 7159.22 words/sec, time elapsed 1754.77 sec\n","epoch 4, iter 2500, avg. loss 56.51, avg. ppl 24.94 cum. examples 64000, speed 7047.32 words/sec, time elapsed 1786.68 sec\n","epoch 4, iter 2600, avg. loss 56.16, avg. ppl 24.09 cum. examples 76800, speed 7128.55 words/sec, time elapsed 1818.38 sec\n","epoch 4, iter 2700, avg. loss 56.17, avg. ppl 23.31 cum. examples 89600, speed 7088.27 words/sec, time elapsed 1850.59 sec\n","epoch 4, iter 2800, avg. loss 55.12, avg. ppl 22.94 cum. examples 102400, speed 7164.87 words/sec, time elapsed 1882.02 sec\n","epoch 4, iter 2900, avg. loss 54.46, avg. ppl 21.79 cum. examples 115200, speed 7071.30 words/sec, time elapsed 1914.01 sec\n","epoch 4, iter 3000, avg. loss 53.91, avg. ppl 21.30 cum. examples 128000, speed 7137.75 words/sec, time elapsed 1945.62 sec\n","epoch 4, iter 3100, avg. loss 53.54, avg. ppl 21.10 cum. examples 140800, speed 7111.06 words/sec, time elapsed 1977.22 sec\n","epoch 4, iter 3200, avg. loss 53.81, avg. ppl 20.50 cum. examples 153600, speed 7171.03 words/sec, time elapsed 2009.03 sec\n","epoch 4, iter 3300, avg. loss 52.89, avg. ppl 20.03 cum. examples 166400, speed 7160.89 words/sec, time elapsed 2040.57 sec\n","epoch 4, iter 3400, avg. loss 52.08, avg. ppl 19.52 cum. examples 179200, speed 7109.96 words/sec, time elapsed 2072.12 sec\n","epoch 5, iter 3500, avg. loss 50.45, avg. ppl 17.05 cum. examples 191913, speed 7103.14 words/sec, time elapsed 2103.96 sec\n","epoch 5, iter 3600, avg. loss 47.26, avg. ppl 14.44 cum. examples 204713, speed 7166.81 words/sec, time elapsed 2135.57 sec\n","epoch 5, iter 3700, avg. loss 46.93, avg. ppl 14.49 cum. examples 217513, speed 7028.95 words/sec, time elapsed 2167.54 sec\n","epoch 5, iter 3800, avg. loss 46.91, avg. ppl 14.25 cum. examples 230313, speed 7210.19 words/sec, time elapsed 2198.88 sec\n","epoch 5, iter 3900, avg. loss 46.95, avg. ppl 14.20 cum. examples 243113, speed 7183.19 words/sec, time elapsed 2230.41 sec\n","epoch 5, iter 4000, avg. loss 47.35, avg. ppl 14.35 cum. examples 255913, speed 7186.94 words/sec, time elapsed 2262.07 sec\n","epoch 5, iter 4000, cum. loss 53.24, cum. ppl 20.29 cum. examples 255913\n","begin validation ...\n","validation: iter 4000, dev. ppl 23.299947\n","save currently the best model to [NMT_model]\n","save model parameters to [NMT_model]\n","epoch 5, iter 4100, avg. loss 46.25, avg. ppl 13.85 cum. examples 12800, speed 6408.59 words/sec, time elapsed 2297.21 sec\n","epoch 5, iter 4200, avg. loss 46.69, avg. ppl 14.06 cum. examples 25600, speed 7068.39 words/sec, time elapsed 2329.20 sec\n","epoch 5, iter 4300, avg. loss 46.68, avg. ppl 13.95 cum. examples 38400, speed 7160.15 words/sec, time elapsed 2360.87 sec\n","epoch 5, iter 4400, avg. loss 46.06, avg. ppl 13.70 cum. examples 51200, speed 7084.04 words/sec, time elapsed 2392.66 sec\n","epoch 5, iter 4500, avg. loss 46.34, avg. ppl 13.58 cum. examples 64000, speed 6980.20 words/sec, time elapsed 2425.24 sec\n","epoch 5, iter 4600, avg. loss 45.93, avg. ppl 13.35 cum. examples 76800, speed 7150.99 words/sec, time elapsed 2456.97 sec\n","epoch 5, iter 4700, avg. loss 45.77, avg. ppl 13.50 cum. examples 89600, speed 7092.59 words/sec, time elapsed 2488.71 sec\n","epoch 5, iter 4800, avg. loss 45.79, avg. ppl 13.32 cum. examples 102400, speed 7211.98 words/sec, time elapsed 2520.09 sec\n","epoch 5, iter 4900, avg. loss 45.68, avg. ppl 13.26 cum. examples 115200, speed 7132.16 words/sec, time elapsed 2551.81 sec\n","epoch 5, iter 5000, avg. loss 45.66, avg. ppl 13.52 cum. examples 128000, speed 7100.12 words/sec, time elapsed 2583.42 sec\n","epoch 5, iter 5100, avg. loss 45.76, avg. ppl 13.25 cum. examples 140800, speed 7085.65 words/sec, time elapsed 2615.41 sec\n","epoch 6, iter 5200, avg. loss 43.22, avg. ppl 11.44 cum. examples 153513, speed 7095.82 words/sec, time elapsed 2647.18 sec\n","epoch 6, iter 5300, avg. loss 40.27, avg. ppl 9.83 cum. examples 166313, speed 7140.35 words/sec, time elapsed 2678.77 sec\n","epoch 6, iter 5400, avg. loss 41.16, avg. ppl 10.10 cum. examples 179113, speed 7137.93 words/sec, time elapsed 2710.68 sec\n","epoch 6, iter 5500, avg. loss 41.21, avg. ppl 10.16 cum. examples 191913, speed 7094.23 words/sec, time elapsed 2742.75 sec\n","epoch 6, iter 5600, avg. loss 40.50, avg. ppl 10.09 cum. examples 204713, speed 7186.08 words/sec, time elapsed 2773.96 sec\n","epoch 6, iter 5700, avg. loss 40.94, avg. ppl 10.15 cum. examples 217513, speed 7081.24 words/sec, time elapsed 2805.88 sec\n","epoch 6, iter 5800, avg. loss 40.65, avg. ppl 10.08 cum. examples 230313, speed 7090.53 words/sec, time elapsed 2837.64 sec\n","epoch 6, iter 5900, avg. loss 41.57, avg. ppl 10.34 cum. examples 243113, speed 7092.59 words/sec, time elapsed 2869.76 sec\n","epoch 6, iter 6000, avg. loss 41.27, avg. ppl 10.33 cum. examples 255913, speed 7090.03 words/sec, time elapsed 2901.67 sec\n","epoch 6, iter 6000, cum. loss 43.87, cum. ppl 11.97 cum. examples 255913\n","begin validation ...\n","validation: iter 6000, dev. ppl 20.010672\n","save currently the best model to [NMT_model]\n","save model parameters to [NMT_model]\n","epoch 6, iter 6100, avg. loss 41.52, avg. ppl 10.39 cum. examples 12800, speed 6466.62 words/sec, time elapsed 2936.78 sec\n","epoch 6, iter 6200, avg. loss 41.53, avg. ppl 10.39 cum. examples 25600, speed 7078.36 words/sec, time elapsed 2968.87 sec\n","epoch 6, iter 6300, avg. loss 40.92, avg. ppl 10.16 cum. examples 38400, speed 7110.59 words/sec, time elapsed 3000.64 sec\n","epoch 6, iter 6400, avg. loss 40.96, avg. ppl 10.33 cum. examples 51200, speed 7005.96 words/sec, time elapsed 3032.69 sec\n","epoch 6, iter 6500, avg. loss 40.99, avg. ppl 10.23 cum. examples 64000, speed 7024.30 words/sec, time elapsed 3064.81 sec\n","epoch 6, iter 6600, avg. loss 41.36, avg. ppl 10.36 cum. examples 76800, speed 7121.00 words/sec, time elapsed 3096.60 sec\n","epoch 6, iter 6700, avg. loss 40.78, avg. ppl 10.18 cum. examples 89600, speed 7101.97 words/sec, time elapsed 3128.28 sec\n","epoch 6, iter 6800, avg. loss 41.18, avg. ppl 10.26 cum. examples 102400, speed 7098.92 words/sec, time elapsed 3160.16 sec\n","epoch 7, iter 6900, avg. loss 38.30, avg. ppl 8.81 cum. examples 115113, speed 7073.44 words/sec, time elapsed 3191.80 sec\n","epoch 7, iter 7000, avg. loss 36.36, avg. ppl 7.85 cum. examples 127913, speed 7074.22 words/sec, time elapsed 3223.73 sec\n","epoch 7, iter 7100, avg. loss 36.81, avg. ppl 8.11 cum. examples 140713, speed 7205.06 words/sec, time elapsed 3254.98 sec\n","epoch 7, iter 7200, avg. loss 37.04, avg. ppl 8.05 cum. examples 153513, speed 7137.57 words/sec, time elapsed 3286.82 sec\n","epoch 7, iter 7300, avg. loss 36.47, avg. ppl 8.05 cum. examples 166313, speed 7053.34 words/sec, time elapsed 3318.54 sec\n","epoch 7, iter 7400, avg. loss 36.99, avg. ppl 8.22 cum. examples 179113, speed 7155.28 words/sec, time elapsed 3349.96 sec\n","epoch 7, iter 7500, avg. loss 37.12, avg. ppl 8.18 cum. examples 191913, speed 7087.32 words/sec, time elapsed 3381.87 sec\n","epoch 7, iter 7600, avg. loss 37.84, avg. ppl 8.45 cum. examples 204713, speed 7201.41 words/sec, time elapsed 3413.38 sec\n","epoch 7, iter 7700, avg. loss 37.66, avg. ppl 8.39 cum. examples 217513, speed 7114.54 words/sec, time elapsed 3445.24 sec\n","epoch 7, iter 7800, avg. loss 37.71, avg. ppl 8.41 cum. examples 230313, speed 7164.74 words/sec, time elapsed 3476.88 sec\n","epoch 7, iter 7900, avg. loss 37.92, avg. ppl 8.53 cum. examples 243113, speed 7139.29 words/sec, time elapsed 3508.60 sec\n","epoch 7, iter 8000, avg. loss 37.53, avg. ppl 8.48 cum. examples 255913, speed 7023.23 words/sec, time elapsed 3540.60 sec\n","epoch 7, iter 8000, cum. loss 38.85, cum. ppl 9.04 cum. examples 255913\n","begin validation ...\n","validation: iter 8000, dev. ppl 18.312628\n","save currently the best model to [NMT_model]\n","save model parameters to [NMT_model]\n","epoch 7, iter 8100, avg. loss 38.07, avg. ppl 8.55 cum. examples 12800, speed 6394.24 words/sec, time elapsed 3576.11 sec\n","epoch 7, iter 8200, avg. loss 38.26, avg. ppl 8.53 cum. examples 25600, speed 7137.29 words/sec, time elapsed 3608.11 sec\n","epoch 7, iter 8300, avg. loss 38.52, avg. ppl 8.72 cum. examples 38400, speed 7144.86 words/sec, time elapsed 3639.98 sec\n","epoch 7, iter 8400, avg. loss 38.20, avg. ppl 8.63 cum. examples 51200, speed 7203.54 words/sec, time elapsed 3671.46 sec\n","epoch 7, iter 8500, avg. loss 38.30, avg. ppl 8.72 cum. examples 64000, speed 7048.88 words/sec, time elapsed 3703.57 sec\n","epoch 8, iter 8600, avg. loss 35.07, avg. ppl 7.29 cum. examples 76713, speed 7145.22 words/sec, time elapsed 3734.99 sec\n","epoch 8, iter 8700, avg. loss 33.54, avg. ppl 6.65 cum. examples 89513, speed 7106.56 words/sec, time elapsed 3766.87 sec\n","epoch 8, iter 8800, avg. loss 33.70, avg. ppl 6.77 cum. examples 102313, speed 7139.24 words/sec, time elapsed 3798.46 sec\n","epoch 8, iter 8900, avg. loss 33.92, avg. ppl 6.90 cum. examples 115113, speed 7070.73 words/sec, time elapsed 3830.25 sec\n","epoch 8, iter 9000, avg. loss 34.39, avg. ppl 7.01 cum. examples 127913, speed 7049.23 words/sec, time elapsed 3862.31 sec\n","epoch 8, iter 9100, avg. loss 34.23, avg. ppl 6.95 cum. examples 140713, speed 7113.27 words/sec, time elapsed 3894.08 sec\n","epoch 8, iter 9200, avg. loss 34.71, avg. ppl 7.12 cum. examples 153513, speed 7151.90 words/sec, time elapsed 3925.73 sec\n","epoch 8, iter 9300, avg. loss 35.37, avg. ppl 7.31 cum. examples 166313, speed 7086.93 words/sec, time elapsed 3957.86 sec\n","epoch 8, iter 9400, avg. loss 35.08, avg. ppl 7.22 cum. examples 179113, speed 7141.37 words/sec, time elapsed 3989.67 sec\n","epoch 8, iter 9500, avg. loss 35.08, avg. ppl 7.25 cum. examples 191913, speed 7103.83 words/sec, time elapsed 4021.57 sec\n","epoch 8, iter 9600, avg. loss 35.29, avg. ppl 7.38 cum. examples 204713, speed 7109.00 words/sec, time elapsed 4053.37 sec\n","epoch 8, iter 9700, avg. loss 35.12, avg. ppl 7.34 cum. examples 217513, speed 7060.35 words/sec, time elapsed 4085.30 sec\n","epoch 8, iter 9800, avg. loss 35.36, avg. ppl 7.41 cum. examples 230313, speed 7074.79 words/sec, time elapsed 4117.24 sec\n","epoch 8, iter 9900, avg. loss 35.38, avg. ppl 7.46 cum. examples 243113, speed 7077.95 words/sec, time elapsed 4149.08 sec\n","epoch 8, iter 10000, avg. loss 35.32, avg. ppl 7.38 cum. examples 255913, speed 7104.37 words/sec, time elapsed 4180.92 sec\n","epoch 8, iter 10000, cum. loss 35.64, cum. ppl 7.50 cum. examples 255913\n","begin validation ...\n","validation: iter 10000, dev. ppl 18.107445\n","save currently the best model to [NMT_model]\n","save model parameters to [NMT_model]\n","epoch 8, iter 10100, avg. loss 35.81, avg. ppl 7.56 cum. examples 12800, speed 6336.25 words/sec, time elapsed 4216.69 sec\n","epoch 8, iter 10200, avg. loss 35.70, avg. ppl 7.52 cum. examples 25600, speed 7108.06 words/sec, time elapsed 4248.56 sec\n","epoch 9, iter 10300, avg. loss 32.47, avg. ppl 6.31 cum. examples 38313, speed 7071.34 words/sec, time elapsed 4280.24 sec\n","epoch 9, iter 10400, avg. loss 31.26, avg. ppl 5.88 cum. examples 51113, speed 7116.13 words/sec, time elapsed 4311.97 sec\n","epoch 9, iter 10500, avg. loss 31.95, avg. ppl 6.03 cum. examples 63913, speed 7036.00 words/sec, time elapsed 4344.33 sec\n","epoch 9, iter 10600, avg. loss 31.84, avg. ppl 6.09 cum. examples 76713, speed 7153.78 words/sec, time elapsed 4375.88 sec\n","epoch 9, iter 10700, avg. loss 32.28, avg. ppl 6.15 cum. examples 89513, speed 7065.58 words/sec, time elapsed 4408.08 sec\n","epoch 9, iter 10800, avg. loss 32.59, avg. ppl 6.22 cum. examples 102313, speed 7101.22 words/sec, time elapsed 4440.21 sec\n","epoch 9, iter 10900, avg. loss 32.67, avg. ppl 6.34 cum. examples 115113, speed 7132.92 words/sec, time elapsed 4471.97 sec\n","epoch 9, iter 11000, avg. loss 32.73, avg. ppl 6.36 cum. examples 127913, speed 7142.15 words/sec, time elapsed 4503.67 sec\n","epoch 9, iter 11100, avg. loss 32.59, avg. ppl 6.35 cum. examples 140713, speed 7075.43 words/sec, time elapsed 4535.58 sec\n","epoch 9, iter 11200, avg. loss 33.00, avg. ppl 6.44 cum. examples 153513, speed 7031.02 words/sec, time elapsed 4567.84 sec\n","epoch 9, iter 11300, avg. loss 33.23, avg. ppl 6.55 cum. examples 166313, speed 7080.78 words/sec, time elapsed 4599.80 sec\n","epoch 9, iter 11400, avg. loss 32.97, avg. ppl 6.50 cum. examples 179113, speed 7079.27 words/sec, time elapsed 4631.64 sec\n","epoch 9, iter 11500, avg. loss 33.55, avg. ppl 6.58 cum. examples 191913, speed 7092.78 words/sec, time elapsed 4663.78 sec\n","epoch 9, iter 11600, avg. loss 33.03, avg. ppl 6.51 cum. examples 204713, speed 7144.57 words/sec, time elapsed 4695.37 sec\n","epoch 9, iter 11700, avg. loss 32.99, avg. ppl 6.64 cum. examples 217513, speed 7071.53 words/sec, time elapsed 4726.92 sec\n","epoch 9, iter 11800, avg. loss 33.50, avg. ppl 6.60 cum. examples 230313, speed 7117.08 words/sec, time elapsed 4758.83 sec\n","epoch 9, iter 11900, avg. loss 33.44, avg. ppl 6.73 cum. examples 243113, speed 7092.82 words/sec, time elapsed 4790.49 sec\n","epoch 10, iter 12000, avg. loss 29.95, avg. ppl 5.49 cum. examples 255826, speed 7009.22 words/sec, time elapsed 4822.39 sec\n","epoch 10, iter 12000, cum. loss 32.88, cum. ppl 6.43 cum. examples 255826\n","begin validation ...\n","validation: iter 12000, dev. ppl 18.633494\n","hit patience 1\n","epoch 10, iter 12100, avg. loss 29.55, avg. ppl 5.34 cum. examples 12800, speed 6936.29 words/sec, time elapsed 4854.94 sec\n","epoch 10, iter 12200, avg. loss 29.64, avg. ppl 5.35 cum. examples 25600, speed 7081.32 words/sec, time elapsed 4886.89 sec\n","epoch 10, iter 12300, avg. loss 29.73, avg. ppl 5.39 cum. examples 38400, speed 7057.11 words/sec, time elapsed 4918.91 sec\n","epoch 10, iter 12400, avg. loss 30.06, avg. ppl 5.58 cum. examples 51200, speed 7034.41 words/sec, time elapsed 4950.73 sec\n","epoch 10, iter 12500, avg. loss 30.34, avg. ppl 5.53 cum. examples 64000, speed 7073.44 words/sec, time elapsed 4982.82 sec\n","epoch 10, iter 12600, avg. loss 31.06, avg. ppl 5.67 cum. examples 76800, speed 7258.85 words/sec, time elapsed 5014.36 sec\n","epoch 10, iter 12700, avg. loss 31.16, avg. ppl 5.75 cum. examples 89600, speed 7178.69 words/sec, time elapsed 5046.14 sec\n","epoch 10, iter 12800, avg. loss 31.14, avg. ppl 5.78 cum. examples 102400, speed 7169.97 words/sec, time elapsed 5077.82 sec\n","epoch 10, iter 12900, avg. loss 31.17, avg. ppl 5.76 cum. examples 115200, speed 7182.24 words/sec, time elapsed 5109.54 sec\n","epoch 10, iter 13000, avg. loss 31.18, avg. ppl 5.84 cum. examples 128000, speed 7099.49 words/sec, time elapsed 5141.40 sec\n","epoch 10, iter 13100, avg. loss 31.34, avg. ppl 5.92 cum. examples 140800, speed 7150.86 words/sec, time elapsed 5172.96 sec\n","epoch 10, iter 13200, avg. loss 31.27, avg. ppl 5.91 cum. examples 153600, speed 7078.06 words/sec, time elapsed 5204.80 sec\n","epoch 10, iter 13300, avg. loss 31.51, avg. ppl 5.94 cum. examples 166400, speed 7091.16 words/sec, time elapsed 5236.70 sec\n","epoch 10, iter 13400, avg. loss 31.42, avg. ppl 5.98 cum. examples 179200, speed 7058.26 words/sec, time elapsed 5268.57 sec\n","epoch 10, iter 13500, avg. loss 31.87, avg. ppl 6.10 cum. examples 192000, speed 7077.76 words/sec, time elapsed 5300.45 sec\n","epoch 10, iter 13600, avg. loss 31.45, avg. ppl 6.04 cum. examples 204800, speed 7096.54 words/sec, time elapsed 5332.01 sec\n","epoch 11, iter 13700, avg. loss 28.08, avg. ppl 4.95 cum. examples 217513, speed 7015.48 words/sec, time elapsed 5363.80 sec\n","epoch 11, iter 13800, avg. loss 28.07, avg. ppl 4.87 cum. examples 230313, speed 7102.24 words/sec, time elapsed 5395.78 sec\n","epoch 11, iter 13900, avg. loss 28.22, avg. ppl 4.89 cum. examples 243113, speed 7131.56 words/sec, time elapsed 5427.66 sec\n","epoch 11, iter 14000, avg. loss 28.34, avg. ppl 4.94 cum. examples 255913, speed 7139.45 words/sec, time elapsed 5459.46 sec\n","epoch 11, iter 14000, cum. loss 30.33, cum. ppl 5.56 cum. examples 255913\n","begin validation ...\n","validation: iter 14000, dev. ppl 19.532416\n","hit patience 2\n","epoch 11, iter 14100, avg. loss 28.94, avg. ppl 5.11 cum. examples 12800, speed 6946.36 words/sec, time elapsed 5492.15 sec\n","epoch 11, iter 14200, avg. loss 28.68, avg. ppl 5.05 cum. examples 25600, speed 7075.43 words/sec, time elapsed 5524.18 sec\n","epoch 11, iter 14300, avg. loss 29.30, avg. ppl 5.16 cum. examples 38400, speed 7046.95 words/sec, time elapsed 5556.62 sec\n","epoch 11, iter 14400, avg. loss 29.00, avg. ppl 5.17 cum. examples 51200, speed 7159.84 words/sec, time elapsed 5588.19 sec\n","epoch 11, iter 14500, avg. loss 29.21, avg. ppl 5.17 cum. examples 64000, speed 7115.31 words/sec, time elapsed 5620.18 sec\n","epoch 11, iter 14600, avg. loss 29.30, avg. ppl 5.31 cum. examples 76800, speed 7101.81 words/sec, time elapsed 5651.81 sec\n","epoch 11, iter 14700, avg. loss 29.37, avg. ppl 5.34 cum. examples 89600, speed 7048.23 words/sec, time elapsed 5683.65 sec\n","epoch 11, iter 14800, avg. loss 29.60, avg. ppl 5.35 cum. examples 102400, speed 7073.43 words/sec, time elapsed 5715.60 sec\n","epoch 11, iter 14900, avg. loss 29.76, avg. ppl 5.38 cum. examples 115200, speed 7073.17 words/sec, time elapsed 5747.62 sec\n","epoch 11, iter 15000, avg. loss 30.23, avg. ppl 5.52 cum. examples 128000, speed 7112.02 words/sec, time elapsed 5779.48 sec\n","epoch 11, iter 15100, avg. loss 29.90, avg. ppl 5.40 cum. examples 140800, speed 7082.17 words/sec, time elapsed 5811.53 sec\n","epoch 11, iter 15200, avg. loss 30.07, avg. ppl 5.55 cum. examples 153600, speed 7107.05 words/sec, time elapsed 5843.12 sec\n","epoch 11, iter 15300, avg. loss 30.09, avg. ppl 5.54 cum. examples 166400, speed 7034.07 words/sec, time elapsed 5875.10 sec\n","epoch 12, iter 15400, avg. loss 26.18, avg. ppl 4.44 cum. examples 179113, speed 7159.56 words/sec, time elapsed 5906.28 sec\n","epoch 12, iter 15500, avg. loss 26.49, avg. ppl 4.52 cum. examples 191913, speed 7126.79 words/sec, time elapsed 5937.84 sec\n","epoch 12, iter 15600, avg. loss 26.62, avg. ppl 4.44 cum. examples 204713, speed 7181.91 words/sec, time elapsed 5969.69 sec\n","epoch 12, iter 15700, avg. loss 26.89, avg. ppl 4.61 cum. examples 217513, speed 7100.26 words/sec, time elapsed 6001.40 sec\n","epoch 12, iter 15800, avg. loss 27.28, avg. ppl 4.65 cum. examples 230313, speed 7118.70 words/sec, time elapsed 6033.33 sec\n","epoch 12, iter 15900, avg. loss 27.01, avg. ppl 4.67 cum. examples 243113, speed 7011.25 words/sec, time elapsed 6065.32 sec\n","epoch 12, iter 16000, avg. loss 27.50, avg. ppl 4.70 cum. examples 255913, speed 7091.07 words/sec, time elapsed 6097.41 sec\n","epoch 12, iter 16000, cum. loss 28.57, cum. ppl 5.04 cum. examples 255913\n","begin validation ...\n","validation: iter 16000, dev. ppl 21.066770\n","hit patience 3\n","epoch 12, iter 16100, avg. loss 27.29, avg. ppl 4.75 cum. examples 12800, speed 6907.46 words/sec, time elapsed 6129.86 sec\n","epoch 12, iter 16200, avg. loss 27.86, avg. ppl 4.84 cum. examples 25600, speed 7069.51 words/sec, time elapsed 6161.86 sec\n","epoch 12, iter 16300, avg. loss 27.66, avg. ppl 4.78 cum. examples 38400, speed 7126.96 words/sec, time elapsed 6193.60 sec\n","epoch 12, iter 16400, avg. loss 28.28, avg. ppl 4.89 cum. examples 51200, speed 7129.84 words/sec, time elapsed 6225.57 sec\n","epoch 12, iter 16500, avg. loss 28.42, avg. ppl 4.95 cum. examples 64000, speed 7060.31 words/sec, time elapsed 6257.79 sec\n","epoch 12, iter 16600, avg. loss 27.98, avg. ppl 4.95 cum. examples 76800, speed 7093.44 words/sec, time elapsed 6289.37 sec\n","epoch 12, iter 16700, avg. loss 28.47, avg. ppl 5.02 cum. examples 89600, speed 7197.66 words/sec, time elapsed 6320.74 sec\n","epoch 12, iter 16800, avg. loss 28.82, avg. ppl 5.06 cum. examples 102400, speed 7043.08 words/sec, time elapsed 6353.02 sec\n","epoch 12, iter 16900, avg. loss 28.63, avg. ppl 5.05 cum. examples 115200, speed 7055.88 words/sec, time elapsed 6385.08 sec\n","epoch 12, iter 17000, avg. loss 28.93, avg. ppl 5.14 cum. examples 128000, speed 7096.03 words/sec, time elapsed 6416.98 sec\n","epoch 13, iter 17100, avg. loss 24.68, avg. ppl 4.01 cum. examples 140713, speed 7088.61 words/sec, time elapsed 6448.85 sec\n","epoch 13, iter 17200, avg. loss 25.00, avg. ppl 4.08 cum. examples 153513, speed 7091.08 words/sec, time elapsed 6480.93 sec\n","epoch 13, iter 17300, avg. loss 25.26, avg. ppl 4.19 cum. examples 166313, speed 7046.55 words/sec, time elapsed 6512.97 sec\n","epoch 13, iter 17400, avg. loss 25.52, avg. ppl 4.24 cum. examples 179113, speed 7112.82 words/sec, time elapsed 6544.74 sec\n","epoch 13, iter 17500, avg. loss 25.57, avg. ppl 4.24 cum. examples 191913, speed 7144.69 words/sec, time elapsed 6576.45 sec\n","epoch 13, iter 17600, avg. loss 25.64, avg. ppl 4.27 cum. examples 204713, speed 7159.18 words/sec, time elapsed 6608.00 sec\n","epoch 13, iter 17700, avg. loss 25.85, avg. ppl 4.37 cum. examples 217513, speed 7119.92 words/sec, time elapsed 6639.53 sec\n","epoch 13, iter 17800, avg. loss 26.36, avg. ppl 4.40 cum. examples 230313, speed 7044.16 words/sec, time elapsed 6671.85 sec\n","epoch 13, iter 17900, avg. loss 26.02, avg. ppl 4.45 cum. examples 243113, speed 7068.39 words/sec, time elapsed 6703.39 sec\n","epoch 13, iter 18000, avg. loss 26.82, avg. ppl 4.53 cum. examples 255913, speed 7180.94 words/sec, time elapsed 6735.06 sec\n","epoch 13, iter 18000, cum. loss 26.95, cum. ppl 4.60 cum. examples 255913\n","begin validation ...\n","validation: iter 18000, dev. ppl 21.315497\n","hit patience 4\n","epoch 13, iter 18100, avg. loss 26.71, avg. ppl 4.53 cum. examples 12800, speed 6977.90 words/sec, time elapsed 6767.51 sec\n","epoch 13, iter 18200, avg. loss 27.23, avg. ppl 4.64 cum. examples 25600, speed 7075.95 words/sec, time elapsed 6799.63 sec\n","epoch 13, iter 18300, avg. loss 26.91, avg. ppl 4.61 cum. examples 38400, speed 7068.00 words/sec, time elapsed 6831.50 sec\n","epoch 13, iter 18400, avg. loss 27.11, avg. ppl 4.65 cum. examples 51200, speed 7075.27 words/sec, time elapsed 6863.43 sec\n","epoch 13, iter 18500, avg. loss 27.06, avg. ppl 4.64 cum. examples 64000, speed 7153.39 words/sec, time elapsed 6894.96 sec\n","epoch 13, iter 18600, avg. loss 27.50, avg. ppl 4.71 cum. examples 76800, speed 6990.55 words/sec, time elapsed 6927.47 sec\n","epoch 14, iter 18700, avg. loss 27.23, avg. ppl 4.67 cum. examples 89513, speed 7159.79 words/sec, time elapsed 6958.83 sec\n","epoch 14, iter 18800, avg. loss 23.64, avg. ppl 3.76 cum. examples 102313, speed 7173.65 words/sec, time elapsed 6990.66 sec\n","epoch 14, iter 18900, avg. loss 23.52, avg. ppl 3.81 cum. examples 115113, speed 7132.77 words/sec, time elapsed 7022.25 sec\n","epoch 14, iter 19000, avg. loss 24.07, avg. ppl 3.91 cum. examples 127913, speed 7061.19 words/sec, time elapsed 7054.27 sec\n","epoch 14, iter 19100, avg. loss 24.19, avg. ppl 3.92 cum. examples 140713, speed 6969.96 words/sec, time elapsed 7086.81 sec\n","epoch 14, iter 19200, avg. loss 24.40, avg. ppl 3.97 cum. examples 153513, speed 7175.07 words/sec, time elapsed 7118.40 sec\n","epoch 14, iter 19300, avg. loss 24.98, avg. ppl 4.11 cum. examples 166313, speed 7074.05 words/sec, time elapsed 7150.37 sec\n","epoch 14, iter 19400, avg. loss 24.90, avg. ppl 4.10 cum. examples 179113, speed 7124.20 words/sec, time elapsed 7182.08 sec\n","epoch 14, iter 19500, avg. loss 24.69, avg. ppl 4.08 cum. examples 191913, speed 7124.96 words/sec, time elapsed 7213.60 sec\n","epoch 14, iter 19600, avg. loss 25.04, avg. ppl 4.15 cum. examples 204713, speed 7047.47 words/sec, time elapsed 7245.58 sec\n","epoch 14, iter 19700, avg. loss 25.48, avg. ppl 4.19 cum. examples 217513, speed 7131.20 words/sec, time elapsed 7277.49 sec\n","epoch 14, iter 19800, avg. loss 25.21, avg. ppl 4.20 cum. examples 230313, speed 7063.92 words/sec, time elapsed 7309.31 sec\n","epoch 14, iter 19900, avg. loss 25.69, avg. ppl 4.27 cum. examples 243113, speed 7245.89 words/sec, time elapsed 7340.59 sec\n","epoch 14, iter 20000, avg. loss 25.88, avg. ppl 4.32 cum. examples 255913, speed 7120.49 words/sec, time elapsed 7372.38 sec\n","epoch 14, iter 20000, cum. loss 25.57, cum. ppl 4.25 cum. examples 255913\n","begin validation ...\n","validation: iter 20000, dev. ppl 21.794502\n","hit patience 5\n","hit #1 trial\n","load previously best model and decay learning rate to 0.000500\n","restore parameters of the optimizers\n","epoch 14, iter 20100, avg. loss 30.87, avg. ppl 5.75 cum. examples 12800, speed 6820.47 words/sec, time elapsed 7405.50 sec\n","epoch 14, iter 20200, avg. loss 31.05, avg. ppl 5.83 cum. examples 25600, speed 7068.06 words/sec, time elapsed 7437.40 sec\n","epoch 14, iter 20300, avg. loss 31.02, avg. ppl 5.74 cum. examples 38400, speed 7116.30 words/sec, time elapsed 7469.32 sec\n","epoch 15, iter 20400, avg. loss 30.79, avg. ppl 5.71 cum. examples 51113, speed 7122.44 words/sec, time elapsed 7500.87 sec\n","epoch 15, iter 20500, avg. loss 30.30, avg. ppl 5.52 cum. examples 63913, speed 7061.16 words/sec, time elapsed 7533.01 sec\n","epoch 15, iter 20600, avg. loss 30.48, avg. ppl 5.56 cum. examples 76713, speed 7056.66 words/sec, time elapsed 7565.22 sec\n","epoch 15, iter 20700, avg. loss 30.54, avg. ppl 5.63 cum. examples 89513, speed 7044.63 words/sec, time elapsed 7597.33 sec\n","epoch 15, iter 20800, avg. loss 30.77, avg. ppl 5.67 cum. examples 102313, speed 7106.20 words/sec, time elapsed 7629.28 sec\n","epoch 15, iter 20900, avg. loss 30.54, avg. ppl 5.61 cum. examples 115113, speed 7148.86 words/sec, time elapsed 7661.00 sec\n","epoch 15, iter 21000, avg. loss 30.95, avg. ppl 5.74 cum. examples 127913, speed 7070.42 words/sec, time elapsed 7693.07 sec\n","epoch 15, iter 21100, avg. loss 30.40, avg. ppl 5.64 cum. examples 140713, speed 7068.44 words/sec, time elapsed 7724.90 sec\n","epoch 15, iter 21200, avg. loss 30.72, avg. ppl 5.71 cum. examples 153513, speed 7065.23 words/sec, time elapsed 7756.85 sec\n","epoch 15, iter 21300, avg. loss 31.00, avg. ppl 5.81 cum. examples 166313, speed 7048.64 words/sec, time elapsed 7788.85 sec\n","epoch 15, iter 21400, avg. loss 30.60, avg. ppl 5.71 cum. examples 179113, speed 7071.08 words/sec, time elapsed 7820.63 sec\n","epoch 15, iter 21500, avg. loss 30.83, avg. ppl 5.77 cum. examples 191913, speed 7139.57 words/sec, time elapsed 7852.18 sec\n","epoch 15, iter 21600, avg. loss 30.79, avg. ppl 5.74 cum. examples 204713, speed 7120.64 words/sec, time elapsed 7883.85 sec\n","epoch 15, iter 21700, avg. loss 31.43, avg. ppl 5.86 cum. examples 217513, speed 7093.79 words/sec, time elapsed 7915.92 sec\n","epoch 15, iter 21800, avg. loss 31.08, avg. ppl 5.84 cum. examples 230313, speed 7064.72 words/sec, time elapsed 7947.83 sec\n","epoch 15, iter 21900, avg. loss 31.55, avg. ppl 5.93 cum. examples 243113, speed 7125.52 words/sec, time elapsed 7979.66 sec\n","epoch 15, iter 22000, avg. loss 31.42, avg. ppl 5.89 cum. examples 255913, speed 7044.86 words/sec, time elapsed 8011.87 sec\n","epoch 15, iter 22000, cum. loss 30.86, cum. ppl 5.73 cum. examples 255913\n","begin validation ...\n","validation: iter 22000, dev. ppl 17.696275\n","save currently the best model to [NMT_model]\n","save model parameters to [NMT_model]\n","epoch 16, iter 22100, avg. loss 30.61, avg. ppl 5.70 cum. examples 12713, speed 6365.05 words/sec, time elapsed 8046.99 sec\n","epoch 16, iter 22200, avg. loss 28.34, avg. ppl 4.95 cum. examples 25513, speed 7189.89 words/sec, time elapsed 8078.53 sec\n","epoch 16, iter 22300, avg. loss 28.45, avg. ppl 5.01 cum. examples 38313, speed 7126.10 words/sec, time elapsed 8110.25 sec\n","epoch 16, iter 22400, avg. loss 28.48, avg. ppl 4.97 cum. examples 51113, speed 7199.25 words/sec, time elapsed 8141.82 sec\n","epoch 16, iter 22500, avg. loss 28.58, avg. ppl 5.02 cum. examples 63913, speed 7077.39 words/sec, time elapsed 8173.87 sec\n","epoch 16, iter 22600, avg. loss 29.37, avg. ppl 5.21 cum. examples 76713, speed 7152.16 words/sec, time elapsed 8205.71 sec\n","epoch 16, iter 22700, avg. loss 28.92, avg. ppl 5.13 cum. examples 89513, speed 7094.27 words/sec, time elapsed 8237.61 sec\n","epoch 16, iter 22800, avg. loss 29.01, avg. ppl 5.17 cum. examples 102313, speed 7115.43 words/sec, time elapsed 8269.40 sec\n","epoch 16, iter 22900, avg. loss 28.85, avg. ppl 5.15 cum. examples 115113, speed 6964.99 words/sec, time elapsed 8301.74 sec\n","epoch 16, iter 23000, avg. loss 29.25, avg. ppl 5.24 cum. examples 127913, speed 7087.07 words/sec, time elapsed 8333.63 sec\n","epoch 16, iter 23100, avg. loss 29.63, avg. ppl 5.31 cum. examples 140713, speed 7088.77 words/sec, time elapsed 8365.70 sec\n","epoch 16, iter 23200, avg. loss 29.64, avg. ppl 5.34 cum. examples 153513, speed 7188.76 words/sec, time elapsed 8397.21 sec\n","epoch 16, iter 23300, avg. loss 29.49, avg. ppl 5.27 cum. examples 166313, speed 7152.18 words/sec, time elapsed 8428.96 sec\n","epoch 16, iter 23400, avg. loss 29.72, avg. ppl 5.39 cum. examples 179113, speed 7120.91 words/sec, time elapsed 8460.68 sec\n","epoch 16, iter 23500, avg. loss 29.48, avg. ppl 5.33 cum. examples 191913, speed 7042.16 words/sec, time elapsed 8492.70 sec\n","epoch 16, iter 23600, avg. loss 29.36, avg. ppl 5.32 cum. examples 204713, speed 6998.42 words/sec, time elapsed 8524.83 sec\n","epoch 16, iter 23700, avg. loss 29.90, avg. ppl 5.43 cum. examples 217513, speed 7206.91 words/sec, time elapsed 8556.21 sec\n","epoch 17, iter 23800, avg. loss 28.84, avg. ppl 5.18 cum. examples 230226, speed 7024.47 words/sec, time elapsed 8587.93 sec\n","epoch 17, iter 23900, avg. loss 26.82, avg. ppl 4.59 cum. examples 243026, speed 6986.21 words/sec, time elapsed 8620.18 sec\n","epoch 17, iter 24000, avg. loss 27.21, avg. ppl 4.63 cum. examples 255826, speed 7091.72 words/sec, time elapsed 8652.20 sec\n","epoch 17, iter 24000, cum. loss 29.00, cum. ppl 5.16 cum. examples 255826\n","begin validation ...\n","validation: iter 24000, dev. ppl 19.129556\n","hit patience 1\n","epoch 17, iter 24100, avg. loss 27.25, avg. ppl 4.68 cum. examples 12800, speed 6935.43 words/sec, time elapsed 8684.81 sec\n","epoch 17, iter 24200, avg. loss 27.25, avg. ppl 4.71 cum. examples 25600, speed 7065.59 words/sec, time elapsed 8716.68 sec\n","epoch 17, iter 24300, avg. loss 27.54, avg. ppl 4.74 cum. examples 38400, speed 7054.05 words/sec, time elapsed 8748.79 sec\n","epoch 17, iter 24400, avg. loss 27.84, avg. ppl 4.85 cum. examples 51200, speed 7073.20 words/sec, time elapsed 8780.70 sec\n","epoch 17, iter 24500, avg. loss 27.96, avg. ppl 4.83 cum. examples 64000, speed 7160.77 words/sec, time elapsed 8812.44 sec\n","epoch 17, iter 24600, avg. loss 27.81, avg. ppl 4.80 cum. examples 76800, speed 7053.79 words/sec, time elapsed 8844.61 sec\n","epoch 17, iter 24700, avg. loss 27.88, avg. ppl 4.84 cum. examples 89600, speed 7091.11 words/sec, time elapsed 8876.54 sec\n","epoch 17, iter 24800, avg. loss 28.42, avg. ppl 4.96 cum. examples 102400, speed 7162.90 words/sec, time elapsed 8908.24 sec\n","epoch 17, iter 24900, avg. loss 28.24, avg. ppl 4.88 cum. examples 115200, speed 7082.77 words/sec, time elapsed 8940.42 sec\n","epoch 17, iter 25000, avg. loss 28.31, avg. ppl 4.96 cum. examples 128000, speed 7120.02 words/sec, time elapsed 8972.19 sec\n","epoch 17, iter 25100, avg. loss 28.15, avg. ppl 4.93 cum. examples 140800, speed 7057.89 words/sec, time elapsed 9004.21 sec\n","epoch 17, iter 25200, avg. loss 28.19, avg. ppl 4.96 cum. examples 153600, speed 7140.33 words/sec, time elapsed 9035.75 sec\n","epoch 17, iter 25300, avg. loss 28.17, avg. ppl 4.94 cum. examples 166400, speed 7121.80 words/sec, time elapsed 9067.45 sec\n","epoch 17, iter 25400, avg. loss 27.97, avg. ppl 4.95 cum. examples 179200, speed 7041.85 words/sec, time elapsed 9099.22 sec\n","epoch 18, iter 25500, avg. loss 27.86, avg. ppl 4.82 cum. examples 191913, speed 7128.85 words/sec, time elapsed 9130.83 sec\n","epoch 18, iter 25600, avg. loss 25.43, avg. ppl 4.27 cum. examples 204713, speed 7087.50 words/sec, time elapsed 9162.49 sec\n","epoch 18, iter 25700, avg. loss 26.13, avg. ppl 4.37 cum. examples 217513, speed 7095.81 words/sec, time elapsed 9194.43 sec\n","epoch 18, iter 25800, avg. loss 25.80, avg. ppl 4.35 cum. examples 230313, speed 7115.99 words/sec, time elapsed 9225.99 sec\n","epoch 18, iter 25900, avg. loss 26.12, avg. ppl 4.40 cum. examples 243113, speed 7058.18 words/sec, time elapsed 9257.98 sec\n","epoch 18, iter 26000, avg. loss 26.51, avg. ppl 4.43 cum. examples 255913, speed 7201.88 words/sec, time elapsed 9289.61 sec\n","epoch 18, iter 26000, cum. loss 27.44, cum. ppl 4.73 cum. examples 255913\n","begin validation ...\n","validation: iter 26000, dev. ppl 20.141237\n","hit patience 2\n","epoch 18, iter 26100, avg. loss 26.79, avg. ppl 4.51 cum. examples 12800, speed 6947.70 words/sec, time elapsed 9322.38 sec\n","epoch 18, iter 26200, avg. loss 26.86, avg. ppl 4.55 cum. examples 25600, speed 7096.07 words/sec, time elapsed 9354.34 sec\n","epoch 18, iter 26300, avg. loss 26.62, avg. ppl 4.54 cum. examples 38400, speed 7129.08 words/sec, time elapsed 9385.93 sec\n","epoch 18, iter 26400, avg. loss 26.55, avg. ppl 4.49 cum. examples 51200, speed 7117.76 words/sec, time elapsed 9417.70 sec\n","epoch 18, iter 26500, avg. loss 26.80, avg. ppl 4.56 cum. examples 64000, speed 7082.05 words/sec, time elapsed 9449.62 sec\n","epoch 18, iter 26600, avg. loss 27.08, avg. ppl 4.60 cum. examples 76800, speed 7043.21 words/sec, time elapsed 9481.87 sec\n","epoch 18, iter 26700, avg. loss 27.23, avg. ppl 4.64 cum. examples 89600, speed 7149.22 words/sec, time elapsed 9513.64 sec\n","epoch 18, iter 26800, avg. loss 27.07, avg. ppl 4.61 cum. examples 102400, speed 7061.74 words/sec, time elapsed 9545.77 sec\n","epoch 18, iter 26900, avg. loss 27.09, avg. ppl 4.65 cum. examples 115200, speed 6974.21 words/sec, time elapsed 9578.12 sec\n","epoch 18, iter 27000, avg. loss 27.43, avg. ppl 4.68 cum. examples 128000, speed 7127.62 words/sec, time elapsed 9610.03 sec\n","epoch 18, iter 27100, avg. loss 27.19, avg. ppl 4.73 cum. examples 140800, speed 7113.54 words/sec, time elapsed 9641.52 sec\n","epoch 19, iter 27200, avg. loss 26.48, avg. ppl 4.46 cum. examples 153513, speed 7137.82 words/sec, time elapsed 9673.09 sec\n","epoch 19, iter 27300, avg. loss 24.62, avg. ppl 4.05 cum. examples 166313, speed 7031.19 words/sec, time elapsed 9705.13 sec\n","epoch 19, iter 27400, avg. loss 24.83, avg. ppl 4.08 cum. examples 179113, speed 7008.81 words/sec, time elapsed 9737.37 sec\n","epoch 19, iter 27500, avg. loss 24.97, avg. ppl 4.08 cum. examples 191913, speed 7059.53 words/sec, time elapsed 9769.56 sec\n","epoch 19, iter 27600, avg. loss 25.25, avg. ppl 4.16 cum. examples 204713, speed 7029.91 words/sec, time elapsed 9801.80 sec\n","epoch 19, iter 27700, avg. loss 25.22, avg. ppl 4.16 cum. examples 217513, speed 7089.12 words/sec, time elapsed 9833.76 sec\n","epoch 19, iter 27800, avg. loss 25.34, avg. ppl 4.21 cum. examples 230313, speed 6998.08 words/sec, time elapsed 9866.02 sec\n","epoch 19, iter 27900, avg. loss 25.55, avg. ppl 4.25 cum. examples 243113, speed 7088.59 words/sec, time elapsed 9897.89 sec\n","epoch 19, iter 28000, avg. loss 25.61, avg. ppl 4.25 cum. examples 255913, speed 7106.84 words/sec, time elapsed 9929.75 sec\n","epoch 19, iter 28000, cum. loss 26.23, cum. ppl 4.41 cum. examples 255913\n","begin validation ...\n","validation: iter 28000, dev. ppl 20.768209\n","hit patience 3\n","epoch 19, iter 28100, avg. loss 25.79, avg. ppl 4.26 cum. examples 12800, speed 7007.24 words/sec, time elapsed 9962.27 sec\n","epoch 19, iter 28200, avg. loss 25.78, avg. ppl 4.32 cum. examples 25600, speed 6982.50 words/sec, time elapsed 9994.58 sec\n","epoch 19, iter 28300, avg. loss 26.12, avg. ppl 4.37 cum. examples 38400, speed 7119.63 words/sec, time elapsed 10026.40 sec\n","epoch 19, iter 28400, avg. loss 26.00, avg. ppl 4.35 cum. examples 51200, speed 7061.12 words/sec, time elapsed 10058.46 sec\n","epoch 19, iter 28500, avg. loss 25.73, avg. ppl 4.35 cum. examples 64000, speed 7066.48 words/sec, time elapsed 10090.17 sec\n","epoch 19, iter 28600, avg. loss 26.25, avg. ppl 4.43 cum. examples 76800, speed 7079.43 words/sec, time elapsed 10122.06 sec\n","epoch 19, iter 28700, avg. loss 26.39, avg. ppl 4.43 cum. examples 89600, speed 7023.37 words/sec, time elapsed 10154.36 sec\n","epoch 19, iter 28800, avg. loss 26.36, avg. ppl 4.43 cum. examples 102400, speed 7086.39 words/sec, time elapsed 10186.36 sec\n","epoch 20, iter 28900, avg. loss 25.02, avg. ppl 4.16 cum. examples 115113, speed 7003.20 words/sec, time elapsed 10218.21 sec\n","epoch 20, iter 29000, avg. loss 23.65, avg. ppl 3.82 cum. examples 127913, speed 7060.45 words/sec, time elapsed 10250.21 sec\n","epoch 20, iter 29100, avg. loss 23.60, avg. ppl 3.84 cum. examples 140713, speed 7104.17 words/sec, time elapsed 10281.83 sec\n","epoch 20, iter 29200, avg. loss 23.99, avg. ppl 3.88 cum. examples 153513, speed 7114.43 words/sec, time elapsed 10313.68 sec\n","epoch 20, iter 29300, avg. loss 24.18, avg. ppl 3.89 cum. examples 166313, speed 7114.22 words/sec, time elapsed 10345.71 sec\n","epoch 20, iter 29400, avg. loss 24.20, avg. ppl 3.92 cum. examples 179113, speed 7064.85 words/sec, time elapsed 10377.82 sec\n","epoch 20, iter 29500, avg. loss 24.56, avg. ppl 4.02 cum. examples 191913, speed 7085.77 words/sec, time elapsed 10409.71 sec\n","epoch 20, iter 29600, avg. loss 24.65, avg. ppl 4.05 cum. examples 204713, speed 7106.62 words/sec, time elapsed 10441.45 sec\n","epoch 20, iter 29700, avg. loss 24.96, avg. ppl 4.06 cum. examples 217513, speed 7038.35 words/sec, time elapsed 10473.85 sec\n","epoch 20, iter 29800, avg. loss 24.84, avg. ppl 4.07 cum. examples 230313, speed 7020.49 words/sec, time elapsed 10506.08 sec\n","epoch 20, iter 29900, avg. loss 24.83, avg. ppl 4.09 cum. examples 243113, speed 7062.79 words/sec, time elapsed 10538.02 sec\n","epoch 20, iter 30000, avg. loss 24.97, avg. ppl 4.10 cum. examples 255913, speed 7024.27 words/sec, time elapsed 10570.24 sec\n","epoch 20, iter 30000, cum. loss 25.09, cum. ppl 4.14 cum. examples 255913\n","begin validation ...\n","validation: iter 30000, dev. ppl 22.057927\n","hit patience 4\n","epoch 20, iter 30100, avg. loss 25.16, avg. ppl 4.12 cum. examples 12800, speed 7030.10 words/sec, time elapsed 10602.57 sec\n","epoch 20, iter 30200, avg. loss 25.25, avg. ppl 4.17 cum. examples 25600, speed 6999.00 words/sec, time elapsed 10634.94 sec\n","epoch 20, iter 30300, avg. loss 24.86, avg. ppl 4.16 cum. examples 38400, speed 7072.37 words/sec, time elapsed 10666.52 sec\n","epoch 20, iter 30400, avg. loss 25.28, avg. ppl 4.19 cum. examples 51200, speed 7091.53 words/sec, time elapsed 10698.36 sec\n","epoch 20, iter 30500, avg. loss 25.35, avg. ppl 4.20 cum. examples 64000, speed 7116.57 words/sec, time elapsed 10730.13 sec\n","epoch 21, iter 30600, avg. loss 23.73, avg. ppl 3.85 cum. examples 76713, speed 7070.49 words/sec, time elapsed 10761.80 sec\n","epoch 21, iter 30700, avg. loss 22.74, avg. ppl 3.60 cum. examples 89513, speed 6987.34 words/sec, time elapsed 10794.30 sec\n","epoch 21, iter 30800, avg. loss 22.56, avg. ppl 3.61 cum. examples 102313, speed 7029.06 words/sec, time elapsed 10826.29 sec\n","epoch 21, iter 30900, avg. loss 22.82, avg. ppl 3.71 cum. examples 115113, speed 6973.66 words/sec, time elapsed 10858.23 sec\n","epoch 21, iter 31000, avg. loss 23.53, avg. ppl 3.73 cum. examples 127913, speed 7066.31 words/sec, time elapsed 10890.60 sec\n","epoch 21, iter 31100, avg. loss 23.54, avg. ppl 3.77 cum. examples 140713, speed 7071.16 words/sec, time elapsed 10922.73 sec\n","epoch 21, iter 31200, avg. loss 23.51, avg. ppl 3.75 cum. examples 153513, speed 7064.53 words/sec, time elapsed 10954.94 sec\n","epoch 21, iter 31300, avg. loss 23.75, avg. ppl 3.83 cum. examples 166313, speed 7185.08 words/sec, time elapsed 10986.48 sec\n","epoch 21, iter 31400, avg. loss 23.78, avg. ppl 3.82 cum. examples 179113, speed 6977.98 words/sec, time elapsed 11019.04 sec\n","epoch 21, iter 31500, avg. loss 23.78, avg. ppl 3.86 cum. examples 191913, speed 7080.60 words/sec, time elapsed 11050.84 sec\n","epoch 21, iter 31600, avg. loss 24.30, avg. ppl 3.91 cum. examples 204713, speed 7109.05 words/sec, time elapsed 11082.94 sec\n","epoch 21, iter 31700, avg. loss 23.86, avg. ppl 3.86 cum. examples 217513, speed 7043.46 words/sec, time elapsed 11115.05 sec\n","epoch 21, iter 31800, avg. loss 24.11, avg. ppl 3.94 cum. examples 230313, speed 7135.35 words/sec, time elapsed 11146.59 sec\n","epoch 21, iter 31900, avg. loss 24.47, avg. ppl 3.98 cum. examples 243113, speed 7088.17 words/sec, time elapsed 11178.58 sec\n","epoch 21, iter 32000, avg. loss 24.07, avg. ppl 3.95 cum. examples 255913, speed 7065.72 words/sec, time elapsed 11210.30 sec\n","epoch 21, iter 32000, cum. loss 24.02, cum. ppl 3.90 cum. examples 255913\n","begin validation ...\n","validation: iter 32000, dev. ppl 22.586454\n","hit patience 5\n","hit #2 trial\n","load previously best model and decay learning rate to 0.000250\n","restore parameters of the optimizers\n","epoch 21, iter 32100, avg. loss 28.14, avg. ppl 4.92 cum. examples 12800, speed 6876.83 words/sec, time elapsed 11243.19 sec\n","epoch 21, iter 32200, avg. loss 28.16, avg. ppl 4.90 cum. examples 25600, speed 7184.57 words/sec, time elapsed 11274.75 sec\n","epoch 22, iter 32300, avg. loss 27.83, avg. ppl 4.87 cum. examples 38313, speed 7112.12 words/sec, time elapsed 11306.17 sec\n","epoch 22, iter 32400, avg. loss 28.19, avg. ppl 4.89 cum. examples 51113, speed 7091.39 words/sec, time elapsed 11338.22 sec\n","epoch 22, iter 32500, avg. loss 27.91, avg. ppl 4.89 cum. examples 63913, speed 7087.10 words/sec, time elapsed 11369.96 sec\n","epoch 22, iter 32600, avg. loss 27.83, avg. ppl 4.87 cum. examples 76713, speed 6993.96 words/sec, time elapsed 11402.15 sec\n","epoch 22, iter 32700, avg. loss 28.52, avg. ppl 4.93 cum. examples 89513, speed 7128.05 words/sec, time elapsed 11434.24 sec\n","epoch 22, iter 32800, avg. loss 28.26, avg. ppl 4.93 cum. examples 102313, speed 7149.51 words/sec, time elapsed 11465.95 sec\n","epoch 22, iter 32900, avg. loss 27.91, avg. ppl 4.90 cum. examples 115113, speed 7080.16 words/sec, time elapsed 11497.68 sec\n","epoch 22, iter 33000, avg. loss 27.89, avg. ppl 4.91 cum. examples 127913, speed 7065.19 words/sec, time elapsed 11529.44 sec\n","epoch 22, iter 33100, avg. loss 28.14, avg. ppl 4.88 cum. examples 140713, speed 7112.92 words/sec, time elapsed 11561.37 sec\n","epoch 22, iter 33200, avg. loss 28.24, avg. ppl 4.93 cum. examples 153513, speed 7016.61 words/sec, time elapsed 11593.66 sec\n","epoch 22, iter 33300, avg. loss 27.90, avg. ppl 4.95 cum. examples 166313, speed 7068.61 words/sec, time elapsed 11625.23 sec\n","epoch 22, iter 33400, avg. loss 28.35, avg. ppl 5.01 cum. examples 179113, speed 7102.41 words/sec, time elapsed 11656.95 sec\n","epoch 22, iter 33500, avg. loss 28.84, avg. ppl 5.02 cum. examples 191913, speed 7156.06 words/sec, time elapsed 11688.91 sec\n","epoch 22, iter 33600, avg. loss 28.59, avg. ppl 5.05 cum. examples 204713, speed 6995.91 words/sec, time elapsed 11721.21 sec\n","epoch 22, iter 33700, avg. loss 28.74, avg. ppl 5.06 cum. examples 217513, speed 7065.04 words/sec, time elapsed 11753.34 sec\n","epoch 22, iter 33800, avg. loss 28.66, avg. ppl 5.01 cum. examples 230313, speed 7097.07 words/sec, time elapsed 11785.41 sec\n","epoch 22, iter 33900, avg. loss 28.41, avg. ppl 4.98 cum. examples 243113, speed 7045.74 words/sec, time elapsed 11817.55 sec\n","epoch 23, iter 34000, avg. loss 27.44, avg. ppl 4.74 cum. examples 255826, speed 7063.99 words/sec, time elapsed 11849.29 sec\n","epoch 23, iter 34000, cum. loss 28.20, cum. ppl 4.93 cum. examples 255826\n","begin validation ...\n","validation: iter 34000, dev. ppl 18.316745\n","hit patience 1\n","epoch 23, iter 34100, avg. loss 26.35, avg. ppl 4.48 cum. examples 12800, speed 6920.54 words/sec, time elapsed 11881.79 sec\n","epoch 23, iter 34200, avg. loss 27.16, avg. ppl 4.60 cum. examples 25600, speed 7142.50 words/sec, time elapsed 11913.71 sec\n","epoch 23, iter 34300, avg. loss 26.61, avg. ppl 4.55 cum. examples 38400, speed 7035.45 words/sec, time elapsed 11945.65 sec\n","epoch 23, iter 34400, avg. loss 27.30, avg. ppl 4.61 cum. examples 51200, speed 7092.35 words/sec, time elapsed 11977.89 sec\n","epoch 23, iter 34500, avg. loss 26.92, avg. ppl 4.61 cum. examples 64000, speed 7104.47 words/sec, time elapsed 12009.63 sec\n","epoch 23, iter 34600, avg. loss 27.26, avg. ppl 4.64 cum. examples 76800, speed 7078.80 words/sec, time elapsed 12041.73 sec\n","epoch 23, iter 34700, avg. loss 27.52, avg. ppl 4.71 cum. examples 89600, speed 6986.99 words/sec, time elapsed 12074.28 sec\n","epoch 23, iter 34800, avg. loss 27.60, avg. ppl 4.77 cum. examples 102400, speed 7176.91 words/sec, time elapsed 12105.78 sec\n","epoch 23, iter 34900, avg. loss 27.43, avg. ppl 4.75 cum. examples 115200, speed 7039.66 words/sec, time elapsed 12137.78 sec\n","epoch 23, iter 35000, avg. loss 26.95, avg. ppl 4.65 cum. examples 128000, speed 7117.35 words/sec, time elapsed 12169.30 sec\n","epoch 23, iter 35100, avg. loss 27.28, avg. ppl 4.67 cum. examples 140800, speed 7090.38 words/sec, time elapsed 12201.25 sec\n","epoch 23, iter 35200, avg. loss 27.62, avg. ppl 4.73 cum. examples 153600, speed 7064.57 words/sec, time elapsed 12233.43 sec\n","epoch 23, iter 35300, avg. loss 27.40, avg. ppl 4.72 cum. examples 166400, speed 7221.49 words/sec, time elapsed 12264.73 sec\n","epoch 23, iter 35400, avg. loss 27.78, avg. ppl 4.79 cum. examples 179200, speed 7136.98 words/sec, time elapsed 12296.55 sec\n","epoch 23, iter 35500, avg. loss 27.85, avg. ppl 4.85 cum. examples 192000, speed 7101.35 words/sec, time elapsed 12328.35 sec\n","epoch 23, iter 35600, avg. loss 27.32, avg. ppl 4.74 cum. examples 204800, speed 7110.01 words/sec, time elapsed 12359.95 sec\n","epoch 24, iter 35700, avg. loss 26.37, avg. ppl 4.45 cum. examples 217513, speed 7061.71 words/sec, time elapsed 12391.75 sec\n","epoch 24, iter 35800, avg. loss 25.82, avg. ppl 4.34 cum. examples 230313, speed 7103.31 words/sec, time elapsed 12423.42 sec\n","epoch 24, iter 35900, avg. loss 26.01, avg. ppl 4.33 cum. examples 243113, speed 7156.27 words/sec, time elapsed 12455.15 sec\n","epoch 24, iter 36000, avg. loss 26.15, avg. ppl 4.40 cum. examples 255913, speed 7052.01 words/sec, time elapsed 12487.21 sec\n","epoch 24, iter 36000, cum. loss 27.04, cum. ppl 4.62 cum. examples 255913\n","begin validation ...\n","validation: iter 36000, dev. ppl 19.360724\n","hit patience 2\n","epoch 24, iter 36100, avg. loss 26.46, avg. ppl 4.40 cum. examples 12800, speed 7084.41 words/sec, time elapsed 12519.47 sec\n","epoch 24, iter 36200, avg. loss 26.19, avg. ppl 4.40 cum. examples 25600, speed 7084.63 words/sec, time elapsed 12551.41 sec\n","epoch 24, iter 36300, avg. loss 26.08, avg. ppl 4.44 cum. examples 38400, speed 7161.40 words/sec, time elapsed 12582.67 sec\n","epoch 24, iter 36400, avg. loss 26.36, avg. ppl 4.47 cum. examples 51200, speed 7072.25 words/sec, time elapsed 12614.52 sec\n","epoch 24, iter 36500, avg. loss 26.21, avg. ppl 4.42 cum. examples 64000, speed 7088.63 words/sec, time elapsed 12646.37 sec\n","epoch 24, iter 36600, avg. loss 26.75, avg. ppl 4.48 cum. examples 76800, speed 7179.83 words/sec, time elapsed 12678.16 sec\n","epoch 24, iter 36700, avg. loss 26.66, avg. ppl 4.53 cum. examples 89600, speed 7116.27 words/sec, time elapsed 12709.90 sec\n","epoch 24, iter 36800, avg. loss 26.88, avg. ppl 4.54 cum. examples 102400, speed 7112.69 words/sec, time elapsed 12741.87 sec\n","epoch 24, iter 36900, avg. loss 26.49, avg. ppl 4.51 cum. examples 115200, speed 7069.33 words/sec, time elapsed 12773.71 sec\n","epoch 24, iter 37000, avg. loss 27.11, avg. ppl 4.61 cum. examples 128000, speed 7101.07 words/sec, time elapsed 12805.68 sec\n","epoch 24, iter 37100, avg. loss 26.81, avg. ppl 4.55 cum. examples 140800, speed 7082.10 words/sec, time elapsed 12837.66 sec\n","epoch 24, iter 37200, avg. loss 26.66, avg. ppl 4.58 cum. examples 153600, speed 7071.23 words/sec, time elapsed 12869.36 sec\n","epoch 24, iter 37300, avg. loss 26.98, avg. ppl 4.58 cum. examples 166400, speed 7109.13 words/sec, time elapsed 12901.29 sec\n","epoch 25, iter 37400, avg. loss 25.35, avg. ppl 4.23 cum. examples 179113, speed 7174.28 words/sec, time elapsed 12932.45 sec\n","epoch 25, iter 37500, avg. loss 25.40, avg. ppl 4.18 cum. examples 191913, speed 7019.27 words/sec, time elapsed 12964.85 sec\n","epoch 25, iter 37600, avg. loss 25.19, avg. ppl 4.16 cum. examples 204713, speed 7173.53 words/sec, time elapsed 12996.38 sec\n","epoch 25, iter 37700, avg. loss 25.40, avg. ppl 4.25 cum. examples 217513, speed 7081.14 words/sec, time elapsed 13028.11 sec\n","epoch 25, iter 37800, avg. loss 25.53, avg. ppl 4.22 cum. examples 230313, speed 7150.48 words/sec, time elapsed 13059.83 sec\n","epoch 25, iter 37900, avg. loss 25.47, avg. ppl 4.19 cum. examples 243113, speed 7116.68 words/sec, time elapsed 13091.78 sec\n","epoch 25, iter 38000, avg. loss 25.48, avg. ppl 4.26 cum. examples 255913, speed 7015.51 words/sec, time elapsed 13123.85 sec\n","epoch 25, iter 38000, cum. loss 26.17, cum. ppl 4.40 cum. examples 255913\n","begin validation ...\n","validation: iter 38000, dev. ppl 19.822481\n","hit patience 3\n","epoch 25, iter 38100, avg. loss 25.56, avg. ppl 4.27 cum. examples 12800, speed 6909.77 words/sec, time elapsed 13156.47 sec\n","epoch 25, iter 38200, avg. loss 25.69, avg. ppl 4.26 cum. examples 25600, speed 6983.02 words/sec, time elapsed 13188.97 sec\n","epoch 25, iter 38300, avg. loss 25.69, avg. ppl 4.28 cum. examples 38400, speed 7187.76 words/sec, time elapsed 13220.46 sec\n","epoch 25, iter 38400, avg. loss 25.84, avg. ppl 4.36 cum. examples 51200, speed 7157.84 words/sec, time elapsed 13251.83 sec\n","epoch 25, iter 38500, avg. loss 25.99, avg. ppl 4.34 cum. examples 64000, speed 7029.48 words/sec, time elapsed 13284.07 sec\n","epoch 25, iter 38600, avg. loss 26.26, avg. ppl 4.37 cum. examples 76800, speed 7231.06 words/sec, time elapsed 13315.56 sec\n","epoch 25, iter 38700, avg. loss 26.15, avg. ppl 4.38 cum. examples 89600, speed 7089.70 words/sec, time elapsed 13347.53 sec\n","epoch 25, iter 38800, avg. loss 25.96, avg. ppl 4.32 cum. examples 102400, speed 7242.11 words/sec, time elapsed 13378.86 sec\n","epoch 25, iter 38900, avg. loss 25.98, avg. ppl 4.36 cum. examples 115200, speed 7137.24 words/sec, time elapsed 13410.53 sec\n","epoch 25, iter 39000, avg. loss 26.21, avg. ppl 4.42 cum. examples 128000, speed 7036.38 words/sec, time elapsed 13442.62 sec\n","epoch 26, iter 39100, avg. loss 24.60, avg. ppl 4.03 cum. examples 140713, speed 7092.32 words/sec, time elapsed 13474.24 sec\n","epoch 26, iter 39200, avg. loss 24.67, avg. ppl 4.03 cum. examples 153513, speed 7152.92 words/sec, time elapsed 13505.95 sec\n","epoch 26, iter 39300, avg. loss 24.62, avg. ppl 4.04 cum. examples 166313, speed 7146.31 words/sec, time elapsed 13537.52 sec\n","epoch 26, iter 39400, avg. loss 24.56, avg. ppl 4.03 cum. examples 179113, speed 7152.08 words/sec, time elapsed 13569.03 sec\n","epoch 26, iter 39500, avg. loss 24.53, avg. ppl 4.05 cum. examples 191913, speed 7016.37 words/sec, time elapsed 13601.03 sec\n","epoch 26, iter 39600, avg. loss 24.55, avg. ppl 4.03 cum. examples 204713, speed 7149.90 words/sec, time elapsed 13632.54 sec\n","epoch 26, iter 39700, avg. loss 24.79, avg. ppl 4.07 cum. examples 217513, speed 7043.29 words/sec, time elapsed 13664.62 sec\n","epoch 26, iter 39800, avg. loss 25.33, avg. ppl 4.14 cum. examples 230313, speed 7108.72 words/sec, time elapsed 13696.71 sec\n","epoch 26, iter 39900, avg. loss 24.95, avg. ppl 4.13 cum. examples 243113, speed 7126.78 words/sec, time elapsed 13728.31 sec\n","epoch 26, iter 40000, avg. loss 24.99, avg. ppl 4.12 cum. examples 255913, speed 7079.24 words/sec, time elapsed 13760.20 sec\n","epoch 26, iter 40000, cum. loss 25.35, cum. ppl 4.20 cum. examples 255913\n","begin validation ...\n","validation: iter 40000, dev. ppl 20.605680\n","hit patience 4\n","epoch 26, iter 40100, avg. loss 25.42, avg. ppl 4.21 cum. examples 12800, speed 6958.78 words/sec, time elapsed 13792.77 sec\n","epoch 26, iter 40200, avg. loss 25.07, avg. ppl 4.13 cum. examples 25600, speed 7011.34 words/sec, time elapsed 13825.02 sec\n","epoch 26, iter 40300, avg. loss 25.60, avg. ppl 4.24 cum. examples 38400, speed 7179.38 words/sec, time elapsed 13856.63 sec\n","epoch 26, iter 40400, avg. loss 25.50, avg. ppl 4.19 cum. examples 51200, speed 7157.68 words/sec, time elapsed 13888.44 sec\n","epoch 26, iter 40500, avg. loss 25.29, avg. ppl 4.20 cum. examples 64000, speed 7076.21 words/sec, time elapsed 13920.31 sec\n","epoch 26, iter 40600, avg. loss 25.70, avg. ppl 4.22 cum. examples 76800, speed 7141.33 words/sec, time elapsed 13952.28 sec\n","epoch 26, iter 40700, avg. loss 25.41, avg. ppl 4.22 cum. examples 89600, speed 7044.17 words/sec, time elapsed 13984.34 sec\n","epoch 27, iter 40800, avg. loss 23.71, avg. ppl 3.81 cum. examples 102313, speed 7084.15 words/sec, time elapsed 14016.14 sec\n","epoch 27, iter 40900, avg. loss 23.73, avg. ppl 3.88 cum. examples 115113, speed 7137.09 words/sec, time elapsed 14047.51 sec\n","epoch 27, iter 41000, avg. loss 24.19, avg. ppl 3.91 cum. examples 127913, speed 7174.81 words/sec, time elapsed 14079.14 sec\n","epoch 27, iter 41100, avg. loss 24.01, avg. ppl 3.90 cum. examples 140713, speed 7148.94 words/sec, time elapsed 14110.75 sec\n","epoch 27, iter 41200, avg. loss 24.25, avg. ppl 3.91 cum. examples 153513, speed 7191.27 words/sec, time elapsed 14142.38 sec\n","epoch 27, iter 41300, avg. loss 24.21, avg. ppl 3.93 cum. examples 166313, speed 7010.60 words/sec, time elapsed 14174.66 sec\n","epoch 27, iter 41400, avg. loss 24.43, avg. ppl 3.96 cum. examples 179113, speed 7018.33 words/sec, time elapsed 14207.03 sec\n","epoch 27, iter 41500, avg. loss 24.41, avg. ppl 3.96 cum. examples 191913, speed 7129.42 words/sec, time elapsed 14238.86 sec\n","epoch 27, iter 41600, avg. loss 24.27, avg. ppl 3.98 cum. examples 204713, speed 7145.48 words/sec, time elapsed 14270.34 sec\n","epoch 27, iter 41700, avg. loss 24.17, avg. ppl 3.95 cum. examples 217513, speed 7143.81 words/sec, time elapsed 14301.84 sec\n","epoch 27, iter 41800, avg. loss 24.56, avg. ppl 4.03 cum. examples 230313, speed 7100.40 words/sec, time elapsed 14333.58 sec\n","epoch 27, iter 41900, avg. loss 24.69, avg. ppl 4.03 cum. examples 243113, speed 7189.85 words/sec, time elapsed 14365.10 sec\n","epoch 27, iter 42000, avg. loss 24.44, avg. ppl 4.00 cum. examples 255913, speed 7042.92 words/sec, time elapsed 14397.16 sec\n","epoch 27, iter 42000, cum. loss 24.65, cum. ppl 4.03 cum. examples 255913\n","begin validation ...\n","validation: iter 42000, dev. ppl 21.407319\n","hit patience 5\n","hit #3 trial\n","load previously best model and decay learning rate to 0.000125\n","restore parameters of the optimizers\n","epoch 27, iter 42100, avg. loss 28.08, avg. ppl 4.94 cum. examples 12800, speed 6722.12 words/sec, time elapsed 14430.64 sec\n","epoch 27, iter 42200, avg. loss 28.08, avg. ppl 4.89 cum. examples 25600, speed 7196.92 words/sec, time elapsed 14462.10 sec\n","epoch 27, iter 42300, avg. loss 28.00, avg. ppl 4.84 cum. examples 38400, speed 7114.38 words/sec, time elapsed 14494.03 sec\n","epoch 28, iter 42400, avg. loss 28.32, avg. ppl 4.92 cum. examples 51113, speed 7039.12 words/sec, time elapsed 14526.12 sec\n","epoch 28, iter 42500, avg. loss 27.75, avg. ppl 4.82 cum. examples 63913, speed 7041.44 words/sec, time elapsed 14558.18 sec\n","epoch 28, iter 42600, avg. loss 27.92, avg. ppl 4.86 cum. examples 76713, speed 7094.65 words/sec, time elapsed 14590.05 sec\n","epoch 28, iter 42700, avg. loss 27.67, avg. ppl 4.82 cum. examples 89513, speed 7126.35 words/sec, time elapsed 14621.64 sec\n","epoch 28, iter 42800, avg. loss 27.85, avg. ppl 4.81 cum. examples 102313, speed 7117.37 words/sec, time elapsed 14653.53 sec\n","epoch 28, iter 42900, avg. loss 27.53, avg. ppl 4.80 cum. examples 115113, speed 7079.80 words/sec, time elapsed 14685.28 sec\n","epoch 28, iter 43000, avg. loss 27.57, avg. ppl 4.80 cum. examples 127913, speed 7106.83 words/sec, time elapsed 14716.94 sec\n","epoch 28, iter 43100, avg. loss 27.82, avg. ppl 4.84 cum. examples 140713, speed 7206.12 words/sec, time elapsed 14748.28 sec\n","epoch 28, iter 43200, avg. loss 27.83, avg. ppl 4.84 cum. examples 153513, speed 7025.86 words/sec, time elapsed 14780.42 sec\n","epoch 28, iter 43300, avg. loss 27.92, avg. ppl 4.82 cum. examples 166313, speed 7115.96 words/sec, time elapsed 14812.34 sec\n","epoch 28, iter 43400, avg. loss 28.00, avg. ppl 4.84 cum. examples 179113, speed 7076.81 words/sec, time elapsed 14844.46 sec\n","epoch 28, iter 43500, avg. loss 27.93, avg. ppl 4.86 cum. examples 191913, speed 7149.77 words/sec, time elapsed 14876.07 sec\n","epoch 28, iter 43600, avg. loss 27.96, avg. ppl 4.87 cum. examples 204713, speed 7092.30 words/sec, time elapsed 14907.97 sec\n","epoch 28, iter 43700, avg. loss 28.49, avg. ppl 4.92 cum. examples 217513, speed 7090.03 words/sec, time elapsed 14940.24 sec\n","epoch 28, iter 43800, avg. loss 28.08, avg. ppl 4.92 cum. examples 230313, speed 7160.21 words/sec, time elapsed 14971.74 sec\n","epoch 28, iter 43900, avg. loss 27.93, avg. ppl 4.84 cum. examples 243113, speed 7212.80 words/sec, time elapsed 15003.17 sec\n","epoch 28, iter 44000, avg. loss 27.96, avg. ppl 4.87 cum. examples 255913, speed 7098.80 words/sec, time elapsed 15035.01 sec\n","epoch 28, iter 44000, cum. loss 27.93, cum. ppl 4.86 cum. examples 255913\n","begin validation ...\n","validation: iter 44000, dev. ppl 18.180376\n","hit patience 1\n","epoch 29, iter 44100, avg. loss 27.84, avg. ppl 4.86 cum. examples 12713, speed 6981.46 words/sec, time elapsed 15067.08 sec\n","epoch 29, iter 44200, avg. loss 26.97, avg. ppl 4.62 cum. examples 25513, speed 7111.78 words/sec, time elapsed 15098.78 sec\n","epoch 29, iter 44300, avg. loss 26.85, avg. ppl 4.61 cum. examples 38313, speed 7016.61 words/sec, time elapsed 15130.83 sec\n","epoch 29, iter 44400, avg. loss 26.87, avg. ppl 4.59 cum. examples 51113, speed 7237.01 words/sec, time elapsed 15162.03 sec\n","epoch 29, iter 44500, avg. loss 27.12, avg. ppl 4.60 cum. examples 63913, speed 7086.09 words/sec, time elapsed 15194.15 sec\n","epoch 29, iter 44600, avg. loss 27.39, avg. ppl 4.68 cum. examples 76713, speed 7199.89 words/sec, time elapsed 15225.72 sec\n","epoch 29, iter 44700, avg. loss 27.06, avg. ppl 4.61 cum. examples 89513, speed 7135.65 words/sec, time elapsed 15257.47 sec\n","epoch 29, iter 44800, avg. loss 26.95, avg. ppl 4.63 cum. examples 102313, speed 7012.10 words/sec, time elapsed 15289.59 sec\n","epoch 29, iter 44900, avg. loss 26.90, avg. ppl 4.63 cum. examples 115113, speed 7085.57 words/sec, time elapsed 15321.28 sec\n","epoch 29, iter 45000, avg. loss 27.09, avg. ppl 4.64 cum. examples 127913, speed 7248.84 words/sec, time elapsed 15352.44 sec\n","epoch 29, iter 45100, avg. loss 27.38, avg. ppl 4.71 cum. examples 140713, speed 7157.84 words/sec, time elapsed 15384.04 sec\n","epoch 29, iter 45200, avg. loss 27.60, avg. ppl 4.75 cum. examples 153513, speed 7193.26 words/sec, time elapsed 15415.57 sec\n","epoch 29, iter 45300, avg. loss 27.35, avg. ppl 4.68 cum. examples 166313, speed 7054.64 words/sec, time elapsed 15447.70 sec\n","epoch 29, iter 45400, avg. loss 27.35, avg. ppl 4.69 cum. examples 179113, speed 7053.22 words/sec, time elapsed 15479.81 sec\n","epoch 29, iter 45500, avg. loss 27.66, avg. ppl 4.71 cum. examples 191913, speed 7161.23 words/sec, time elapsed 15511.73 sec\n","epoch 29, iter 45600, avg. loss 27.37, avg. ppl 4.76 cum. examples 204713, speed 7045.95 words/sec, time elapsed 15543.61 sec\n","epoch 29, iter 45700, avg. loss 27.61, avg. ppl 4.76 cum. examples 217513, speed 7094.01 words/sec, time elapsed 15575.54 sec\n","epoch 30, iter 45800, avg. loss 27.15, avg. ppl 4.64 cum. examples 230226, speed 7038.39 words/sec, time elapsed 15607.48 sec\n","epoch 30, iter 45900, avg. loss 26.73, avg. ppl 4.51 cum. examples 243026, speed 7179.38 words/sec, time elapsed 15639.10 sec\n","epoch 30, iter 46000, avg. loss 26.68, avg. ppl 4.47 cum. examples 255826, speed 7133.76 words/sec, time elapsed 15671.07 sec\n","epoch 30, iter 46000, cum. loss 27.20, cum. ppl 4.66 cum. examples 255826\n","begin validation ...\n","validation: iter 46000, dev. ppl 18.513578\n","hit patience 2\n","reached maximum number of epochs!\n","epoch 30, iter 46100, avg. loss 26.82, avg. ppl 4.54 cum. examples 12800, speed 7059.96 words/sec, time elapsed 15703.19 sec\n","epoch 30, iter 46200, avg. loss 26.37, avg. ppl 4.51 cum. examples 25600, speed 7012.34 words/sec, time elapsed 15735.16 sec\n","epoch 30, iter 46300, avg. loss 26.79, avg. ppl 4.54 cum. examples 38400, speed 7121.18 words/sec, time elapsed 15767.00 sec\n","epoch 30, iter 46400, avg. loss 26.34, avg. ppl 4.48 cum. examples 51200, speed 7144.98 words/sec, time elapsed 15798.47 sec\n","epoch 30, iter 46500, avg. loss 26.43, avg. ppl 4.49 cum. examples 64000, speed 7014.35 words/sec, time elapsed 15830.57 sec\n","epoch 30, iter 46600, avg. loss 26.62, avg. ppl 4.51 cum. examples 76800, speed 7105.64 words/sec, time elapsed 15862.42 sec\n","epoch 30, iter 46700, avg. loss 26.79, avg. ppl 4.53 cum. examples 89600, speed 7110.15 words/sec, time elapsed 15894.34 sec\n","epoch 30, iter 46800, avg. loss 26.67, avg. ppl 4.53 cum. examples 102400, speed 7061.56 words/sec, time elapsed 15926.33 sec\n","epoch 30, iter 46900, avg. loss 26.71, avg. ppl 4.54 cum. examples 115200, speed 7140.25 words/sec, time elapsed 15957.96 sec\n","epoch 30, iter 47000, avg. loss 26.81, avg. ppl 4.58 cum. examples 128000, speed 7166.28 words/sec, time elapsed 15989.44 sec\n","epoch 30, iter 47100, avg. loss 26.98, avg. ppl 4.59 cum. examples 140800, speed 7048.72 words/sec, time elapsed 16021.57 sec\n","epoch 30, iter 47200, avg. loss 26.62, avg. ppl 4.51 cum. examples 153600, speed 7092.35 words/sec, time elapsed 16053.48 sec\n","epoch 30, iter 47300, avg. loss 26.91, avg. ppl 4.57 cum. examples 166400, speed 7035.16 words/sec, time elapsed 16085.68 sec\n","epoch 30, iter 47400, avg. loss 27.01, avg. ppl 4.59 cum. examples 179200, speed 7197.54 words/sec, time elapsed 16117.20 sec\n","epoch 31, iter 47500, avg. loss 26.48, avg. ppl 4.50 cum. examples 191913, speed 7121.00 words/sec, time elapsed 16148.65 sec\n","epoch 31, iter 47600, avg. loss 25.59, avg. ppl 4.28 cum. examples 204713, speed 7146.18 words/sec, time elapsed 16180.15 sec\n","epoch 31, iter 47700, avg. loss 26.04, avg. ppl 4.34 cum. examples 217513, speed 7191.64 words/sec, time elapsed 16211.72 sec\n","epoch 31, iter 47800, avg. loss 26.08, avg. ppl 4.35 cum. examples 230313, speed 7111.89 words/sec, time elapsed 16243.66 sec\n","epoch 31, iter 47900, avg. loss 26.19, avg. ppl 4.42 cum. examples 243113, speed 7190.48 words/sec, time elapsed 16275.01 sec\n","epoch 31, iter 48000, avg. loss 26.56, avg. ppl 4.42 cum. examples 255913, speed 7237.78 words/sec, time elapsed 16306.61 sec\n","epoch 31, iter 48000, cum. loss 26.54, cum. ppl 4.49 cum. examples 255913\n","begin validation ...\n","validation: iter 48000, dev. ppl 18.932082\n","hit patience 3\n","epoch 31, iter 48100, avg. loss 26.11, avg. ppl 4.40 cum. examples 12800, speed 6980.84 words/sec, time elapsed 16338.94 sec\n","epoch 31, iter 48200, avg. loss 25.67, avg. ppl 4.38 cum. examples 25600, speed 7062.30 words/sec, time elapsed 16370.42 sec\n","epoch 31, iter 48300, avg. loss 26.27, avg. ppl 4.40 cum. examples 38400, speed 7058.77 words/sec, time elapsed 16402.55 sec\n","epoch 31, iter 48400, avg. loss 26.49, avg. ppl 4.45 cum. examples 51200, speed 7176.07 words/sec, time elapsed 16434.21 sec\n","epoch 31, iter 48500, avg. loss 26.22, avg. ppl 4.40 cum. examples 64000, speed 7138.78 words/sec, time elapsed 16465.94 sec\n","epoch 31, iter 48600, avg. loss 26.30, avg. ppl 4.41 cum. examples 76800, speed 7084.49 words/sec, time elapsed 16497.95 sec\n","epoch 31, iter 48700, avg. loss 26.18, avg. ppl 4.43 cum. examples 89600, speed 7043.93 words/sec, time elapsed 16529.91 sec\n","epoch 31, iter 48800, avg. loss 26.44, avg. ppl 4.44 cum. examples 102400, speed 7118.22 words/sec, time elapsed 16561.83 sec\n","epoch 31, iter 48900, avg. loss 26.23, avg. ppl 4.46 cum. examples 115200, speed 7069.49 words/sec, time elapsed 16593.59 sec\n","epoch 31, iter 49000, avg. loss 26.72, avg. ppl 4.50 cum. examples 128000, speed 7147.82 words/sec, time elapsed 16625.40 sec\n","epoch 31, iter 49100, avg. loss 26.47, avg. ppl 4.49 cum. examples 140800, speed 7035.49 words/sec, time elapsed 16657.46 sec\n","epoch 32, iter 49200, avg. loss 26.30, avg. ppl 4.38 cum. examples 153513, speed 7091.20 words/sec, time elapsed 16689.36 sec\n","epoch 32, iter 49300, avg. loss 25.84, avg. ppl 4.26 cum. examples 166313, speed 7132.28 words/sec, time elapsed 16721.38 sec\n","epoch 32, iter 49400, avg. loss 25.21, avg. ppl 4.19 cum. examples 179113, speed 7118.53 words/sec, time elapsed 16753.02 sec\n","epoch 32, iter 49500, avg. loss 25.74, avg. ppl 4.26 cum. examples 191913, speed 7058.71 words/sec, time elapsed 16785.24 sec\n","epoch 32, iter 49600, avg. loss 25.82, avg. ppl 4.30 cum. examples 204713, speed 7087.48 words/sec, time elapsed 16817.23 sec\n","epoch 32, iter 49700, avg. loss 25.33, avg. ppl 4.26 cum. examples 217513, speed 7105.06 words/sec, time elapsed 16848.69 sec\n","epoch 32, iter 49800, avg. loss 25.56, avg. ppl 4.24 cum. examples 230313, speed 7085.29 words/sec, time elapsed 16880.64 sec\n","epoch 32, iter 49900, avg. loss 25.71, avg. ppl 4.31 cum. examples 243113, speed 7044.36 words/sec, time elapsed 16912.64 sec\n","epoch 32, iter 50000, avg. loss 25.48, avg. ppl 4.27 cum. examples 255913, speed 7116.99 words/sec, time elapsed 16944.22 sec\n","epoch 32, iter 50000, cum. loss 26.00, cum. ppl 4.36 cum. examples 255913\n","begin validation ...\n","validation: iter 50000, dev. ppl 19.381128\n","hit patience 4\n","epoch 32, iter 50100, avg. loss 25.76, avg. ppl 4.33 cum. examples 12800, speed 6921.24 words/sec, time elapsed 16976.75 sec\n","epoch 32, iter 50200, avg. loss 25.64, avg. ppl 4.23 cum. examples 25600, speed 7146.20 words/sec, time elapsed 17008.60 sec\n","epoch 32, iter 50300, avg. loss 25.81, avg. ppl 4.29 cum. examples 38400, speed 7053.35 words/sec, time elapsed 17040.79 sec\n","epoch 32, iter 50400, avg. loss 25.75, avg. ppl 4.35 cum. examples 51200, speed 7085.45 words/sec, time elapsed 17072.43 sec\n","epoch 32, iter 50500, avg. loss 26.06, avg. ppl 4.38 cum. examples 64000, speed 7142.65 words/sec, time elapsed 17104.06 sec\n","epoch 32, iter 50600, avg. loss 26.11, avg. ppl 4.36 cum. examples 76800, speed 7157.99 words/sec, time elapsed 17135.78 sec\n","epoch 32, iter 50700, avg. loss 26.11, avg. ppl 4.41 cum. examples 89600, speed 7059.15 words/sec, time elapsed 17167.67 sec\n","epoch 32, iter 50800, avg. loss 26.20, avg. ppl 4.38 cum. examples 102400, speed 7067.67 words/sec, time elapsed 17199.79 sec\n","epoch 33, iter 50900, avg. loss 26.02, avg. ppl 4.34 cum. examples 115113, speed 7105.41 words/sec, time elapsed 17231.50 sec\n","epoch 33, iter 51000, avg. loss 24.89, avg. ppl 4.10 cum. examples 127913, speed 7170.87 words/sec, time elapsed 17262.98 sec\n","epoch 33, iter 51100, avg. loss 25.05, avg. ppl 4.14 cum. examples 140713, speed 7104.75 words/sec, time elapsed 17294.72 sec\n","epoch 33, iter 51200, avg. loss 25.26, avg. ppl 4.17 cum. examples 153513, speed 7042.21 words/sec, time elapsed 17326.88 sec\n","epoch 33, iter 51300, avg. loss 25.37, avg. ppl 4.20 cum. examples 166313, speed 7079.95 words/sec, time elapsed 17358.86 sec\n","epoch 33, iter 51400, avg. loss 25.04, avg. ppl 4.14 cum. examples 179113, speed 7079.08 words/sec, time elapsed 17390.72 sec\n","epoch 33, iter 51500, avg. loss 25.10, avg. ppl 4.13 cum. examples 191913, speed 7169.24 words/sec, time elapsed 17422.31 sec\n","epoch 33, iter 51600, avg. loss 25.64, avg. ppl 4.21 cum. examples 204713, speed 7107.95 words/sec, time elapsed 17454.41 sec\n","epoch 33, iter 51700, avg. loss 25.59, avg. ppl 4.22 cum. examples 217513, speed 7157.58 words/sec, time elapsed 17486.21 sec\n","epoch 33, iter 51800, avg. loss 25.48, avg. ppl 4.22 cum. examples 230313, speed 7089.77 words/sec, time elapsed 17518.18 sec\n","epoch 33, iter 51900, avg. loss 25.22, avg. ppl 4.23 cum. examples 243113, speed 7112.07 words/sec, time elapsed 17549.68 sec\n","epoch 33, iter 52000, avg. loss 25.54, avg. ppl 4.25 cum. examples 255913, speed 7107.41 words/sec, time elapsed 17581.44 sec\n","epoch 33, iter 52000, cum. loss 25.58, cum. ppl 4.25 cum. examples 255913\n","begin validation ...\n","validation: iter 52000, dev. ppl 19.784565\n","hit patience 5\n","hit #4 trial\n","load previously best model and decay learning rate to 0.000063\n","restore parameters of the optimizers\n","epoch 33, iter 52100, avg. loss 28.19, avg. ppl 4.97 cum. examples 12800, speed 6769.47 words/sec, time elapsed 17614.71 sec\n","epoch 33, iter 52200, avg. loss 27.73, avg. ppl 4.88 cum. examples 25600, speed 7023.53 words/sec, time elapsed 17646.60 sec\n","epoch 33, iter 52300, avg. loss 28.26, avg. ppl 4.87 cum. examples 38400, speed 7205.47 words/sec, time elapsed 17678.32 sec\n","epoch 33, iter 52400, avg. loss 28.12, avg. ppl 4.85 cum. examples 51200, speed 7086.94 words/sec, time elapsed 17710.49 sec\n","epoch 33, iter 52500, avg. loss 27.79, avg. ppl 4.83 cum. examples 64000, speed 7073.84 words/sec, time elapsed 17742.40 sec\n","epoch 34, iter 52600, avg. loss 27.54, avg. ppl 4.77 cum. examples 76713, speed 7077.65 words/sec, time elapsed 17774.05 sec\n","epoch 34, iter 52700, avg. loss 27.41, avg. ppl 4.77 cum. examples 89513, speed 7159.77 words/sec, time elapsed 17805.42 sec\n","epoch 34, iter 52800, avg. loss 27.98, avg. ppl 4.81 cum. examples 102313, speed 7203.29 words/sec, time elapsed 17837.08 sec\n","epoch 34, iter 52900, avg. loss 28.03, avg. ppl 4.87 cum. examples 115113, speed 7201.53 words/sec, time elapsed 17868.53 sec\n","epoch 34, iter 53000, avg. loss 27.70, avg. ppl 4.81 cum. examples 127913, speed 7122.36 words/sec, time elapsed 17900.24 sec\n","epoch 34, iter 53100, avg. loss 27.93, avg. ppl 4.84 cum. examples 140713, speed 7079.13 words/sec, time elapsed 17932.26 sec\n","epoch 34, iter 53200, avg. loss 27.48, avg. ppl 4.80 cum. examples 153513, speed 7042.63 words/sec, time elapsed 17964.11 sec\n","epoch 34, iter 53300, avg. loss 27.67, avg. ppl 4.80 cum. examples 166313, speed 7087.68 words/sec, time elapsed 17995.98 sec\n","epoch 34, iter 53400, avg. loss 28.03, avg. ppl 4.85 cum. examples 179113, speed 7153.38 words/sec, time elapsed 18027.76 sec\n","epoch 34, iter 53500, avg. loss 27.86, avg. ppl 4.83 cum. examples 191913, speed 7137.39 words/sec, time elapsed 18059.47 sec\n","epoch 34, iter 53600, avg. loss 27.97, avg. ppl 4.84 cum. examples 204713, speed 7036.60 words/sec, time elapsed 18091.75 sec\n","epoch 34, iter 53700, avg. loss 27.85, avg. ppl 4.82 cum. examples 217513, speed 7080.29 words/sec, time elapsed 18123.75 sec\n","epoch 34, iter 53800, avg. loss 27.66, avg. ppl 4.87 cum. examples 230313, speed 7071.71 words/sec, time elapsed 18155.39 sec\n","epoch 34, iter 53900, avg. loss 27.89, avg. ppl 4.84 cum. examples 243113, speed 7073.25 words/sec, time elapsed 18187.41 sec\n","epoch 34, iter 54000, avg. loss 27.91, avg. ppl 4.82 cum. examples 255913, speed 7152.01 words/sec, time elapsed 18219.17 sec\n","epoch 34, iter 54000, cum. loss 27.85, cum. ppl 4.84 cum. examples 255913\n","begin validation ...\n","validation: iter 54000, dev. ppl 17.960190\n","hit patience 1\n","epoch 34, iter 54100, avg. loss 27.80, avg. ppl 4.84 cum. examples 12800, speed 6939.03 words/sec, time elapsed 18251.67 sec\n","epoch 34, iter 54200, avg. loss 27.73, avg. ppl 4.76 cum. examples 25600, speed 7250.58 words/sec, time elapsed 18283.04 sec\n","epoch 35, iter 54300, avg. loss 27.58, avg. ppl 4.77 cum. examples 38313, speed 7071.69 words/sec, time elapsed 18314.79 sec\n","epoch 35, iter 54400, avg. loss 27.05, avg. ppl 4.70 cum. examples 51113, speed 7035.44 words/sec, time elapsed 18346.58 sec\n","epoch 35, iter 54500, avg. loss 27.08, avg. ppl 4.68 cum. examples 63913, speed 7141.61 words/sec, time elapsed 18378.05 sec\n","epoch 35, iter 54600, avg. loss 27.22, avg. ppl 4.67 cum. examples 76713, speed 7061.63 words/sec, time elapsed 18410.06 sec\n","epoch 35, iter 54700, avg. loss 27.42, avg. ppl 4.68 cum. examples 89513, speed 7227.38 words/sec, time elapsed 18441.54 sec\n","epoch 35, iter 54800, avg. loss 27.15, avg. ppl 4.69 cum. examples 102313, speed 7061.02 words/sec, time elapsed 18473.39 sec\n","epoch 35, iter 54900, avg. loss 27.41, avg. ppl 4.72 cum. examples 115113, speed 7176.20 words/sec, time elapsed 18504.89 sec\n","epoch 35, iter 55000, avg. loss 27.26, avg. ppl 4.71 cum. examples 127913, speed 7083.79 words/sec, time elapsed 18536.68 sec\n","epoch 35, iter 55100, avg. loss 27.20, avg. ppl 4.66 cum. examples 140713, speed 7054.25 words/sec, time elapsed 18568.77 sec\n","epoch 35, iter 55200, avg. loss 27.75, avg. ppl 4.74 cum. examples 153513, speed 7078.41 words/sec, time elapsed 18601.02 sec\n","epoch 35, iter 55300, avg. loss 27.22, avg. ppl 4.69 cum. examples 166313, speed 7119.16 words/sec, time elapsed 18632.70 sec\n","epoch 35, iter 55400, avg. loss 27.58, avg. ppl 4.72 cum. examples 179113, speed 7145.03 words/sec, time elapsed 18664.53 sec\n","epoch 35, iter 55500, avg. loss 27.34, avg. ppl 4.68 cum. examples 191913, speed 7060.85 words/sec, time elapsed 18696.64 sec\n","epoch 35, iter 55600, avg. loss 27.45, avg. ppl 4.76 cum. examples 204713, speed 7168.78 words/sec, time elapsed 18728.07 sec\n","epoch 35, iter 55700, avg. loss 27.60, avg. ppl 4.75 cum. examples 217513, speed 7049.07 words/sec, time elapsed 18760.24 sec\n","epoch 35, iter 55800, avg. loss 27.46, avg. ppl 4.70 cum. examples 230313, speed 7135.78 words/sec, time elapsed 18792.08 sec\n","epoch 35, iter 55900, avg. loss 27.51, avg. ppl 4.70 cum. examples 243113, speed 7036.48 words/sec, time elapsed 18824.41 sec\n","epoch 36, iter 56000, avg. loss 27.14, avg. ppl 4.64 cum. examples 255826, speed 7141.38 words/sec, time elapsed 18855.88 sec\n","epoch 36, iter 56000, cum. loss 27.40, cum. ppl 4.71 cum. examples 255826\n","begin validation ...\n","validation: iter 56000, dev. ppl 18.172647\n","hit patience 2\n","epoch 36, iter 56100, avg. loss 26.64, avg. ppl 4.59 cum. examples 12800, speed 6869.49 words/sec, time elapsed 18888.46 sec\n","epoch 36, iter 56200, avg. loss 26.96, avg. ppl 4.61 cum. examples 25600, speed 7145.78 words/sec, time elapsed 18920.05 sec\n","epoch 36, iter 56300, avg. loss 26.90, avg. ppl 4.60 cum. examples 38400, speed 7227.52 words/sec, time elapsed 18951.25 sec\n","epoch 36, iter 56400, avg. loss 27.21, avg. ppl 4.62 cum. examples 51200, speed 7128.08 words/sec, time elapsed 18983.20 sec\n","epoch 36, iter 56500, avg. loss 26.83, avg. ppl 4.56 cum. examples 64000, speed 7128.90 words/sec, time elapsed 19014.94 sec\n","epoch 36, iter 56600, avg. loss 27.00, avg. ppl 4.62 cum. examples 76800, speed 7116.99 words/sec, time elapsed 19046.68 sec\n","epoch 36, iter 56700, avg. loss 27.10, avg. ppl 4.60 cum. examples 89600, speed 7126.18 words/sec, time elapsed 19078.59 sec\n","epoch 36, iter 56800, avg. loss 27.04, avg. ppl 4.59 cum. examples 102400, speed 7159.36 words/sec, time elapsed 19110.29 sec\n","epoch 36, iter 56900, avg. loss 27.39, avg. ppl 4.62 cum. examples 115200, speed 7067.08 words/sec, time elapsed 19142.71 sec\n","epoch 36, iter 57000, avg. loss 26.99, avg. ppl 4.59 cum. examples 128000, speed 7077.28 words/sec, time elapsed 19174.74 sec\n","epoch 36, iter 57100, avg. loss 26.96, avg. ppl 4.64 cum. examples 140800, speed 7141.87 words/sec, time elapsed 19206.24 sec\n","epoch 36, iter 57200, avg. loss 27.04, avg. ppl 4.64 cum. examples 153600, speed 7064.84 words/sec, time elapsed 19238.14 sec\n","epoch 36, iter 57300, avg. loss 27.05, avg. ppl 4.64 cum. examples 166400, speed 7170.34 words/sec, time elapsed 19269.59 sec\n","epoch 36, iter 57400, avg. loss 27.21, avg. ppl 4.62 cum. examples 179200, speed 7053.38 words/sec, time elapsed 19301.86 sec\n","epoch 36, iter 57500, avg. loss 27.23, avg. ppl 4.63 cum. examples 192000, speed 7202.93 words/sec, time elapsed 19333.45 sec\n","epoch 36, iter 57600, avg. loss 26.92, avg. ppl 4.66 cum. examples 204800, speed 7107.59 words/sec, time elapsed 19364.97 sec\n","epoch 37, iter 57700, avg. loss 26.57, avg. ppl 4.56 cum. examples 217513, speed 7085.16 words/sec, time elapsed 19396.38 sec\n","epoch 37, iter 57800, avg. loss 26.83, avg. ppl 4.60 cum. examples 230313, speed 7163.24 words/sec, time elapsed 19427.80 sec\n","epoch 37, iter 57900, avg. loss 26.68, avg. ppl 4.50 cum. examples 243113, speed 7046.09 words/sec, time elapsed 19460.05 sec\n","epoch 37, iter 58000, avg. loss 26.55, avg. ppl 4.47 cum. examples 255913, speed 7124.94 words/sec, time elapsed 19491.89 sec\n","epoch 37, iter 58000, cum. loss 26.95, cum. ppl 4.60 cum. examples 255913\n","begin validation ...\n","validation: iter 58000, dev. ppl 18.370829\n","hit patience 3\n","epoch 37, iter 58100, avg. loss 26.11, avg. ppl 4.43 cum. examples 12800, speed 6987.56 words/sec, time elapsed 19524.04 sec\n","epoch 37, iter 58200, avg. loss 26.90, avg. ppl 4.53 cum. examples 25600, speed 7087.62 words/sec, time elapsed 19556.20 sec\n","epoch 37, iter 58300, avg. loss 26.89, avg. ppl 4.58 cum. examples 38400, speed 7107.26 words/sec, time elapsed 19588.04 sec\n","epoch 37, iter 58400, avg. loss 26.94, avg. ppl 4.56 cum. examples 51200, speed 7203.99 words/sec, time elapsed 19619.56 sec\n","epoch 37, iter 58500, avg. loss 27.05, avg. ppl 4.55 cum. examples 64000, speed 7123.59 words/sec, time elapsed 19651.65 sec\n","epoch 37, iter 58600, avg. loss 27.02, avg. ppl 4.58 cum. examples 76800, speed 7164.88 words/sec, time elapsed 19683.39 sec\n","epoch 37, iter 58700, avg. loss 26.67, avg. ppl 4.53 cum. examples 89600, speed 7178.93 words/sec, time elapsed 19714.85 sec\n","epoch 37, iter 58800, avg. loss 26.54, avg. ppl 4.55 cum. examples 102400, speed 7167.06 words/sec, time elapsed 19746.12 sec\n","epoch 37, iter 58900, avg. loss 26.77, avg. ppl 4.53 cum. examples 115200, speed 7036.67 words/sec, time elapsed 19778.33 sec\n","epoch 37, iter 59000, avg. loss 26.55, avg. ppl 4.54 cum. examples 128000, speed 7058.59 words/sec, time elapsed 19810.16 sec\n","epoch 37, iter 59100, avg. loss 26.73, avg. ppl 4.54 cum. examples 140800, speed 7047.01 words/sec, time elapsed 19842.23 sec\n","epoch 37, iter 59200, avg. loss 26.84, avg. ppl 4.54 cum. examples 153600, speed 7176.04 words/sec, time elapsed 19873.87 sec\n","epoch 37, iter 59300, avg. loss 26.60, avg. ppl 4.55 cum. examples 166400, speed 7111.03 words/sec, time elapsed 19905.46 sec\n","epoch 38, iter 59400, avg. loss 26.14, avg. ppl 4.42 cum. examples 179113, speed 7102.11 words/sec, time elapsed 19936.94 sec\n","epoch 38, iter 59500, avg. loss 26.56, avg. ppl 4.48 cum. examples 191913, speed 7117.61 words/sec, time elapsed 19968.80 sec\n","epoch 38, iter 59600, avg. loss 26.04, avg. ppl 4.39 cum. examples 204713, speed 7083.07 words/sec, time elapsed 20000.60 sec\n","epoch 38, iter 59700, avg. loss 25.76, avg. ppl 4.39 cum. examples 217513, speed 7077.70 words/sec, time elapsed 20032.08 sec\n","epoch 38, iter 59800, avg. loss 26.88, avg. ppl 4.53 cum. examples 230313, speed 7057.49 words/sec, time elapsed 20064.34 sec\n","epoch 38, iter 59900, avg. loss 26.56, avg. ppl 4.47 cum. examples 243113, speed 7175.06 words/sec, time elapsed 20095.99 sec\n","epoch 38, iter 60000, avg. loss 26.49, avg. ppl 4.48 cum. examples 255913, speed 7029.13 words/sec, time elapsed 20128.16 sec\n","epoch 38, iter 60000, cum. loss 26.60, cum. ppl 4.51 cum. examples 255913\n","begin validation ...\n","validation: iter 60000, dev. ppl 18.669490\n","hit patience 4\n","epoch 38, iter 60100, avg. loss 26.39, avg. ppl 4.47 cum. examples 12800, speed 6994.29 words/sec, time elapsed 20160.40 sec\n","epoch 38, iter 60200, avg. loss 26.55, avg. ppl 4.49 cum. examples 25600, speed 7089.86 words/sec, time elapsed 20192.34 sec\n","epoch 38, iter 60300, avg. loss 26.62, avg. ppl 4.48 cum. examples 38400, speed 7215.02 words/sec, time elapsed 20223.85 sec\n","epoch 38, iter 60400, avg. loss 26.42, avg. ppl 4.45 cum. examples 51200, speed 7174.00 words/sec, time elapsed 20255.44 sec\n","epoch 38, iter 60500, avg. loss 26.43, avg. ppl 4.44 cum. examples 64000, speed 7124.79 words/sec, time elapsed 20287.29 sec\n","epoch 38, iter 60600, avg. loss 26.64, avg. ppl 4.49 cum. examples 76800, speed 7122.03 words/sec, time elapsed 20319.16 sec\n","epoch 38, iter 60700, avg. loss 26.56, avg. ppl 4.48 cum. examples 89600, speed 7066.68 words/sec, time elapsed 20351.25 sec\n","epoch 38, iter 60800, avg. loss 26.33, avg. ppl 4.46 cum. examples 102400, speed 7177.81 words/sec, time elapsed 20382.66 sec\n","epoch 38, iter 60900, avg. loss 26.85, avg. ppl 4.51 cum. examples 115200, speed 7112.81 words/sec, time elapsed 20414.72 sec\n","epoch 38, iter 61000, avg. loss 26.56, avg. ppl 4.54 cum. examples 128000, speed 7146.82 words/sec, time elapsed 20446.17 sec\n","epoch 39, iter 61100, avg. loss 26.27, avg. ppl 4.43 cum. examples 140713, speed 7154.22 words/sec, time elapsed 20477.53 sec\n","epoch 39, iter 61200, avg. loss 26.23, avg. ppl 4.35 cum. examples 153513, speed 7168.58 words/sec, time elapsed 20509.38 sec\n","epoch 39, iter 61300, avg. loss 26.23, avg. ppl 4.39 cum. examples 166313, speed 7087.11 words/sec, time elapsed 20541.43 sec\n","epoch 39, iter 61400, avg. loss 26.38, avg. ppl 4.41 cum. examples 179113, speed 7126.80 words/sec, time elapsed 20573.36 sec\n","epoch 39, iter 61500, avg. loss 25.84, avg. ppl 4.36 cum. examples 191913, speed 7144.03 words/sec, time elapsed 20604.79 sec\n","epoch 39, iter 61600, avg. loss 26.03, avg. ppl 4.40 cum. examples 204713, speed 7125.66 words/sec, time elapsed 20636.34 sec\n","epoch 39, iter 61700, avg. loss 26.04, avg. ppl 4.38 cum. examples 217513, speed 7098.40 words/sec, time elapsed 20668.13 sec\n","epoch 39, iter 61800, avg. loss 26.02, avg. ppl 4.38 cum. examples 230313, speed 7104.40 words/sec, time elapsed 20699.89 sec\n","epoch 39, iter 61900, avg. loss 26.23, avg. ppl 4.41 cum. examples 243113, speed 7023.39 words/sec, time elapsed 20732.10 sec\n","epoch 39, iter 62000, avg. loss 26.25, avg. ppl 4.41 cum. examples 255913, speed 7031.65 words/sec, time elapsed 20764.31 sec\n","epoch 39, iter 62000, cum. loss 26.34, cum. ppl 4.44 cum. examples 255913\n","begin validation ...\n","validation: iter 62000, dev. ppl 18.848884\n","hit patience 5\n","hit #5 trial\n","early stop!\n","load previously best model and decay learning rate to 0.000031\n","restore parameters of the optimizers\n","epoch 39, iter 62100, avg. loss 28.16, avg. ppl 4.92 cum. examples 12800, speed 6835.81 words/sec, time elapsed 20797.39 sec\n","epoch 39, iter 62200, avg. loss 28.02, avg. ppl 4.89 cum. examples 25600, speed 7164.28 words/sec, time elapsed 20828.93 sec\n","epoch 39, iter 62300, avg. loss 28.06, avg. ppl 4.91 cum. examples 38400, speed 7080.10 words/sec, time elapsed 20860.82 sec\n","epoch 39, iter 62400, avg. loss 28.12, avg. ppl 4.93 cum. examples 51200, speed 7195.70 words/sec, time elapsed 20892.19 sec\n","epoch 39, iter 62500, avg. loss 27.69, avg. ppl 4.84 cum. examples 64000, speed 7040.97 words/sec, time elapsed 20924.12 sec\n","epoch 39, iter 62600, avg. loss 28.02, avg. ppl 4.86 cum. examples 76800, speed 7227.04 words/sec, time elapsed 20955.49 sec\n","epoch 39, iter 62700, avg. loss 27.78, avg. ppl 4.79 cum. examples 89600, speed 7106.79 words/sec, time elapsed 20987.44 sec\n","epoch 40, iter 62800, avg. loss 27.61, avg. ppl 4.80 cum. examples 102313, speed 7171.06 words/sec, time elapsed 21018.65 sec\n","epoch 40, iter 62900, avg. loss 28.04, avg. ppl 4.88 cum. examples 115113, speed 7060.29 words/sec, time elapsed 21050.70 sec\n","epoch 40, iter 63000, avg. loss 28.13, avg. ppl 4.85 cum. examples 127913, speed 7142.86 words/sec, time elapsed 21082.61 sec\n","epoch 40, iter 63100, avg. loss 27.71, avg. ppl 4.77 cum. examples 140713, speed 7158.84 words/sec, time elapsed 21114.32 sec\n","epoch 40, iter 63200, avg. loss 27.43, avg. ppl 4.79 cum. examples 153513, speed 7138.58 words/sec, time elapsed 21145.74 sec\n","epoch 40, iter 63300, avg. loss 27.82, avg. ppl 4.84 cum. examples 166313, speed 7118.11 words/sec, time elapsed 21177.48 sec\n","epoch 40, iter 63400, avg. loss 27.52, avg. ppl 4.76 cum. examples 179113, speed 7118.69 words/sec, time elapsed 21209.18 sec\n","epoch 40, iter 63500, avg. loss 27.77, avg. ppl 4.83 cum. examples 191913, speed 7127.38 words/sec, time elapsed 21240.83 sec\n","epoch 40, iter 63600, avg. loss 27.75, avg. ppl 4.81 cum. examples 204713, speed 7142.97 words/sec, time elapsed 21272.48 sec\n","epoch 40, iter 63700, avg. loss 28.13, avg. ppl 4.89 cum. examples 217513, speed 7176.81 words/sec, time elapsed 21304.10 sec\n","epoch 40, iter 63800, avg. loss 28.12, avg. ppl 4.84 cum. examples 230313, speed 7115.44 words/sec, time elapsed 21336.17 sec\n","epoch 40, iter 63900, avg. loss 27.84, avg. ppl 4.84 cum. examples 243113, speed 6998.91 words/sec, time elapsed 21368.47 sec\n","epoch 40, iter 64000, avg. loss 27.98, avg. ppl 4.88 cum. examples 255913, speed 7151.29 words/sec, time elapsed 21400.07 sec\n","epoch 40, iter 64000, cum. loss 27.89, cum. ppl 4.85 cum. examples 255913\n","begin validation ...\n","validation: iter 64000, dev. ppl 17.882665\n","hit patience 1\n","epoch 40, iter 64100, avg. loss 27.63, avg. ppl 4.81 cum. examples 12800, speed 6920.50 words/sec, time elapsed 21432.61 sec\n","epoch 40, iter 64200, avg. loss 27.52, avg. ppl 4.76 cum. examples 25600, speed 7161.64 words/sec, time elapsed 21464.13 sec\n","epoch 40, iter 64300, avg. loss 28.15, avg. ppl 4.88 cum. examples 38400, speed 7035.29 words/sec, time elapsed 21496.46 sec\n","epoch 40, iter 64400, avg. loss 27.76, avg. ppl 4.81 cum. examples 51200, speed 7055.32 words/sec, time elapsed 21528.52 sec\n","epoch 41, iter 64500, avg. loss 27.36, avg. ppl 4.71 cum. examples 63913, speed 6998.87 words/sec, time elapsed 21560.58 sec\n","epoch 41, iter 64600, avg. loss 27.59, avg. ppl 4.75 cum. examples 76713, speed 7093.07 words/sec, time elapsed 21592.52 sec\n","epoch 41, iter 64700, avg. loss 27.21, avg. ppl 4.70 cum. examples 89513, speed 7054.06 words/sec, time elapsed 21624.41 sec\n","epoch 41, iter 64800, avg. loss 27.43, avg. ppl 4.72 cum. examples 102313, speed 7067.67 words/sec, time elapsed 21656.45 sec\n","epoch 41, iter 64900, avg. loss 27.47, avg. ppl 4.72 cum. examples 115113, speed 7181.24 words/sec, time elapsed 21687.99 sec\n","epoch 41, iter 65000, avg. loss 27.51, avg. ppl 4.73 cum. examples 127913, speed 7155.46 words/sec, time elapsed 21719.65 sec\n","epoch 41, iter 65100, avg. loss 27.33, avg. ppl 4.73 cum. examples 140713, speed 7058.30 words/sec, time elapsed 21751.56 sec\n","epoch 41, iter 65200, avg. loss 27.67, avg. ppl 4.76 cum. examples 153513, speed 7103.47 words/sec, time elapsed 21783.49 sec\n","epoch 41, iter 65300, avg. loss 27.53, avg. ppl 4.73 cum. examples 166313, speed 7207.59 words/sec, time elapsed 21814.96 sec\n","epoch 41, iter 65400, avg. loss 27.14, avg. ppl 4.73 cum. examples 179113, speed 7049.17 words/sec, time elapsed 21846.69 sec\n","epoch 41, iter 65500, avg. loss 27.56, avg. ppl 4.79 cum. examples 191913, speed 7092.61 words/sec, time elapsed 21878.42 sec\n","epoch 41, iter 65600, avg. loss 27.32, avg. ppl 4.73 cum. examples 204713, speed 7085.80 words/sec, time elapsed 21910.18 sec\n","epoch 41, iter 65700, avg. loss 27.61, avg. ppl 4.76 cum. examples 217513, speed 7088.94 words/sec, time elapsed 21942.13 sec\n","epoch 41, iter 65800, avg. loss 27.58, avg. ppl 4.76 cum. examples 230313, speed 7001.97 words/sec, time elapsed 21974.43 sec\n","epoch 41, iter 65900, avg. loss 27.69, avg. ppl 4.78 cum. examples 243113, speed 7114.77 words/sec, time elapsed 22006.28 sec\n","epoch 41, iter 66000, avg. loss 27.76, avg. ppl 4.77 cum. examples 255913, speed 7194.12 words/sec, time elapsed 22037.88 sec\n","epoch 41, iter 66000, cum. loss 27.54, cum. ppl 4.76 cum. examples 255913\n","begin validation ...\n","validation: iter 66000, dev. ppl 17.960662\n","hit patience 2\n","epoch 42, iter 66100, avg. loss 27.94, avg. ppl 4.77 cum. examples 12713, speed 6975.57 words/sec, time elapsed 22070.49 sec\n","epoch 42, iter 66200, avg. loss 27.29, avg. ppl 4.67 cum. examples 25513, speed 7012.45 words/sec, time elapsed 22102.80 sec\n","epoch 42, iter 66300, avg. loss 27.21, avg. ppl 4.67 cum. examples 38313, speed 7068.59 words/sec, time elapsed 22134.79 sec\n","epoch 42, iter 66400, avg. loss 26.99, avg. ppl 4.65 cum. examples 51113, speed 7078.89 words/sec, time elapsed 22166.55 sec\n","epoch 42, iter 66500, avg. loss 27.22, avg. ppl 4.65 cum. examples 63913, speed 7064.65 words/sec, time elapsed 22198.62 sec\n","epoch 42, iter 66600, avg. loss 27.60, avg. ppl 4.69 cum. examples 76713, speed 7161.52 words/sec, time elapsed 22230.55 sec\n","epoch 42, iter 66700, avg. loss 27.07, avg. ppl 4.69 cum. examples 89513, speed 7118.04 words/sec, time elapsed 22262.05 sec\n","epoch 42, iter 66800, avg. loss 27.35, avg. ppl 4.69 cum. examples 102313, speed 7012.10 words/sec, time elapsed 22294.37 sec\n","epoch 42, iter 66900, avg. loss 27.47, avg. ppl 4.73 cum. examples 115113, speed 7219.47 words/sec, time elapsed 22325.70 sec\n","epoch 42, iter 67000, avg. loss 27.54, avg. ppl 4.74 cum. examples 127913, speed 7097.32 words/sec, time elapsed 22357.61 sec\n","epoch 42, iter 67100, avg. loss 27.08, avg. ppl 4.66 cum. examples 140713, speed 7114.58 words/sec, time elapsed 22389.29 sec\n","epoch 42, iter 67200, avg. loss 27.39, avg. ppl 4.70 cum. examples 153513, speed 7164.12 words/sec, time elapsed 22420.92 sec\n","epoch 42, iter 67300, avg. loss 27.50, avg. ppl 4.71 cum. examples 166313, speed 7170.85 words/sec, time elapsed 22452.59 sec\n","epoch 42, iter 67400, avg. loss 27.50, avg. ppl 4.74 cum. examples 179113, speed 7227.04 words/sec, time elapsed 22483.91 sec\n","epoch 42, iter 67500, avg. loss 27.18, avg. ppl 4.68 cum. examples 191913, speed 7141.04 words/sec, time elapsed 22515.48 sec\n","epoch 42, iter 67600, avg. loss 27.27, avg. ppl 4.69 cum. examples 204713, speed 7106.07 words/sec, time elapsed 22547.26 sec\n","epoch 42, iter 67700, avg. loss 27.15, avg. ppl 4.65 cum. examples 217513, speed 7166.04 words/sec, time elapsed 22578.82 sec\n","epoch 43, iter 67800, avg. loss 27.27, avg. ppl 4.70 cum. examples 230226, speed 7093.86 words/sec, time elapsed 22610.41 sec\n","epoch 43, iter 67900, avg. loss 26.96, avg. ppl 4.61 cum. examples 243026, speed 7118.96 words/sec, time elapsed 22642.13 sec\n","epoch 43, iter 68000, avg. loss 27.13, avg. ppl 4.61 cum. examples 255826, speed 7161.11 words/sec, time elapsed 22673.84 sec\n","epoch 43, iter 68000, cum. loss 27.31, cum. ppl 4.68 cum. examples 255826\n","begin validation ...\n","validation: iter 68000, dev. ppl 18.134009\n","hit patience 3\n","epoch 43, iter 68100, avg. loss 27.10, avg. ppl 4.64 cum. examples 12800, speed 7027.77 words/sec, time elapsed 22706.02 sec\n","epoch 43, iter 68200, avg. loss 27.09, avg. ppl 4.62 cum. examples 25600, speed 7148.55 words/sec, time elapsed 22737.71 sec\n","epoch 43, iter 68300, avg. loss 27.15, avg. ppl 4.63 cum. examples 38400, speed 7088.00 words/sec, time elapsed 22769.71 sec\n","epoch 43, iter 68400, avg. loss 27.02, avg. ppl 4.59 cum. examples 51200, speed 7097.40 words/sec, time elapsed 22801.67 sec\n","epoch 43, iter 68500, avg. loss 26.89, avg. ppl 4.61 cum. examples 64000, speed 7119.34 words/sec, time elapsed 22833.33 sec\n","epoch 43, iter 68600, avg. loss 26.82, avg. ppl 4.64 cum. examples 76800, speed 7103.44 words/sec, time elapsed 22864.82 sec\n","epoch 43, iter 68700, avg. loss 27.11, avg. ppl 4.64 cum. examples 89600, speed 7183.11 words/sec, time elapsed 22896.30 sec\n","epoch 43, iter 68800, avg. loss 27.12, avg. ppl 4.63 cum. examples 102400, speed 7060.44 words/sec, time elapsed 22928.37 sec\n","epoch 43, iter 68900, avg. loss 26.97, avg. ppl 4.61 cum. examples 115200, speed 7136.98 words/sec, time elapsed 22960.04 sec\n","epoch 43, iter 69000, avg. loss 27.30, avg. ppl 4.67 cum. examples 128000, speed 7099.49 words/sec, time elapsed 22991.97 sec\n","epoch 43, iter 69100, avg. loss 27.17, avg. ppl 4.70 cum. examples 140800, speed 7107.19 words/sec, time elapsed 23023.60 sec\n","epoch 43, iter 69200, avg. loss 27.34, avg. ppl 4.64 cum. examples 153600, speed 7161.47 words/sec, time elapsed 23055.46 sec\n","epoch 43, iter 69300, avg. loss 27.22, avg. ppl 4.64 cum. examples 166400, speed 7196.09 words/sec, time elapsed 23087.02 sec\n","epoch 43, iter 69400, avg. loss 27.51, avg. ppl 4.69 cum. examples 179200, speed 7157.55 words/sec, time elapsed 23118.87 sec\n","epoch 44, iter 69500, avg. loss 26.87, avg. ppl 4.65 cum. examples 191913, speed 7030.99 words/sec, time elapsed 23150.50 sec\n","epoch 44, iter 69600, avg. loss 26.97, avg. ppl 4.63 cum. examples 204713, speed 7027.65 words/sec, time elapsed 23182.56 sec\n","epoch 44, iter 69700, avg. loss 27.02, avg. ppl 4.63 cum. examples 217513, speed 7141.65 words/sec, time elapsed 23214.17 sec\n","epoch 44, iter 69800, avg. loss 27.12, avg. ppl 4.60 cum. examples 230313, speed 7102.69 words/sec, time elapsed 23246.18 sec\n","epoch 44, iter 69900, avg. loss 26.61, avg. ppl 4.54 cum. examples 243113, speed 7150.41 words/sec, time elapsed 23277.66 sec\n","epoch 44, iter 70000, avg. loss 26.60, avg. ppl 4.56 cum. examples 255913, speed 7045.57 words/sec, time elapsed 23309.50 sec\n","epoch 44, iter 70000, cum. loss 27.05, cum. ppl 4.63 cum. examples 255913\n","begin validation ...\n","validation: iter 70000, dev. ppl 18.245271\n","hit patience 4\n","epoch 44, iter 70100, avg. loss 27.17, avg. ppl 4.61 cum. examples 12800, speed 7038.70 words/sec, time elapsed 23341.83 sec\n","epoch 44, iter 70200, avg. loss 26.65, avg. ppl 4.56 cum. examples 25600, speed 7106.91 words/sec, time elapsed 23373.46 sec\n","epoch 44, iter 70300, avg. loss 27.01, avg. ppl 4.59 cum. examples 38400, speed 7046.11 words/sec, time elapsed 23405.68 sec\n","epoch 44, iter 70400, avg. loss 27.07, avg. ppl 4.62 cum. examples 51200, speed 7218.65 words/sec, time elapsed 23437.03 sec\n","epoch 44, iter 70500, avg. loss 26.91, avg. ppl 4.58 cum. examples 64000, speed 7133.00 words/sec, time elapsed 23468.74 sec\n","epoch 44, iter 70600, avg. loss 27.34, avg. ppl 4.64 cum. examples 76800, speed 7095.99 words/sec, time elapsed 23500.87 sec\n","epoch 44, iter 70700, avg. loss 26.98, avg. ppl 4.59 cum. examples 89600, speed 7101.97 words/sec, time elapsed 23532.78 sec\n","epoch 44, iter 70800, avg. loss 26.99, avg. ppl 4.61 cum. examples 102400, speed 7105.01 words/sec, time elapsed 23564.58 sec\n","epoch 44, iter 70900, avg. loss 26.87, avg. ppl 4.54 cum. examples 115200, speed 7116.96 words/sec, time elapsed 23596.52 sec\n","epoch 44, iter 71000, avg. loss 26.83, avg. ppl 4.60 cum. examples 128000, speed 7139.25 words/sec, time elapsed 23628.02 sec\n","epoch 44, iter 71100, avg. loss 26.84, avg. ppl 4.58 cum. examples 140800, speed 7130.76 words/sec, time elapsed 23659.70 sec\n","epoch 45, iter 71200, avg. loss 27.07, avg. ppl 4.60 cum. examples 153513, speed 7112.05 words/sec, time elapsed 23691.40 sec\n","epoch 45, iter 71300, avg. loss 26.56, avg. ppl 4.51 cum. examples 166313, speed 7057.67 words/sec, time elapsed 23723.38 sec\n","epoch 45, iter 71400, avg. loss 26.67, avg. ppl 4.53 cum. examples 179113, speed 7149.48 words/sec, time elapsed 23755.00 sec\n","epoch 45, iter 71500, avg. loss 26.61, avg. ppl 4.53 cum. examples 191913, speed 7055.78 words/sec, time elapsed 23786.95 sec\n","epoch 45, iter 71600, avg. loss 26.51, avg. ppl 4.53 cum. examples 204713, speed 7086.27 words/sec, time elapsed 23818.66 sec\n","epoch 45, iter 71700, avg. loss 27.02, avg. ppl 4.58 cum. examples 217513, speed 7137.45 words/sec, time elapsed 23850.48 sec\n","epoch 45, iter 71800, avg. loss 26.92, avg. ppl 4.58 cum. examples 230313, speed 7205.10 words/sec, time elapsed 23881.92 sec\n","epoch 45, iter 71900, avg. loss 26.91, avg. ppl 4.55 cum. examples 243113, speed 7126.74 words/sec, time elapsed 23913.80 sec\n","epoch 45, iter 72000, avg. loss 26.86, avg. ppl 4.55 cum. examples 255913, speed 7150.37 words/sec, time elapsed 23945.53 sec\n","epoch 45, iter 72000, cum. loss 26.89, cum. ppl 4.57 cum. examples 255913\n","begin validation ...\n","validation: iter 72000, dev. ppl 18.344584\n","hit patience 5\n","hit #6 trial\n","load previously best model and decay learning rate to 0.000016\n","restore parameters of the optimizers\n","epoch 45, iter 72100, avg. loss 28.18, avg. ppl 4.91 cum. examples 12800, speed 6820.59 words/sec, time elapsed 23978.76 sec\n","epoch 45, iter 72200, avg. loss 28.17, avg. ppl 4.91 cum. examples 25600, speed 7149.35 words/sec, time elapsed 24010.45 sec\n","epoch 45, iter 72300, avg. loss 27.67, avg. ppl 4.89 cum. examples 38400, speed 7083.10 words/sec, time elapsed 24041.96 sec\n","epoch 45, iter 72400, avg. loss 28.07, avg. ppl 4.87 cum. examples 51200, speed 7180.41 words/sec, time elapsed 24073.57 sec\n","epoch 45, iter 72500, avg. loss 28.35, avg. ppl 4.92 cum. examples 64000, speed 7158.32 words/sec, time elapsed 24105.38 sec\n","epoch 45, iter 72600, avg. loss 28.28, avg. ppl 4.94 cum. examples 76800, speed 7092.33 words/sec, time elapsed 24137.32 sec\n","epoch 45, iter 72700, avg. loss 27.83, avg. ppl 4.88 cum. examples 89600, speed 7070.20 words/sec, time elapsed 24169.10 sec\n","epoch 45, iter 72800, avg. loss 28.08, avg. ppl 4.89 cum. examples 102400, speed 7200.47 words/sec, time elapsed 24200.56 sec\n","epoch 46, iter 72900, avg. loss 28.01, avg. ppl 4.86 cum. examples 115113, speed 7216.60 words/sec, time elapsed 24231.78 sec\n","epoch 46, iter 73000, avg. loss 28.11, avg. ppl 4.90 cum. examples 127913, speed 7129.38 words/sec, time elapsed 24263.54 sec\n","epoch 46, iter 73100, avg. loss 27.91, avg. ppl 4.83 cum. examples 140713, speed 7101.57 words/sec, time elapsed 24295.50 sec\n","epoch 46, iter 73200, avg. loss 27.72, avg. ppl 4.86 cum. examples 153513, speed 7140.07 words/sec, time elapsed 24326.94 sec\n","epoch 46, iter 73300, avg. loss 27.83, avg. ppl 4.80 cum. examples 166313, speed 7132.26 words/sec, time elapsed 24358.78 sec\n","epoch 46, iter 73400, avg. loss 27.85, avg. ppl 4.84 cum. examples 179113, speed 7116.05 words/sec, time elapsed 24390.55 sec\n","epoch 46, iter 73500, avg. loss 27.89, avg. ppl 4.87 cum. examples 191913, speed 7124.32 words/sec, time elapsed 24422.20 sec\n","epoch 46, iter 73600, avg. loss 28.10, avg. ppl 4.84 cum. examples 204713, speed 7044.76 words/sec, time elapsed 24454.57 sec\n","epoch 46, iter 73700, avg. loss 27.73, avg. ppl 4.81 cum. examples 217513, speed 7173.03 words/sec, time elapsed 24486.09 sec\n","epoch 46, iter 73800, avg. loss 27.70, avg. ppl 4.83 cum. examples 230313, speed 7226.50 words/sec, time elapsed 24517.22 sec\n","epoch 46, iter 73900, avg. loss 27.75, avg. ppl 4.80 cum. examples 243113, speed 7157.17 words/sec, time elapsed 24548.87 sec\n","epoch 46, iter 74000, avg. loss 27.75, avg. ppl 4.79 cum. examples 255913, speed 7170.81 words/sec, time elapsed 24580.47 sec\n","epoch 46, iter 74000, cum. loss 27.95, cum. ppl 4.86 cum. examples 255913\n","begin validation ...\n","validation: iter 74000, dev. ppl 17.832330\n","hit patience 1\n","epoch 46, iter 74100, avg. loss 27.93, avg. ppl 4.83 cum. examples 12800, speed 6920.38 words/sec, time elapsed 24613.29 sec\n","epoch 46, iter 74200, avg. loss 27.91, avg. ppl 4.86 cum. examples 25600, speed 7191.96 words/sec, time elapsed 24644.70 sec\n","epoch 46, iter 74300, avg. loss 28.17, avg. ppl 4.91 cum. examples 38400, speed 7157.95 words/sec, time elapsed 24676.34 sec\n","epoch 46, iter 74400, avg. loss 28.07, avg. ppl 4.86 cum. examples 51200, speed 7110.69 words/sec, time elapsed 24708.29 sec\n","epoch 46, iter 74500, avg. loss 27.70, avg. ppl 4.85 cum. examples 64000, speed 7118.60 words/sec, time elapsed 24739.82 sec\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-087d9547d527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# clip gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"zW2HJYn0wXb5","colab_type":"code","colab":{}},"source":["##Test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M6SoJjGPsher","colab_type":"text"},"source":["## Test"]},{"cell_type":"markdown","metadata":{"id":"vSdmzTZ4scw9","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"LTvAYbLFuqUd","colab_type":"code","colab":{}},"source":["def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n","    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n","    @param model (NMT): NMT Model\n","    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n","    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n","    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n","    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    hypotheses = []\n","    with torch.no_grad():\n","        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n","            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n","\n","            hypotheses.append(example_hyps)\n","\n","    if was_training: model.train(was_training)\n","\n","    return hypotheses"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGJbEn1Ew-S-","colab_type":"code","colab":{}},"source":["def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n","    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n","    @param references (List[List[str]]): a list of gold-standard reference target sentences\n","    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n","    @returns bleu_score: corpus-level BLEU score\n","    \"\"\"\n","    if references[0][0] == '<s>':\n","        references = [ref[1:-1] for ref in references]\n","    bleu_score = corpus_bleu([[ref] for ref in references],\n","                             [hyp.value for hyp in hypotheses])\n","    return bleu_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eW9oOGYXVdIa","colab_type":"code","colab":{}},"source":["def decode():\n","    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n","    If the target gold-standard sentences are given, the function also computes\n","    corpus-level BLEU score.\n","    @param args (Dict): args from cmd line\n","    \"\"\"\n","\n","    print(\"load test source sentences\", file=sys.stderr)\n","    test_data_src = read_corpus(test_es, source='src')\n","    \n","    \n","    print(\"load test target sentences\", file=sys.stderr)\n","    test_data_tgt = read_corpus(test_en, source='tgt')\n","\n","    print(\"load trained model\", file=sys.stderr)\n","    model = NMT.load(model_save_path)\n","\n","    #device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n","    if torch.cuda.device_count()>0:\n","        print(\"Transfer to cuda!!\")\n","        model = model.to(torch.device(\"cuda:0\"))\n","\n","    hypotheses = beam_search(model, test_data_src,\n","                             beam_size=5,\n","                             max_decoding_time_step=70)\n","\n","\n","    top_hypotheses = [hyps[0] for hyps in hypotheses]\n","    bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n","    print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n","\n","    with open('test_output.txt', 'w') as f:\n","        for src_sent, hyps in zip(test_data_src, hypotheses):\n","            top_hyp = hyps[0]\n","            hyp_sent = ' '.join(top_hyp.value)\n","            f.write(hyp_sent + '\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xexMtTi-Vcwg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"ba6fe498-e050-424c-bd06-5471bb11870a","executionInfo":{"status":"ok","timestamp":1558449949343,"user_tz":-480,"elapsed":323681,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["decode()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["load test source sentences\n","load test target sentences\n","load trained model\n"],"name":"stderr"},{"output_type":"stream","text":["Transfer to cuda!!\n","Decoding: 100%|| 8064/8064 [05:20<00:00, 25.15it/s]\n"],"name":"stdout"},{"output_type":"stream","text":["Corpus BLEU: 21.989573001713765\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3XnkzmxaAo31","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ge7ElVDwXeP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pE6O6avwwXg-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DebG7DtyO8MF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jS4qJNbvO8OQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lB-e06ftO8Qz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"apPmV31QO8TS","colab_type":"code","colab":{}},"source":["def evaluate_ppl(model, dev_data, batch_size=32):\n","    \"\"\" Evaluate perplexity on dev sentences\n","    @param model (NMT): NMT Model\n","    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (batch size)\n","    @returns ppl (perplixty on dev sentences)\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    cum_loss = 0.\n","    cum_tgt_words = 0.\n","\n","    # no_grad() signals backend to throw away all gradients\n","    with torch.no_grad():\n","        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n","            loss = -model(src_sents, tgt_sents).sum()\n","\n","            cum_loss += loss.item()\n","            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","            cum_tgt_words += tgt_word_num_to_predict\n","\n","        ppl = np.exp(cum_loss / cum_tgt_words)\n","\n","    if was_training:\n","        model.train()\n","\n","    return ppl"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zk_1TvRkSA2G","colab_type":"text"},"source":["## B"]},{"cell_type":"code","metadata":{"id":"tyHJgeMwwXjj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWF3Q5w1wXmS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfJd_QAPwXo6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLAJC9m4wXrX","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N9G69M1QwXuQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}