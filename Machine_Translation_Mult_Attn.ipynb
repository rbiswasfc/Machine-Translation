{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Machine_Translation_Mult_Attn.ipynb","version":"0.3.2","provenance":[{"file_id":"19ewGpyblxKcL2Jk-VOvx2KuIT26fra0j","timestamp":1558506609041}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fqkV84rywSQ-","colab_type":"code","outputId":"9ad68223-d40e-4ebe-c527-286bda9bac10","executionInfo":{"status":"ok","timestamp":1558508448625,"user_tz":-480,"elapsed":3539,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# connect to google drive\n","import os\n","import numpy as np\n","\n","# mount google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PNgE8Zj3UgKm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"04993706-313f-406d-c351-f88497e34480","executionInfo":{"status":"ok","timestamp":1558508455472,"user_tz":-480,"elapsed":10371,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","#!pip install psutil\n","#!pip install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isnâ€™t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm() "],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Gen RAM Free: 12.9 GB  | Proc size: 120.8 MB\n","GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n5cBcqr7wWs0","colab_type":"code","outputId":"5c0ff2e0-88b2-4c6c-d8fe-5703b372f910","executionInfo":{"status":"ok","timestamp":1558508458797,"user_tz":-480,"elapsed":13684,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["root_dir = \"/content/gdrive/My Drive/NLP/MT_ENSP\"\n","os.chdir(root_dir)\n","!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["collect_submission.sh\t   model_embeddings.py\trun.sh\n","Debugging.ipynb\t\t   NMT_model\t\tsanity_check_en_es_data\n","en_es_data\t\t   NMT_model.optim\tsanity_check.py\n","gpu_requirements.txt\t   nmt_model.py\t\ttest_output.txt\n","__init__.py\t\t   __pycache__\t\tutils.py\n","local_env.yml\t\t   README.md\t\tvocab.json\n","Machine_Translation.ipynb  run.py\t\tvocab.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UC9vp7Ef3cqf","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"X74ct1ElwWxv","colab_type":"code","colab":{}},"source":["import math\n","import sys\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.utils\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","\n","from collections import Counter, namedtuple\n","from docopt import docopt\n","from itertools import chain\n","import json\n","from typing import List, Tuple, Dict, Set, Union\n","\n","from docopt import docopt\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Galc-JYk3Xxp","colab_type":"text"},"source":["## Utility Functions"]},{"cell_type":"code","metadata":{"id":"vq13M4PjwW0B","colab_type":"code","colab":{}},"source":["# post-padding for source/target sequences\n","\n","def pad_sents(sents, pad_token):\n","    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n","    @param sents (list[list[str]]): list of sentences, where each sentence\n","                                    is represented as a list of words\n","    @param pad_token (str): padding token\n","    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n","        than the max length sentence are padded out with the pad_token, such that\n","        each sentences in the batch now has equal length.\n","    \"\"\"\n","    sents_padded = []\n","\n","    ### YOUR CODE HERE (~6 Lines)\n","    max_len = max([len(sent) for sent in sents])\n","    for sent in sents:\n","        sent_len = len(sent)\n","        sents_padded.append(sent + (max_len - sent_len) * [pad_token])\n","\n","    ### END YOUR CODE\n","\n","    return sents_padded"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zrhfN0JsxfQO","colab_type":"code","outputId":"51c41652-5a56-4749-b491-674fc2015767","executionInfo":{"status":"ok","timestamp":1558508460455,"user_tz":-480,"elapsed":15307,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# verify padding\n","sents = [['a','clear','day'],['it','is','not','raining','today']]\n","pad_sents(sents,'<Pad>')"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['a', 'clear', 'day', '<Pad>', '<Pad>'],\n"," ['it', 'is', 'not', 'raining', 'today']]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"bowvCZdQwW2k","colab_type":"code","colab":{}},"source":["# read from corpus: vocab building\n","\n","def read_corpus(file_path, source):\n","    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n","    @param file_path (str): path to file containing corpus\n","    @param source (str): \"tgt\" or \"src\" indicating whether text\n","        is of the source language or target language\n","    \"\"\"\n","    data = []\n","    for line in open(file_path):\n","        sent = line.strip().split(' ')\n","        #sent = line.split(' ')\n","        # only append <s> and </s> to the target sentence\n","        if source == 'tgt':\n","            sent = ['<s>'] + sent + ['</s>']\n","        data.append(sent)\n","\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"to7xNtAQyLlT","colab_type":"code","outputId":"43eac242-32e1-4553-b3e9-038c0505b948","executionInfo":{"status":"ok","timestamp":1558508460460,"user_tz":-480,"elapsed":15288,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["# verify if read_corpus is working\n","file_path = 'en_es_data/dev.en'\n","data = read_corpus(file_path, 'src')\n","data[1]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['But',\n"," 'this',\n"," 'understates',\n"," 'the',\n"," 'seriousness',\n"," 'of',\n"," 'this',\n"," 'particular',\n"," 'problem',\n"," '',\n"," 'because',\n"," 'it',\n"," \"doesn't\",\n"," 'show',\n"," 'the',\n"," 'thickness',\n"," 'of',\n"," 'the',\n"," 'ice.']"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"WGuqqY1xwW5J","colab_type":"code","colab":{}},"source":["# generate batches for taining\n","\n","def batch_iter(data, batch_size, shuffle=False):\n","    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n","    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (int): batch size\n","    @param shuffle (boolean): whether to randomly shuffle the dataset\n","    \"\"\"\n","    batch_num = math.ceil(len(data) / batch_size)\n","    index_array = list(range(len(data)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        examples = [data[idx] for idx in indices]\n","\n","        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n","        src_sents = [e[0] for e in examples]\n","        tgt_sents = [e[1] for e in examples]\n","\n","        yield src_sents, tgt_sents"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H62BeCkswW8E","colab_type":"code","outputId":"85218857-7754-4eac-b9d6-5a03a34007f9","executionInfo":{"status":"ok","timestamp":1558508460462,"user_tz":-480,"elapsed":15266,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# check batch_iter\n","es_path = 'en_es_data/dev.es'\n","en_path = 'en_es_data/dev.en'\n","\n","es_data = read_corpus(es_path, source = 'src')\n","en_data = read_corpus(en_path, source = 'tgt')\n","\n","data = list(zip(es_data, en_data))\n","\n","for src, tgt in batch_iter(data[:4],2):\n","  print(src, tgt)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[['El', 'ao', 'pasado', 'proyect', 'estas', 'dos', 'diapositivas', 'para', 'demostrar', 'que', 'la', 'capa', 'de', 'hielo', 'rtico,', 'que', 'durante', 'los', 'ltimos', 'tres', 'millones', 'de', 'aos', 'ha', 'sido', 'del', 'tamao', 'de', 'los', '48', 'estados,', 'se', 'ha', 'reducido', 'en', 'un', '40', 'por', 'ciento.'], ['Pero', 'esto', 'minimiza', 'la', 'seriedad', 'de', 'este', 'problema', 'concreto', 'porque', 'no', 'muestra', 'el', 'grosor', 'del', 'hielo.']] [['<s>', 'Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.', '</s>'], ['<s>', 'But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', \"doesn't\", 'show', 'the', 'thickness', 'of', 'the', 'ice.', '</s>']]\n","[['La', 'capa', 'de', 'hielo', 'rtico', 'es,', 'en', 'cierta', 'forma,', 'el', 'corazn', 'palpitante', 'del', 'sistema', 'climtico', 'global.'], ['Se', 'expande', 'en', 'invierno', 'y', 'se', 'contrae', 'en', 'verano.']] [['<s>', 'The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.', '</s>'], ['<s>', 'It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.', '</s>']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZvQsfbG03TKx","colab_type":"text"},"source":["## Build Vocab"]},{"cell_type":"code","metadata":{"id":"2E2oAQGXwXBW","colab_type":"code","colab":{}},"source":["class VocabEntry(object):\n","    \"\"\" Vocabulary Entry, i.e. structure containing either\n","    src or tgt language terms.\n","    \"\"\"\n","    def __init__(self, word2id=None):\n","        \"\"\" Init VocabEntry Instance.\n","        @param word2id (dict): dictionary mapping words 2 indices\n","        \"\"\"\n","        if word2id:\n","            self.word2id = word2id\n","        else:\n","            self.word2id = dict()\n","            self.word2id['<pad>'] = 0   # Pad Token\n","            self.word2id['<s>'] = 1 # Start Token\n","            self.word2id['</s>'] = 2    # End Token\n","            self.word2id['<unk>'] = 3   # Unknown Token\n","        self.unk_id = self.word2id['<unk>']\n","        self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","    def __getitem__(self, word):\n","        \"\"\" Retrieve word's index. Return the index for the unk\n","        token if the word is out of vocabulary.\n","        @param word (str): word to look up.\n","        @returns index (int): index of word \n","        \"\"\"\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        \"\"\" Check if word is captured by VocabEntry.\n","        @param word (str): word to look up\n","        @returns contains (bool): whether word is contained    \n","        \"\"\"\n","        return word in self.word2id\n","\n","    def __setitem__(self, key, value):\n","        \"\"\" Raise error, if one tries to edit the VocabEntry.\n","        \"\"\"\n","        raise ValueError('vocabulary is readonly')\n","\n","    def __len__(self):\n","        \"\"\" Compute number of words in VocabEntry.\n","        @returns len (int): number of words in VocabEntry\n","        \"\"\"\n","        return len(self.word2id)\n","\n","    def __repr__(self):\n","        \"\"\" Representation of VocabEntry to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocabulary[size=%d]' % len(self)\n","\n","    def id2word(self, wid):\n","        \"\"\" Return mapping of index to word.\n","        @param wid (int): word index\n","        @returns word (str): word corresponding to index\n","        \"\"\"\n","        return self.id2word[wid]\n","\n","    def add(self, word):\n","        \"\"\" Add word to VocabEntry, if it is previously unseen.\n","        @param word (str): word to add to VocabEntry\n","        @return index (int): index that the word has been assigned\n","        \"\"\"\n","        if word not in self:\n","            wid = self.word2id[word] = len(self)\n","            self.id2word[wid] = word\n","            return wid\n","        else:\n","            return self[word]\n","\n","    def words2indices(self, sents):\n","        \"\"\" Convert list of words or list of sentences of words\n","        into list or list of list of indices.\n","        @param sents (list[str] or list[list[str]]): sentence(s) in words\n","        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n","        \"\"\"\n","        if type(sents[0]) == list:\n","            return [[self[w] for w in s] for s in sents]\n","        else:\n","            return [self[w] for w in sents]\n","\n","    def indices2words(self, word_ids):\n","        \"\"\" Convert list of indices into words.\n","        @param word_ids (list[int]): list of word ids\n","        @return sents (list[str]): list of words\n","        \"\"\"\n","        return [self.id2word[w_id] for w_id in word_ids]\n","\n","    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n","        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n","        shorter sentences.\n","\n","        @param sents (List[List[str]]): list of sentences (words)\n","        @param device: device on which to load the tesnor, i.e. CPU or GPU\n","\n","        @returns sents_var: tensor of (max_sentence_length, batch_size)\n","        \"\"\"\n","        word_ids = self.words2indices(sents)\n","        sents_t = pad_sents(word_ids, self['<pad>'])\n","        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n","        return torch.t(sents_var)\n","\n","    @staticmethod\n","    def from_corpus(corpus, size, freq_cutoff=2):\n","        \"\"\" Given a corpus construct a Vocab Entry.\n","        @param corpus (list[str]): corpus of text produced by read_corpus function\n","        @param size (int): # of words in vocabulary\n","        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n","        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n","        \"\"\"\n","        vocab_entry = VocabEntry()\n","        word_freq = Counter(chain(*corpus))\n","        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n","        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n","              .format(len(word_freq), freq_cutoff, len(valid_words)))\n","        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n","        for word in top_k_words:\n","            vocab_entry.add(word)\n","        return vocab_entry"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRxSESD94TVd","colab_type":"code","outputId":"0ac034e6-1c04-45cb-914c-f8de3fd2e07c","executionInfo":{"status":"ok","timestamp":1558508463136,"user_tz":-480,"elapsed":17922,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["# Check Vocab Entry Object\n","lang = VocabEntry()\n","#\n","print('Check if <pad> token is inside vocab:')\n","print('<pad>' in lang) #__contains__#\n","print('Length of vocab:{}'.format(len(lang))) # __len__\n","print('Adding a new word')\n","lang.add('new') # add new entry 'add' method\n","print(lang) #__repr__\n","print('The index of \"new\" is:{}'.format(lang.word2id['new']))\n","## \n","print('Generate a vocab with the from_corpus static method:')\n","en_vocab = VocabEntry.from_corpus(en_data, 100)\n","print('The token for the word \"the\" is:{}'.format(en_vocab.word2id['the']))\n","# \n","print('check to_input_tensor method:')\n","# set device name\n","device = torch.tensor(1).cuda().device\n","\n","temp = en_vocab.to_input_tensor(sents,device)\n","print(temp)\n","# Note: output tensor shape--> (max_len, batch)\n","# len(en_vocab)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Check if <pad> token is inside vocab:\n","True\n","Length of vocab:4\n","Adding a new word\n","Vocabulary[size=5]\n","The index of \"new\" is:4\n","Generate a vocab with the from_corpus static method:\n","number of word types: 3955, number of word types w/ frequency >= 2: 1339\n","The token for the word \"the\" is:5\n","check to_input_tensor method:\n","tensor([[10, 18],\n","        [ 3, 11],\n","        [ 3, 37],\n","        [ 0,  3],\n","        [ 0,  3]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WlZkM2Xm4Nsl","colab_type":"code","colab":{}},"source":["class Vocab(object):\n","    \"\"\" Vocab encapsulating src and target langauges.\n","    \"\"\"\n","    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n","        \"\"\" Init Vocab.\n","        @param src_vocab (VocabEntry): VocabEntry for source language\n","        @param tgt_vocab (VocabEntry): VocabEntry for target language\n","        \"\"\"\n","        self.src = src_vocab\n","        self.tgt = tgt_vocab\n","\n","    @staticmethod\n","    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n","        \"\"\" Build Vocabulary.\n","        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n","        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n","        @param vocab_size (int): Size of vocabulary for both source and target languages\n","        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n","        \"\"\"\n","        assert len(src_sents) == len(tgt_sents)\n","\n","        print('initialize source vocabulary ..')\n","        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n","\n","        print('initialize target vocabulary ..')\n","        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n","\n","        return Vocab(src, tgt)\n","\n","    def save(self, file_path):\n","        \"\"\" Save Vocab to file as JSON dump.\n","        @param file_path (str): file path to vocab file\n","        \"\"\"\n","        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n","\n","    @staticmethod\n","    def load(file_path):\n","        \"\"\" Load vocabulary from JSON dump.\n","        @param file_path (str): file path to vocab file\n","        @returns Vocab object loaded from JSON dump\n","        \"\"\"\n","        entry = json.load(open(file_path, 'r'))\n","        src_word2id = entry['src_word2id']\n","        tgt_word2id = entry['tgt_word2id']\n","\n","        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n","\n","    def __repr__(self):\n","        \"\"\" Representation of Vocab to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFaikyYkAXtT","colab_type":"code","outputId":"7363fe4d-8d1b-4ef8-a876-0c5c333b10c2","executionInfo":{"status":"ok","timestamp":1558508466158,"user_tz":-480,"elapsed":20929,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# Now build source [Spanish] and target [English] vocab\n","\n","train_es = 'en_es_data/train.es'\n","train_en = 'en_es_data/train.en'\n","vocab_file = 'en_es_data/vocab.json'\n","\n","src_sents = read_corpus(train_es, source='src')\n","tgt_sents = read_corpus(train_en, source='tgt')\n","\n","size = 50000\n","freq_cutoff= 2\n","\n","vocab = Vocab.build(src_sents, tgt_sents, size, freq_cutoff)\n","print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n","\n","vocab.save(vocab_file)\n","print('vocabulary saved to %s' % vocab_file)\n","\n","#  \n","print('Note that the <s> and </s> tokens are added while vocab initialization.\\n These tokens are also present in target top frequent words. \\nThat is why vocab size for target language is lesser by 2.')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["initialize source vocabulary ..\n","number of word types: 172418, number of word types w/ frequency >= 2: 80623\n","initialize target vocabulary ..\n","number of word types: 128873, number of word types w/ frequency >= 2: 64215\n","generated vocabulary, source 50004 words, target 50002 words\n","vocabulary saved to en_es_data/vocab.json\n","Note that the <s> and </s> tokens are added while vocab initialization.\n"," These tokens are also present in target top frequent words. \n","That is why vocab size for target language is lesser by 2.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e-95CL6vFfp9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ojs5sb5QFg_Q","colab_type":"text"},"source":["## Embedding"]},{"cell_type":"code","metadata":{"id":"PqpaQFz_CTOA","colab_type":"code","colab":{}},"source":["class ModelEmbeddings(nn.Module): \n","    \"\"\"\n","    Class that converts input words to their embeddings.\n","    \"\"\"\n","    def __init__(self, embed_size, vocab):\n","        \"\"\"\n","        Init the Embedding layers.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        \"\"\"\n","        super(ModelEmbeddings, self).__init__()\n","        self.embed_size = embed_size\n","\n","        # default values\n","        self.source = None\n","        self.target = None\n","\n","        src_pad_token_idx = vocab.src['<pad>']\n","        tgt_pad_token_idx = vocab.tgt['<pad>']\n","\n","        \n","        self.source = nn.Embedding(len(vocab.src),embed_size,padding_idx=src_pad_token_idx)\n","        self.target = nn.Embedding(len(vocab.tgt),embed_size,padding_idx=tgt_pad_token_idx)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cPIYL6RxF4-v","colab_type":"text"},"source":["## Encoder-Decoder model"]},{"cell_type":"code","metadata":{"id":"O0Dgrx1awXD9","colab_type":"code","colab":{}},"source":["Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJAf3QnCwXGW","colab_type":"code","colab":{}},"source":["class NMT(nn.Module):\n","    \"\"\" Simple Neural Machine Translation Model:\n","        - Bidrectional LSTM Encoder\n","        - Unidirection LSTM Decoder\n","        - Global Attention Model (Luong, et al. 2015)\n","    \"\"\"\n","    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n","        \"\"\" Init NMT Model.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param hidden_size (int): Hidden Size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        @param dropout_rate (float): Dropout probability, for attention\n","        \"\"\"\n","        super(NMT, self).__init__()\n","        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n","        self.hidden_size = hidden_size\n","        self.dropout_rate = dropout_rate\n","        self.vocab = vocab\n","\n","        # default values\n","        self.encoder = None\n","        self.decoder = None\n","        self.h_projection = None\n","        self.c_projection = None\n","        self.att_projection = None\n","        self.combined_output_projection = None\n","        self.target_vocab_projection = None\n","        self.dropout = None\n","\n","        # different layers        \n","        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n","        self.decoder = nn.LSTMCell(embed_size+hidden_size, hidden_size)\n","        self.h_projection= nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        self.c_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        self.att_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        self.combined_output_projection = nn.Linear(3*hidden_size, hidden_size, bias=False)\n","        self.target_vocab_projection = nn.Linear(hidden_size,len(vocab.tgt),bias=False)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        \n","        # multipolicative attention\n","        self.mult_atten = nn.Linear(hidden_size, hidden_size)\n","\n","\n","\n","    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n","        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n","        target sentences under the language models learned by the NMT system.\n","\n","        @param source (List[List[str]]): list of source sentence tokens\n","        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n","\n","        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n","                                    log-likelihood of generating the gold-standard target sentence for\n","                                    each example in the input batch. Here b = batch size.\n","        \"\"\"\n","        # Compute sentence lengths\n","        source_lengths = [len(s) for s in source]\n","\n","        # Convert list of lists into tensors\n","        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n","        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n","\n","        ###     Run the network forward:\n","\n","        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n","        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n","        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n","        #print(combined_outputs.size())\n","        #temp = self.target_vocab_projection(combined_outputs)\n","        #print(temp.size())\n","        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n","\n","        # Zero out, probabilities for which we have nothing in the target text\n","        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n","        \n","        # Compute log probability of generating true target words\n","        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n","        scores = target_gold_words_log_prob.sum(dim=0)\n","        return scores\n","\n","\n","    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n","            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n","\n","        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n","                                        b = batch_size, src_len = maximum source sentence length. Note that \n","                                       these have already been sorted in order of longest to shortest sentence.\n","        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n","        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n","                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n","                                                hidden state and cell.\n","        \"\"\"\n","        enc_hiddens, dec_init_state = None, None\n","\n","        \n","        X = self.model_embeddings.source(source_padded) #(src_len, b, embed_size)\n","        X = pack_padded_sequence(X, source_lengths)\n","        enc_hiddens, (last_hidden,last_cell) = self.encoder(X) #(h0,c0) defaults to zero\n","        enc_hiddens, _ = pad_packed_sequence(enc_hiddens, batch_first=True)\n","        last_hidden = torch.cat((last_hidden[0,:],last_hidden[1,:]),1)\n","        last_cell = torch.cat((last_cell[0,:],last_cell[1,:]),1)\n","        init_decoder_hidden = self.h_projection(last_hidden)\n","        init_decoder_cell = self.c_projection(last_cell)\n","        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n","\n","\n","        return enc_hiddens, dec_init_state\n","\n","\n","    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n","                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Compute combined output vectors for a batch.\n","\n","        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n","                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n","                                     b = batch size, src_len = maximum source sentence length.\n","        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n","        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n","                                       tgt_len = maximum target sentence length, b = batch size. \n","\n","        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n","                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n","        \"\"\"\n","        # Chop of the <END> token for max length sentences.\n","        target_padded = target_padded[:-1]\n","\n","        # Initialize the decoder state (hidden and cell)\n","        dec_state = dec_init_state\n","\n","        # Initialize previous combined output vector o_{t-1} as zero\n","        batch_size = enc_hiddens.size(0)\n","        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n","\n","        # Initialize a list we will use to collect the combined output o_t on each step\n","        combined_outputs = []\n","\n","\n","        enc_hiddens_proj = self.att_projection(enc_hiddens)\n","        Y = self.model_embeddings.target(target_padded) #(tgt_len,b,e)\n","        Y_splited = torch.split(Y,1, dim=0)\n","        tgt_len = target_padded.size(0)\n","        for i in range(tgt_len):\n","            Y_t = Y_splited[i]\n","            Y_t = torch.squeeze(Y_t,dim =0) #(b,e) --> after removal of time dim\n","            Ybar_t = torch.cat((Y_t,o_prev),dim=1) #(b,e+h)\n","            dec_state, o_prev, e_t = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n","            combined_outputs.append(o_prev)\n","        combined_outputs = torch.stack(combined_outputs)\n","        ### END YOUR CODE\n","\n","        return combined_outputs\n","\n","\n","    def step(self, Ybar_t: torch.Tensor,\n","            dec_state: Tuple[torch.Tensor, torch.Tensor],\n","            enc_hiddens: torch.Tensor,\n","            enc_hiddens_proj: torch.Tensor,\n","            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n","        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n","\n","        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n","                                where b = batch size, e = embedding size, h = hidden size.\n","        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n","                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n","        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n","                                    src_len = maximum source length, h = hidden size.\n","        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n","                                    where b = batch size, src_len = maximum source length, h = hidden size.\n","        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n","                                    where b = batch size, src_len is maximum source length. \n","\n","        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n","                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n","        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n","        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n","                                Note: You will not use this outside of this function.\n","                                      We are simply returning this value so that we can sanity check\n","                                      your implementation.\n","        \"\"\"\n","\n","        combined_output = None\n","\n","        dec_state = self.decoder(Ybar_t, dec_state)\n","        dec_hidden, dec_cell = dec_state\n","        aug_dec_hidden = torch.unsqueeze(dec_hidden, dim=2) #(b,hidden_size,1)\n","        \n","        mul_enc_proj = self.mult_atten(enc_hiddens_proj)\n"," \n","        e_t = torch.bmm(mul_enc_proj,aug_dec_hidden) # (b,max_len, hidden_size) * (b,hidden_size,1) --> (b, max_len, 1)\n","        e_t = torch.squeeze(e_t, dim=2) #(b, max_len)\n","\n","        ### END YOUR CODE\n","\n","        # Set e_t to -inf where enc_masks has 1\n","        if enc_masks is not None:\n","            e_t.data.masked_fill_(enc_masks.byte(), -float('inf'))\n","\n","        alpha_t = F.softmax(e_t, dim=1) #(b, src_len)\n","        # enc_hiddens --> (b, src_len, hidden_size*2) #\n","        aug_att = torch.unsqueeze(alpha_t,2) #(b, src_len, 1)\n","        tr_hiddens = enc_hiddens.transpose(1,2) #(b,hidden_size*2, src_len)\n","        a_t = torch.bmm(tr_hiddens,aug_att) #(b,2*hidden_size,1)\n","        a_t = torch.squeeze(a_t,dim=2) #(b,2*hidden_size)\n","        #print(a_t.size(),dec_hidden.size())\n","        \n","        U_t = torch.cat((a_t,dec_hidden), dim=1) #(b,3*hidden_size)\n","        V_t = self.combined_output_projection(U_t) #(b,hidden_size)\n","        O_t = self.dropout(torch.tanh(V_t))\n","        \n","\n","        combined_output = O_t\n","        return dec_state, combined_output, e_t\n","\n","    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n","        \"\"\" Generate sentence masks for encoder hidden states.\n","\n","        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n","                                     src_len = max source length, h = hidden size. \n","        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n","        \n","        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n","                                    where src_len = max source length, h = hidden size.\n","        \"\"\"\n","        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n","        for e_id, src_len in enumerate(source_lengths):\n","            enc_masks[e_id, src_len:] = 1\n","        return enc_masks.to(self.device)\n","\n","\n","    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n","        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n","        @param src_sent (List[str]): a single source sentence (words)\n","        @param beam_size (int): beam size\n","        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n","        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n","                value: List[str]: the decoded target sentence, represented as a list of words\n","                score: float: the log-likelihood of the target sentence\n","        \"\"\"\n","        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n","\n","        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n","        src_encodings_att_linear = self.att_projection(src_encodings)\n","\n","        h_tm1 = dec_init_vec\n","        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n","\n","        eos_id = self.vocab.tgt['</s>']\n","\n","        hypotheses = [['<s>']]\n","        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n","        completed_hypotheses = []\n","\n","        t = 0\n","        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n","            t += 1\n","            hyp_num = len(hypotheses)\n","\n","            exp_src_encodings = src_encodings.expand(hyp_num,\n","                                                     src_encodings.size(1),\n","                                                     src_encodings.size(2))\n","\n","            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n","                                                                           src_encodings_att_linear.size(1),\n","                                                                           src_encodings_att_linear.size(2))\n","\n","            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n","            y_t_embed = self.model_embeddings.target(y_tm1)\n","\n","            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n","\n","            (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n","                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n","\n","            # log probabilities over target words\n","            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n","\n","            live_hyp_num = beam_size - len(completed_hypotheses)\n","            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n","            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n","\n","            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n","            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n","\n","            new_hypotheses = []\n","            live_hyp_ids = []\n","            new_hyp_scores = []\n","\n","            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n","                prev_hyp_id = prev_hyp_id.item()\n","                hyp_word_id = hyp_word_id.item()\n","                cand_new_hyp_score = cand_new_hyp_score.item()\n","\n","                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n","                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n","                if hyp_word == '</s>':\n","                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n","                                                           score=cand_new_hyp_score))\n","                else:\n","                    new_hypotheses.append(new_hyp_sent)\n","                    live_hyp_ids.append(prev_hyp_id)\n","                    new_hyp_scores.append(cand_new_hyp_score)\n","\n","            if len(completed_hypotheses) == beam_size:\n","                break\n","\n","            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n","            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n","            att_tm1 = att_t[live_hyp_ids]\n","\n","            hypotheses = new_hypotheses\n","            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n","\n","        if len(completed_hypotheses) == 0:\n","            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n","                                                   score=hyp_scores[0].item()))\n","\n","        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n","\n","        return completed_hypotheses\n","\n","    @property\n","    def device(self) -> torch.device:\n","        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","        \"\"\"\n","        return self.model_embeddings.source.weight.device\n","\n","    @staticmethod\n","    def load(model_path: str):\n","        \"\"\" Load the model from a file.\n","        @param model_path (str): path to model\n","        \"\"\"\n","        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","        args = params['args']\n","        model = NMT(vocab=params['vocab'], **args)\n","        model.load_state_dict(params['state_dict'])\n","\n","        return model\n","\n","    def save(self, path: str):\n","        \"\"\" Save the odel to a file.\n","        @param path (str): path to the model\n","        \"\"\"\n","        print('save model parameters to [%s]' % path, file=sys.stderr)\n","\n","        params = {\n","            'args': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n","            'vocab': self.vocab,\n","            'state_dict': self.state_dict()\n","        }\n","\n","        torch.save(params, path)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TAZvZPOtHLD5","colab_type":"text"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"_etPbHxUwXIq","colab_type":"code","colab":{}},"source":["# load data\n","train_es = 'en_es_data/train.es'\n","train_en = 'en_es_data/train.en'\n","\n","dev_es = 'en_es_data/dev.es'\n","dev_en = 'en_es_data/dev.en'\n","\n","test_es = 'en_es_data/test.es'\n","test_en = 'en_es_data/test.en'\n","\n","vocab_file = 'en_es_data/vocab.json'\n","\n","\n","train_data_src = read_corpus(train_es, source='src')\n","train_data_tgt = read_corpus(train_en, source='tgt')\n","\n","dev_data_src = read_corpus(dev_es, source='src')\n","dev_data_tgt = read_corpus(dev_en, source='tgt')\n","\n","test_data_src = read_corpus(test_es, source='src')\n","test_data_tgt = read_corpus(test_en, source='tgt')\n","\n","train_data = list(zip(train_data_src,train_data_tgt))\n","dev_data = list(zip(dev_data_src,dev_data_tgt))\n","test_data = list(zip(test_data_src,test_data_tgt))\n","\n","vocab = Vocab.load(vocab_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktso8XG2wXLC","colab_type":"code","colab":{}},"source":["# We can set these parameters\n","train_batch_size = 64\n","clip_grad = 5.0\n","#valid_niter = int(args['--valid-niter'])\n","valid_niter = 2000\n","\n","log_every = 100\n","model_save_path = 'NMT_model_mul_atten'\n","max_epoch = 30\n","max_patience = 3\n","max_trial = 3\n","lr_decay = 0.5\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmAzDGIgwXNW","colab_type":"code","outputId":"586fa39f-8ad1-4293-b932-9b7898866445","executionInfo":{"status":"ok","timestamp":1558508470417,"user_tz":-480,"elapsed":25138,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["model = NMT(embed_size= 256, hidden_size=256, dropout_rate=0.3, vocab=vocab)\n","model.train()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NMT(\n","  (model_embeddings): ModelEmbeddings(\n","    (source): Embedding(50004, 256, padding_idx=0)\n","    (target): Embedding(50002, 256, padding_idx=0)\n","  )\n","  (encoder): LSTM(256, 256, bidirectional=True)\n","  (decoder): LSTMCell(512, 256)\n","  (h_projection): Linear(in_features=512, out_features=256, bias=False)\n","  (c_projection): Linear(in_features=512, out_features=256, bias=False)\n","  (att_projection): Linear(in_features=512, out_features=256, bias=False)\n","  (combined_output_projection): Linear(in_features=768, out_features=256, bias=False)\n","  (target_vocab_projection): Linear(in_features=256, out_features=50002, bias=False)\n","  (dropout): Dropout(p=0.3)\n","  (mult_atten): Linear(in_features=256, out_features=256, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"XRB6pKvcwXPy","colab_type":"code","outputId":"cd16ce64-8f35-47ec-bcfa-0ff9f7c863b0","executionInfo":{"status":"ok","timestamp":1558508471112,"user_tz":-480,"elapsed":25821,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["uniform_init = 0.1\n","if np.abs(uniform_init) > 0.:\n","  print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n","  for p in model.parameters():\n","    p.data.uniform_(-uniform_init, uniform_init)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["uniformly initialize parameters [-0.100000, +0.100000]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hATdhBJowXSD","colab_type":"code","colab":{}},"source":["# mask pad tokens\n","vocab_mask = torch.ones(len(vocab.tgt))\n","vocab_mask[vocab.tgt['<pad>']] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7Ax9a3awXUd","colab_type":"code","outputId":"76566cb6-147a-4021-d6c4-479789ad27d7","executionInfo":{"status":"ok","timestamp":1558508471116,"user_tz":-480,"elapsed":25811,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n","print('use device: %s' % device)\n","model = model.to(device)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["use device: cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9hibQuGaNTgc","colab_type":"code","colab":{}},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFIUK2VLwXXE","colab_type":"code","outputId":"94243c35-a478-482d-abda-e85e2bc21cbd","executionInfo":{"status":"ok","timestamp":1558508471118,"user_tz":-480,"elapsed":25796,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import time\n","num_trial = 0\n","train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","cum_examples = report_examples = epoch = valid_num = 0\n","hist_valid_scores = []\n","train_time = begin_time = time.time()\n","\n","print('Start Maximum Likelihood training:')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Start Maximum Likelihood training:\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"raP4jK47T0w0","colab_type":"code","colab":{}},"source":["def evaluate_ppl(model, dev_data, batch_size=32):\n","    \"\"\" Evaluate perplexity on dev sentences\n","    @param model (NMT): NMT Model\n","    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (batch size)\n","    @returns ppl (perplixty on dev sentences)\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    cum_loss = 0.\n","    cum_tgt_words = 0.\n","\n","    # no_grad() signals backend to throw away all gradients\n","    with torch.no_grad():\n","        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n","            loss = -model(src_sents, tgt_sents).sum()\n","\n","            cum_loss += loss.item()\n","            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","            cum_tgt_words += tgt_word_num_to_predict\n","\n","        ppl = np.exp(cum_loss / cum_tgt_words)\n","\n","    if was_training:\n","        model.train()\n","\n","    return ppl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fAiNNGGewXZb","colab_type":"code","outputId":"bd6e1900-075b-411a-c279-4ca147d00b60","executionInfo":{"status":"error","timestamp":1558522265812,"user_tz":-480,"elapsed":3549114,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":8058}},"source":["while True:\n","    epoch += 1\n","\n","    for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n","        train_iter += 1\n","\n","        optimizer.zero_grad()\n","\n","        batch_size = len(src_sents)\n","        #\n","        #print(batch_size)\n","        example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n","        batch_loss = example_losses.sum()\n","        loss = batch_loss / batch_size\n","\n","        loss.backward()\n","\n","        # clip gradient\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n","\n","        optimizer.step()\n","\n","        batch_losses_val = batch_loss.item()\n","        report_loss += batch_losses_val\n","        cum_loss += batch_losses_val\n","\n","        tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","        report_tgt_words += tgt_words_num_to_predict\n","        cum_tgt_words += tgt_words_num_to_predict\n","        report_examples += batch_size\n","        cum_examples += batch_size\n","\n","        if train_iter % log_every == 0:\n","            print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n","                  'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n","                                                                                     report_loss / report_examples,\n","                                                                                     math.exp(report_loss / report_tgt_words),\n","                                                                                     cum_examples,\n","                                                                                     report_tgt_words / (time.time() - train_time),\n","                                                                                     time.time() - begin_time), file=sys.stderr)\n","\n","            train_time = time.time()\n","            report_loss = report_tgt_words = report_examples = 0.\n","\n","        # perform validation\n","        if train_iter % valid_niter == 0:\n","            print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n","                                                                                     cum_loss / cum_examples,\n","                                                                                     np.exp(cum_loss / cum_tgt_words),\n","                                                                                     cum_examples), file=sys.stderr)\n","\n","            cum_loss = cum_examples = cum_tgt_words = 0.\n","            valid_num += 1\n","\n","            print('begin validation ...', file=sys.stderr)\n","\n","            # compute dev. ppl and bleu\n","            dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n","            valid_metric = -dev_ppl\n","\n","            print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n","\n","            is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n","            hist_valid_scores.append(valid_metric)\n","\n","            if is_better:\n","                patience = 0\n","                print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n","                model.save(model_save_path)\n","\n","                # also save the optimizers' state\n","                torch.save(optimizer.state_dict(), model_save_path + '.optim')\n","            elif patience < int(max_patience):\n","                patience += 1\n","                print('hit patience %d' % patience, file=sys.stderr)\n","\n","                if patience == int(max_patience):\n","                    num_trial += 1\n","                    print('hit #%d trial' % num_trial, file=sys.stderr)\n","                    if num_trial == int(max_trial):\n","                        print('early stop!', file=sys.stderr)\n","                        exit(0)\n","\n","                    # decay lr, and restore from previously best checkpoint\n","                    lr = optimizer.param_groups[0]['lr'] * float(lr_decay)\n","                    print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n","\n","                    # load model\n","                    params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n","                    model.load_state_dict(params['state_dict'])\n","                    model = model.to(device)\n","\n","                    print('restore parameters of the optimizers', file=sys.stderr)\n","                    optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n","\n","                    # set new lr\n","                    for param_group in optimizer.param_groups:\n","                        param_group['lr'] = lr\n","\n","                    # reset patience\n","                    patience = 0\n","\n","            if epoch == int(max_epoch):\n","                print('reached maximum number of epochs!', file=sys.stderr)\n","                break"],"execution_count":27,"outputs":[{"output_type":"stream","text":["epoch 1, iter 100, avg. loss 132.87, avg. ppl 1846.13 cum. examples 6400, speed 2936.88 words/sec, time elapsed 38.50 sec\n","epoch 1, iter 200, avg. loss 117.89, avg. ppl 797.09 cum. examples 12800, speed 3043.33 words/sec, time elapsed 75.61 sec\n","epoch 1, iter 300, avg. loss 111.04, avg. ppl 526.82 cum. examples 19200, speed 3050.67 words/sec, time elapsed 112.78 sec\n","epoch 1, iter 400, avg. loss 104.73, avg. ppl 382.67 cum. examples 25600, speed 3026.76 words/sec, time elapsed 150.02 sec\n","epoch 1, iter 500, avg. loss 102.50, avg. ppl 312.44 cum. examples 32000, speed 3042.45 words/sec, time elapsed 187.55 sec\n","epoch 1, iter 600, avg. loss 97.50, avg. ppl 249.20 cum. examples 38400, speed 3046.70 words/sec, time elapsed 224.67 sec\n","epoch 1, iter 700, avg. loss 93.93, avg. ppl 207.86 cum. examples 44800, speed 3011.69 words/sec, time elapsed 262.07 sec\n","epoch 1, iter 800, avg. loss 90.99, avg. ppl 174.51 cum. examples 51200, speed 2978.75 words/sec, time elapsed 299.94 sec\n","epoch 1, iter 900, avg. loss 88.39, avg. ppl 154.25 cum. examples 57600, speed 2979.21 words/sec, time elapsed 337.63 sec\n","epoch 1, iter 1000, avg. loss 85.80, avg. ppl 135.23 cum. examples 64000, speed 3042.51 words/sec, time elapsed 374.41 sec\n","epoch 1, iter 1100, avg. loss 84.89, avg. ppl 119.50 cum. examples 70400, speed 2956.62 words/sec, time elapsed 412.83 sec\n","epoch 1, iter 1200, avg. loss 82.72, avg. ppl 109.36 cum. examples 76800, speed 3049.66 words/sec, time elapsed 449.80 sec\n","epoch 1, iter 1300, avg. loss 80.72, avg. ppl 98.02 cum. examples 83200, speed 3018.96 words/sec, time elapsed 487.12 sec\n","epoch 1, iter 1400, avg. loss 79.07, avg. ppl 90.34 cum. examples 89600, speed 3014.59 words/sec, time elapsed 524.40 sec\n","epoch 1, iter 1500, avg. loss 77.58, avg. ppl 81.51 cum. examples 96000, speed 2963.16 words/sec, time elapsed 562.47 sec\n","epoch 1, iter 1600, avg. loss 75.80, avg. ppl 73.85 cum. examples 102400, speed 3017.73 words/sec, time elapsed 599.84 sec\n","epoch 1, iter 1700, avg. loss 76.52, avg. ppl 73.02 cum. examples 108800, speed 2974.80 words/sec, time elapsed 638.21 sec\n","epoch 1, iter 1800, avg. loss 74.15, avg. ppl 66.07 cum. examples 115200, speed 3014.07 words/sec, time elapsed 675.78 sec\n","epoch 1, iter 1900, avg. loss 73.86, avg. ppl 62.82 cum. examples 121600, speed 3018.03 words/sec, time elapsed 713.61 sec\n","epoch 1, iter 2000, avg. loss 72.90, avg. ppl 59.19 cum. examples 128000, speed 3008.90 words/sec, time elapsed 751.61 sec\n","epoch 1, iter 2000, cum. loss 90.19, cum. ppl 164.68 cum. examples 128000\n","begin validation ...\n","validation: iter 2000, dev. ppl 63.357944\n","save currently the best model to [NMT_model_mul_atten]\n","save model parameters to [NMT_model_mul_atten]\n","epoch 1, iter 2100, avg. loss 71.09, avg. ppl 56.29 cum. examples 6400, speed 2653.36 words/sec, time elapsed 794.15 sec\n","epoch 1, iter 2200, avg. loss 68.89, avg. ppl 52.28 cum. examples 12800, speed 2957.58 words/sec, time elapsed 831.83 sec\n","epoch 1, iter 2300, avg. loss 70.15, avg. ppl 51.48 cum. examples 19200, speed 3000.35 words/sec, time elapsed 869.80 sec\n","epoch 1, iter 2400, avg. loss 68.07, avg. ppl 48.63 cum. examples 25600, speed 3019.23 words/sec, time elapsed 906.94 sec\n","epoch 1, iter 2500, avg. loss 67.50, avg. ppl 46.59 cum. examples 32000, speed 2986.97 words/sec, time elapsed 944.59 sec\n","epoch 1, iter 2600, avg. loss 67.34, avg. ppl 45.13 cum. examples 38400, speed 2978.72 words/sec, time elapsed 982.57 sec\n","epoch 1, iter 2700, avg. loss 66.36, avg. ppl 42.52 cum. examples 44800, speed 2996.76 words/sec, time elapsed 1020.37 sec\n","epoch 1, iter 2800, avg. loss 66.14, avg. ppl 40.28 cum. examples 51200, speed 2981.95 words/sec, time elapsed 1058.78 sec\n","epoch 1, iter 2900, avg. loss 65.31, avg. ppl 39.29 cum. examples 57600, speed 3048.75 words/sec, time elapsed 1096.13 sec\n","epoch 1, iter 3000, avg. loss 64.39, avg. ppl 38.53 cum. examples 64000, speed 3002.61 words/sec, time elapsed 1133.71 sec\n","epoch 1, iter 3100, avg. loss 64.52, avg. ppl 37.20 cum. examples 70400, speed 3022.36 words/sec, time elapsed 1171.50 sec\n","epoch 1, iter 3200, avg. loss 62.01, avg. ppl 34.40 cum. examples 76800, speed 3035.69 words/sec, time elapsed 1208.45 sec\n","epoch 1, iter 3300, avg. loss 62.71, avg. ppl 33.88 cum. examples 83200, speed 2983.40 words/sec, time elapsed 1246.63 sec\n","epoch 2, iter 3400, avg. loss 60.83, avg. ppl 31.89 cum. examples 89577, speed 3022.97 words/sec, time elapsed 1283.70 sec\n","epoch 2, iter 3500, avg. loss 57.49, avg. ppl 25.66 cum. examples 95977, speed 3043.06 words/sec, time elapsed 1320.96 sec\n","epoch 2, iter 3600, avg. loss 57.88, avg. ppl 26.20 cum. examples 102377, speed 3022.57 words/sec, time elapsed 1358.49 sec\n","epoch 2, iter 3700, avg. loss 56.94, avg. ppl 25.87 cum. examples 108777, speed 3028.10 words/sec, time elapsed 1395.48 sec\n","epoch 2, iter 3800, avg. loss 57.70, avg. ppl 25.54 cum. examples 115177, speed 3031.86 words/sec, time elapsed 1433.07 sec\n","epoch 2, iter 3900, avg. loss 56.73, avg. ppl 24.93 cum. examples 121577, speed 2999.91 words/sec, time elapsed 1470.71 sec\n","epoch 2, iter 4000, avg. loss 56.66, avg. ppl 24.64 cum. examples 127977, speed 3032.58 words/sec, time elapsed 1508.02 sec\n","epoch 2, iter 4000, cum. loss 63.44, cum. ppl 36.22 cum. examples 127977\n","begin validation ...\n","validation: iter 4000, dev. ppl 33.163264\n","save currently the best model to [NMT_model_mul_atten]\n","save model parameters to [NMT_model_mul_atten]\n","epoch 2, iter 4100, avg. loss 55.75, avg. ppl 24.13 cum. examples 6400, speed 2695.15 words/sec, time elapsed 1549.61 sec\n","epoch 2, iter 4200, avg. loss 55.21, avg. ppl 23.59 cum. examples 12800, speed 2957.79 words/sec, time elapsed 1587.40 sec\n","epoch 2, iter 4300, avg. loss 55.43, avg. ppl 22.99 cum. examples 19200, speed 2947.97 words/sec, time elapsed 1625.79 sec\n","epoch 2, iter 4400, avg. loss 55.76, avg. ppl 22.92 cum. examples 25600, speed 3015.69 words/sec, time elapsed 1663.57 sec\n","epoch 2, iter 4500, avg. loss 54.99, avg. ppl 22.21 cum. examples 32000, speed 3027.26 words/sec, time elapsed 1701.07 sec\n","epoch 2, iter 4600, avg. loss 54.80, avg. ppl 22.30 cum. examples 38400, speed 2982.83 words/sec, time elapsed 1738.95 sec\n","epoch 2, iter 4700, avg. loss 54.42, avg. ppl 22.02 cum. examples 44800, speed 3015.42 words/sec, time elapsed 1776.30 sec\n","epoch 2, iter 4800, avg. loss 54.79, avg. ppl 21.87 cum. examples 51200, speed 2997.45 words/sec, time elapsed 1814.22 sec\n","epoch 2, iter 4900, avg. loss 53.55, avg. ppl 20.88 cum. examples 57600, speed 2971.20 words/sec, time elapsed 1852.17 sec\n","epoch 2, iter 5000, avg. loss 53.51, avg. ppl 20.81 cum. examples 64000, speed 3012.58 words/sec, time elapsed 1889.62 sec\n","epoch 2, iter 5100, avg. loss 54.12, avg. ppl 21.14 cum. examples 70400, speed 2988.60 words/sec, time elapsed 1927.61 sec\n","epoch 2, iter 5200, avg. loss 54.05, avg. ppl 20.34 cum. examples 76800, speed 3040.31 words/sec, time elapsed 1965.38 sec\n","epoch 2, iter 5300, avg. loss 52.90, avg. ppl 19.80 cum. examples 83200, speed 3008.69 words/sec, time elapsed 2003.07 sec\n","epoch 2, iter 5400, avg. loss 53.35, avg. ppl 20.32 cum. examples 89600, speed 2982.19 words/sec, time elapsed 2041.09 sec\n","epoch 2, iter 5500, avg. loss 53.19, avg. ppl 19.86 cum. examples 96000, speed 2993.83 words/sec, time elapsed 2079.14 sec\n","epoch 2, iter 5600, avg. loss 52.31, avg. ppl 19.76 cum. examples 102400, speed 2983.88 words/sec, time elapsed 2116.74 sec\n","epoch 2, iter 5700, avg. loss 53.58, avg. ppl 19.88 cum. examples 108800, speed 3041.79 words/sec, time elapsed 2154.45 sec\n","epoch 2, iter 5800, avg. loss 51.53, avg. ppl 19.13 cum. examples 115200, speed 3034.24 words/sec, time elapsed 2191.28 sec\n","epoch 2, iter 5900, avg. loss 51.94, avg. ppl 19.25 cum. examples 121600, speed 3021.43 words/sec, time elapsed 2228.48 sec\n","epoch 2, iter 6000, avg. loss 52.59, avg. ppl 19.04 cum. examples 128000, speed 3077.15 words/sec, time elapsed 2265.59 sec\n","epoch 2, iter 6000, cum. loss 53.89, cum. ppl 21.06 cum. examples 128000\n","begin validation ...\n","validation: iter 6000, dev. ppl 23.586891\n","save currently the best model to [NMT_model_mul_atten]\n","save model parameters to [NMT_model_mul_atten]\n","epoch 2, iter 6100, avg. loss 51.33, avg. ppl 18.74 cum. examples 6400, speed 2670.46 words/sec, time elapsed 2307.57 sec\n","epoch 2, iter 6200, avg. loss 51.50, avg. ppl 18.37 cum. examples 12800, speed 3077.07 words/sec, time elapsed 2344.37 sec\n","epoch 2, iter 6300, avg. loss 51.11, avg. ppl 18.11 cum. examples 19200, speed 3012.91 words/sec, time elapsed 2381.86 sec\n","epoch 2, iter 6400, avg. loss 50.63, avg. ppl 18.05 cum. examples 25600, speed 3011.26 words/sec, time elapsed 2419.05 sec\n","epoch 2, iter 6500, avg. loss 51.33, avg. ppl 17.79 cum. examples 32000, speed 3003.66 words/sec, time elapsed 2457.04 sec\n","epoch 2, iter 6600, avg. loss 50.72, avg. ppl 17.86 cum. examples 38400, speed 2967.39 words/sec, time elapsed 2495.00 sec\n","epoch 2, iter 6700, avg. loss 50.16, avg. ppl 17.11 cum. examples 44800, speed 3025.79 words/sec, time elapsed 2532.36 sec\n","epoch 3, iter 6800, avg. loss 49.37, avg. ppl 16.48 cum. examples 51177, speed 2975.44 words/sec, time elapsed 2570.11 sec\n","epoch 3, iter 6900, avg. loss 45.42, avg. ppl 13.09 cum. examples 57577, speed 3068.92 words/sec, time elapsed 2606.94 sec\n","epoch 3, iter 7000, avg. loss 45.44, avg. ppl 13.15 cum. examples 63977, speed 2964.20 words/sec, time elapsed 2645.02 sec\n","epoch 3, iter 7100, avg. loss 44.88, avg. ppl 12.98 cum. examples 70377, speed 3009.54 words/sec, time elapsed 2682.25 sec\n","epoch 3, iter 7200, avg. loss 46.06, avg. ppl 13.27 cum. examples 76777, speed 2959.95 words/sec, time elapsed 2720.77 sec\n","epoch 3, iter 7300, avg. loss 45.68, avg. ppl 12.94 cum. examples 83177, speed 3035.41 words/sec, time elapsed 2758.39 sec\n","epoch 3, iter 7400, avg. loss 44.92, avg. ppl 13.03 cum. examples 89577, speed 2967.74 words/sec, time elapsed 2796.13 sec\n","epoch 3, iter 7500, avg. loss 45.23, avg. ppl 13.03 cum. examples 95977, speed 3009.08 words/sec, time elapsed 2833.60 sec\n","epoch 3, iter 7600, avg. loss 45.81, avg. ppl 13.26 cum. examples 102377, speed 3052.43 words/sec, time elapsed 2870.77 sec\n","epoch 3, iter 7700, avg. loss 45.74, avg. ppl 13.13 cum. examples 108777, speed 3049.79 words/sec, time elapsed 2908.05 sec\n","epoch 3, iter 7800, avg. loss 45.81, avg. ppl 13.49 cum. examples 115177, speed 2959.38 words/sec, time elapsed 2946.13 sec\n","epoch 3, iter 7900, avg. loss 46.31, avg. ppl 13.46 cum. examples 121577, speed 3074.00 words/sec, time elapsed 2983.22 sec\n","epoch 3, iter 8000, avg. loss 44.78, avg. ppl 12.82 cum. examples 127977, speed 3012.68 words/sec, time elapsed 3020.51 sec\n","epoch 3, iter 8000, cum. loss 47.61, cum. ppl 14.83 cum. examples 127977\n","begin validation ...\n","validation: iter 8000, dev. ppl 21.267544\n","save currently the best model to [NMT_model_mul_atten]\n","save model parameters to [NMT_model_mul_atten]\n","epoch 3, iter 8100, avg. loss 46.07, avg. ppl 13.36 cum. examples 6400, speed 2738.85 words/sec, time elapsed 3062.04 sec\n","epoch 3, iter 8200, avg. loss 45.23, avg. ppl 12.84 cum. examples 12800, speed 3049.46 words/sec, time elapsed 3099.22 sec\n","epoch 3, iter 8300, avg. loss 45.58, avg. ppl 13.04 cum. examples 19200, speed 3007.24 words/sec, time elapsed 3137.00 sec\n","epoch 3, iter 8400, avg. loss 45.77, avg. ppl 12.81 cum. examples 25600, speed 2995.74 words/sec, time elapsed 3175.35 sec\n","epoch 3, iter 8500, avg. loss 45.98, avg. ppl 13.04 cum. examples 32000, speed 3024.34 words/sec, time elapsed 3213.24 sec\n","epoch 3, iter 8600, avg. loss 45.83, avg. ppl 13.03 cum. examples 38400, speed 2984.24 words/sec, time elapsed 3251.52 sec\n","epoch 3, iter 8700, avg. loss 45.21, avg. ppl 12.80 cum. examples 44800, speed 2999.12 words/sec, time elapsed 3289.37 sec\n","epoch 3, iter 8800, avg. loss 45.27, avg. ppl 13.02 cum. examples 51200, speed 3038.56 words/sec, time elapsed 3326.52 sec\n","epoch 3, iter 8900, avg. loss 45.27, avg. ppl 12.84 cum. examples 57600, speed 3080.51 words/sec, time elapsed 3363.37 sec\n","epoch 3, iter 9000, avg. loss 45.28, avg. ppl 12.69 cum. examples 64000, speed 3001.23 words/sec, time elapsed 3401.37 sec\n","epoch 3, iter 9100, avg. loss 45.17, avg. ppl 12.67 cum. examples 70400, speed 3015.81 words/sec, time elapsed 3439.13 sec\n","epoch 3, iter 9200, avg. loss 45.29, avg. ppl 13.07 cum. examples 76800, speed 3021.51 words/sec, time elapsed 3476.45 sec\n","epoch 3, iter 9300, avg. loss 44.13, avg. ppl 12.65 cum. examples 83200, speed 3004.20 words/sec, time elapsed 3513.50 sec\n","epoch 3, iter 9400, avg. loss 44.44, avg. ppl 12.64 cum. examples 89600, speed 3023.11 words/sec, time elapsed 3550.58 sec\n","epoch 3, iter 9500, avg. loss 44.39, avg. ppl 12.77 cum. examples 96000, speed 2967.57 words/sec, time elapsed 3588.16 sec\n","epoch 3, iter 9600, avg. loss 44.87, avg. ppl 12.66 cum. examples 102400, speed 3012.98 words/sec, time elapsed 3625.70 sec\n","epoch 3, iter 9700, avg. loss 44.41, avg. ppl 12.58 cum. examples 108800, speed 3000.98 words/sec, time elapsed 3663.10 sec\n","epoch 3, iter 9800, avg. loss 44.87, avg. ppl 12.76 cum. examples 115200, speed 2944.55 words/sec, time elapsed 3701.40 sec\n","epoch 3, iter 9900, avg. loss 44.67, avg. ppl 12.51 cum. examples 121600, speed 3020.59 words/sec, time elapsed 3738.86 sec\n","epoch 3, iter 10000, avg. loss 44.06, avg. ppl 12.55 cum. examples 128000, speed 3022.86 words/sec, time elapsed 3775.74 sec\n","epoch 3, iter 10000, cum. loss 45.09, cum. ppl 12.82 cum. examples 128000\n","begin validation ...\n","validation: iter 10000, dev. ppl 18.872608\n","save currently the best model to [NMT_model_mul_atten]\n","save model parameters to [NMT_model_mul_atten]\n","epoch 3, iter 10100, avg. loss 44.85, avg. ppl 12.58 cum. examples 6400, speed 2758.37 words/sec, time elapsed 3816.83 sec\n","epoch 4, iter 10200, avg. loss 41.68, avg. ppl 10.77 cum. examples 12777, speed 2945.49 words/sec, time elapsed 3854.80 sec\n","epoch 4, iter 10300, avg. loss 39.74, avg. ppl 9.28 cum. examples 19177, speed 3006.71 words/sec, time elapsed 3892.77 sec\n","epoch 4, iter 10400, avg. loss 39.31, avg. ppl 9.55 cum. examples 25577, speed 2957.14 words/sec, time elapsed 3930.49 sec\n","epoch 4, iter 10500, avg. loss 39.91, avg. ppl 9.46 cum. examples 31977, speed 2973.40 words/sec, time elapsed 3968.71 sec\n","epoch 4, iter 10600, avg. loss 40.56, avg. ppl 9.73 cum. examples 38377, speed 2989.48 words/sec, time elapsed 4006.88 sec\n","epoch 4, iter 10700, avg. loss 40.15, avg. ppl 9.68 cum. examples 44777, speed 3084.12 words/sec, time elapsed 4043.58 sec\n","epoch 4, iter 10800, avg. loss 40.21, avg. ppl 9.61 cum. examples 51177, speed 2989.08 words/sec, time elapsed 4081.63 sec\n","epoch 4, iter 10900, avg. loss 40.38, avg. ppl 9.90 cum. examples 57577, speed 3017.54 words/sec, time elapsed 4119.00 sec\n","epoch 4, iter 11000, avg. loss 39.92, avg. ppl 9.81 cum. examples 63977, speed 3006.44 words/sec, time elapsed 4156.21 sec\n","epoch 4, iter 11100, avg. loss 40.35, avg. ppl 9.81 cum. examples 70377, speed 3001.25 words/sec, time elapsed 4193.90 sec\n","epoch 4, iter 11200, avg. loss 40.46, avg. ppl 9.89 cum. examples 76777, speed 2961.45 words/sec, time elapsed 4232.05 sec\n","epoch 4, iter 11300, avg. loss 40.06, avg. ppl 9.74 cum. examples 83177, speed 2987.17 words/sec, time elapsed 4269.76 sec\n","epoch 4, iter 11400, avg. loss 40.59, avg. ppl 9.90 cum. examples 89577, speed 3009.80 words/sec, time elapsed 4307.40 sec\n","epoch 4, iter 11500, avg. loss 40.00, avg. ppl 9.70 cum. examples 95977, speed 3034.13 words/sec, time elapsed 4344.53 sec\n","epoch 4, iter 11600, avg. loss 40.47, avg. ppl 9.88 cum. examples 102377, speed 2978.10 words/sec, time elapsed 4382.51 sec\n","epoch 4, iter 11700, avg. loss 41.22, avg. ppl 9.96 cum. examples 108777, speed 3083.59 words/sec, time elapsed 4419.74 sec\n","epoch 4, iter 11800, avg. loss 40.54, avg. ppl 9.80 cum. examples 115177, speed 3108.26 words/sec, time elapsed 4456.30 sec\n","epoch 4, iter 11900, avg. loss 40.12, avg. ppl 9.77 cum. examples 121577, speed 3018.35 words/sec, time elapsed 4493.62 sec\n","epoch 4, iter 12000, avg. loss 40.91, avg. ppl 9.99 cum. examples 127977, speed 3002.86 words/sec, time elapsed 4531.50 sec\n","epoch 4, iter 12000, cum. loss 40.57, cum. ppl 9.92 cum. examples 127977\n","begin validation ...\n","validation: iter 12000, dev. ppl 18.045022\n","save currently the best model to [NMT_model_mul_atten]\n","save model parameters to [NMT_model_mul_atten]\n","epoch 4, iter 12100, avg. loss 40.31, avg. ppl 9.76 cum. examples 6400, speed 2631.48 words/sec, time elapsed 4574.53 sec\n","epoch 4, iter 12200, avg. loss 40.30, avg. ppl 9.94 cum. examples 12800, speed 3020.03 words/sec, time elapsed 4611.73 sec\n","epoch 4, iter 12300, avg. loss 40.36, avg. ppl 9.75 cum. examples 19200, speed 3031.96 words/sec, time elapsed 4649.13 sec\n","epoch 4, iter 12400, avg. loss 40.95, avg. ppl 10.30 cum. examples 25600, speed 2973.36 words/sec, time elapsed 4686.93 sec\n","epoch 4, iter 12500, avg. loss 40.80, avg. ppl 10.12 cum. examples 32000, speed 2935.68 words/sec, time elapsed 4725.36 sec\n","epoch 4, iter 12600, avg. loss 41.04, avg. ppl 10.05 cum. examples 38400, speed 3016.13 words/sec, time elapsed 4763.09 sec\n","epoch 4, iter 12700, avg. loss 40.85, avg. ppl 10.10 cum. examples 44800, speed 3040.02 words/sec, time elapsed 4800.28 sec\n","epoch 4, iter 12800, avg. loss 40.51, avg. ppl 10.02 cum. examples 51200, speed 2987.81 words/sec, time elapsed 4837.94 sec\n","epoch 4, iter 12900, avg. loss 40.67, avg. ppl 10.09 cum. examples 57600, speed 3022.38 words/sec, time elapsed 4875.19 sec\n","epoch 4, iter 13000, avg. loss 41.08, avg. ppl 10.06 cum. examples 64000, speed 3014.62 words/sec, time elapsed 4912.97 sec\n","epoch 4, iter 13100, avg. loss 41.51, avg. ppl 10.29 cum. examples 70400, speed 3031.96 words/sec, time elapsed 4950.55 sec\n","epoch 4, iter 13200, avg. loss 40.47, avg. ppl 10.04 cum. examples 76800, speed 3036.23 words/sec, time elapsed 4987.53 sec\n","epoch 4, iter 13300, avg. loss 40.32, avg. ppl 9.97 cum. examples 83200, speed 3024.17 words/sec, time elapsed 5024.63 sec\n","epoch 4, iter 13400, avg. loss 41.11, avg. ppl 10.22 cum. examples 89600, speed 3025.90 words/sec, time elapsed 5062.04 sec\n","epoch 4, iter 13500, avg. loss 40.63, avg. ppl 10.04 cum. examples 96000, speed 2996.85 words/sec, time elapsed 5099.66 sec\n","epoch 5, iter 13600, avg. loss 37.41, avg. ppl 8.35 cum. examples 102377, speed 3005.13 words/sec, time elapsed 5137.07 sec\n","epoch 5, iter 13700, avg. loss 35.94, avg. ppl 7.56 cum. examples 108777, speed 3056.13 words/sec, time elapsed 5174.27 sec\n","epoch 5, iter 13800, avg. loss 36.10, avg. ppl 7.51 cum. examples 115177, speed 3004.63 words/sec, time elapsed 5212.41 sec\n","epoch 5, iter 13900, avg. loss 36.34, avg. ppl 7.76 cum. examples 121577, speed 2989.26 words/sec, time elapsed 5250.38 sec\n","epoch 5, iter 14000, avg. loss 36.31, avg. ppl 7.80 cum. examples 127977, speed 3002.23 words/sec, time elapsed 5288.05 sec\n","epoch 5, iter 14000, cum. loss 39.65, cum. ppl 9.43 cum. examples 127977\n","begin validation ...\n","validation: iter 14000, dev. ppl 18.726682\n","hit patience 1\n","epoch 5, iter 14100, avg. loss 36.49, avg. ppl 7.82 cum. examples 6400, speed 2918.00 words/sec, time elapsed 5326.97 sec\n","epoch 5, iter 14200, avg. loss 36.63, avg. ppl 7.85 cum. examples 12800, speed 3051.98 words/sec, time elapsed 5364.26 sec\n","epoch 5, iter 14300, avg. loss 36.30, avg. ppl 7.89 cum. examples 19200, speed 3020.82 words/sec, time elapsed 5401.50 sec\n","epoch 5, iter 14400, avg. loss 36.68, avg. ppl 7.92 cum. examples 25600, speed 3050.30 words/sec, time elapsed 5438.70 sec\n","epoch 5, iter 14500, avg. loss 35.84, avg. ppl 7.88 cum. examples 32000, speed 2978.77 words/sec, time elapsed 5476.00 sec\n","epoch 5, iter 14600, avg. loss 36.71, avg. ppl 7.95 cum. examples 38400, speed 3018.76 words/sec, time elapsed 5513.54 sec\n","epoch 5, iter 14700, avg. loss 37.99, avg. ppl 8.38 cum. examples 44800, speed 3043.74 words/sec, time elapsed 5551.10 sec\n","epoch 5, iter 14800, avg. loss 36.82, avg. ppl 8.11 cum. examples 51200, speed 2922.41 words/sec, time elapsed 5589.63 sec\n","epoch 5, iter 14900, avg. loss 36.91, avg. ppl 8.28 cum. examples 57600, speed 2986.07 words/sec, time elapsed 5627.05 sec\n","epoch 5, iter 15000, avg. loss 37.31, avg. ppl 8.07 cum. examples 64000, speed 3042.72 words/sec, time elapsed 5664.63 sec\n","epoch 5, iter 15100, avg. loss 37.23, avg. ppl 8.07 cum. examples 70400, speed 3007.24 words/sec, time elapsed 5702.59 sec\n","epoch 5, iter 15200, avg. loss 37.22, avg. ppl 8.34 cum. examples 76800, speed 2991.82 words/sec, time elapsed 5740.13 sec\n","epoch 5, iter 15300, avg. loss 37.31, avg. ppl 8.29 cum. examples 83200, speed 2994.73 words/sec, time elapsed 5777.82 sec\n","epoch 5, iter 15400, avg. loss 37.28, avg. ppl 8.35 cum. examples 89600, speed 2987.37 words/sec, time elapsed 5815.45 sec\n","epoch 5, iter 15500, avg. loss 37.65, avg. ppl 8.43 cum. examples 96000, speed 2989.81 words/sec, time elapsed 5853.26 sec\n","epoch 5, iter 15600, avg. loss 37.13, avg. ppl 8.25 cum. examples 102400, speed 2977.63 words/sec, time elapsed 5891.08 sec\n","epoch 5, iter 15700, avg. loss 37.41, avg. ppl 8.38 cum. examples 108800, speed 3010.38 words/sec, time elapsed 5928.48 sec\n","epoch 5, iter 15800, avg. loss 37.69, avg. ppl 8.40 cum. examples 115200, speed 3031.35 words/sec, time elapsed 5965.87 sec\n","epoch 5, iter 15900, avg. loss 37.62, avg. ppl 8.36 cum. examples 121600, speed 3046.21 words/sec, time elapsed 6003.09 sec\n","epoch 5, iter 16000, avg. loss 37.59, avg. ppl 8.56 cum. examples 128000, speed 3022.49 words/sec, time elapsed 6040.17 sec\n","epoch 5, iter 16000, cum. loss 37.09, cum. ppl 8.17 cum. examples 128000\n","begin validation ...\n","validation: iter 16000, dev. ppl 17.237067\n","save currently the best model to [NMT_model_mul_atten]\n","save model parameters to [NMT_model_mul_atten]\n","epoch 5, iter 16100, avg. loss 38.24, avg. ppl 8.61 cum. examples 6400, speed 2704.44 words/sec, time elapsed 6082.20 sec\n","epoch 5, iter 16200, avg. loss 37.17, avg. ppl 8.47 cum. examples 12800, speed 3016.69 words/sec, time elapsed 6119.12 sec\n","epoch 5, iter 16300, avg. loss 37.90, avg. ppl 8.68 cum. examples 19200, speed 3022.50 words/sec, time elapsed 6156.26 sec\n","epoch 5, iter 16400, avg. loss 37.56, avg. ppl 8.34 cum. examples 25600, speed 3041.55 words/sec, time elapsed 6193.52 sec\n","epoch 5, iter 16500, avg. loss 38.10, avg. ppl 8.50 cum. examples 32000, speed 2968.30 words/sec, time elapsed 6231.91 sec\n","epoch 5, iter 16600, avg. loss 37.68, avg. ppl 8.48 cum. examples 38400, speed 3053.05 words/sec, time elapsed 6268.86 sec\n","epoch 5, iter 16700, avg. loss 38.01, avg. ppl 8.44 cum. examples 44800, speed 3057.10 words/sec, time elapsed 6306.16 sec\n","epoch 5, iter 16800, avg. loss 38.26, avg. ppl 8.67 cum. examples 51200, speed 3040.13 words/sec, time elapsed 6343.45 sec\n","epoch 5, iter 16900, avg. loss 38.11, avg. ppl 8.63 cum. examples 57600, speed 2967.39 words/sec, time elapsed 6381.59 sec\n","epoch 6, iter 17000, avg. loss 34.51, avg. ppl 6.98 cum. examples 63977, speed 2922.76 words/sec, time elapsed 6420.34 sec\n","epoch 6, iter 17100, avg. loss 33.07, avg. ppl 6.57 cum. examples 70377, speed 2979.87 words/sec, time elapsed 6458.07 sec\n","epoch 6, iter 17200, avg. loss 33.08, avg. ppl 6.50 cum. examples 76777, speed 3035.44 words/sec, time elapsed 6495.32 sec\n","epoch 6, iter 17300, avg. loss 33.24, avg. ppl 6.64 cum. examples 83177, speed 2921.53 words/sec, time elapsed 6533.77 sec\n","epoch 6, iter 17400, avg. loss 33.85, avg. ppl 6.75 cum. examples 89577, speed 3016.66 words/sec, time elapsed 6571.38 sec\n","epoch 6, iter 17500, avg. loss 33.75, avg. ppl 6.70 cum. examples 95977, speed 3034.81 words/sec, time elapsed 6608.80 sec\n","epoch 6, iter 17600, avg. loss 34.02, avg. ppl 6.72 cum. examples 102377, speed 3052.96 words/sec, time elapsed 6646.23 sec\n","epoch 6, iter 17700, avg. loss 34.24, avg. ppl 6.80 cum. examples 108777, speed 2972.56 words/sec, time elapsed 6684.69 sec\n","epoch 6, iter 17800, avg. loss 33.80, avg. ppl 6.88 cum. examples 115177, speed 3032.07 words/sec, time elapsed 6721.68 sec\n","epoch 6, iter 17900, avg. loss 34.03, avg. ppl 6.81 cum. examples 121577, speed 2993.84 words/sec, time elapsed 6759.60 sec\n","epoch 6, iter 18000, avg. loss 34.20, avg. ppl 6.92 cum. examples 127977, speed 3080.37 words/sec, time elapsed 6796.32 sec\n","epoch 6, iter 18000, cum. loss 35.64, cum. ppl 7.50 cum. examples 127977\n","begin validation ...\n","validation: iter 18000, dev. ppl 17.900360\n","hit patience 1\n","epoch 6, iter 18100, avg. loss 34.35, avg. ppl 7.01 cum. examples 6400, speed 2887.09 words/sec, time elapsed 6835.41 sec\n","epoch 6, iter 18200, avg. loss 34.70, avg. ppl 7.00 cum. examples 12800, speed 2994.14 words/sec, time elapsed 6873.54 sec\n","epoch 6, iter 18300, avg. loss 34.14, avg. ppl 7.00 cum. examples 19200, speed 3046.82 words/sec, time elapsed 6910.41 sec\n","epoch 6, iter 18400, avg. loss 34.85, avg. ppl 7.01 cum. examples 25600, speed 3044.80 words/sec, time elapsed 6948.02 sec\n","epoch 6, iter 18500, avg. loss 35.21, avg. ppl 7.31 cum. examples 32000, speed 3012.07 words/sec, time elapsed 6985.63 sec\n","epoch 6, iter 18600, avg. loss 35.19, avg. ppl 7.31 cum. examples 38400, speed 3000.17 words/sec, time elapsed 7023.38 sec\n","epoch 6, iter 18700, avg. loss 34.91, avg. ppl 7.20 cum. examples 44800, speed 2968.21 words/sec, time elapsed 7061.51 sec\n","epoch 6, iter 18800, avg. loss 34.93, avg. ppl 7.07 cum. examples 51200, speed 3045.42 words/sec, time elapsed 7099.03 sec\n","epoch 6, iter 18900, avg. loss 35.47, avg. ppl 7.38 cum. examples 57600, speed 2972.69 words/sec, time elapsed 7137.23 sec\n","epoch 6, iter 19000, avg. loss 34.67, avg. ppl 7.25 cum. examples 64000, speed 2983.73 words/sec, time elapsed 7174.77 sec\n","epoch 6, iter 19100, avg. loss 35.84, avg. ppl 7.44 cum. examples 70400, speed 3061.15 words/sec, time elapsed 7212.11 sec\n","epoch 6, iter 19200, avg. loss 34.76, avg. ppl 7.09 cum. examples 76800, speed 3018.50 words/sec, time elapsed 7249.73 sec\n","epoch 6, iter 19300, avg. loss 35.40, avg. ppl 7.35 cum. examples 83200, speed 3052.10 words/sec, time elapsed 7286.94 sec\n","epoch 6, iter 19400, avg. loss 34.72, avg. ppl 7.26 cum. examples 89600, speed 3040.94 words/sec, time elapsed 7323.79 sec\n","epoch 6, iter 19500, avg. loss 35.07, avg. ppl 7.31 cum. examples 96000, speed 3056.82 words/sec, time elapsed 7360.70 sec\n","epoch 6, iter 19600, avg. loss 35.52, avg. ppl 7.53 cum. examples 102400, speed 2992.51 words/sec, time elapsed 7398.33 sec\n","epoch 6, iter 19700, avg. loss 34.89, avg. ppl 7.47 cum. examples 108800, speed 2989.86 words/sec, time elapsed 7435.49 sec\n","epoch 6, iter 19800, avg. loss 35.99, avg. ppl 7.54 cum. examples 115200, speed 3064.97 words/sec, time elapsed 7472.68 sec\n","epoch 6, iter 19900, avg. loss 34.97, avg. ppl 7.41 cum. examples 121600, speed 3007.88 words/sec, time elapsed 7509.82 sec\n","epoch 6, iter 20000, avg. loss 35.52, avg. ppl 7.49 cum. examples 128000, speed 3033.26 words/sec, time elapsed 7547.05 sec\n","epoch 6, iter 20000, cum. loss 35.06, cum. ppl 7.27 cum. examples 128000\n","begin validation ...\n","validation: iter 20000, dev. ppl 17.507820\n","hit patience 2\n","epoch 6, iter 20100, avg. loss 35.55, avg. ppl 7.57 cum. examples 6400, speed 2876.55 words/sec, time elapsed 7586.13 sec\n","epoch 6, iter 20200, avg. loss 34.99, avg. ppl 7.29 cum. examples 12800, speed 2997.76 words/sec, time elapsed 7623.72 sec\n","epoch 6, iter 20300, avg. loss 35.15, avg. ppl 7.47 cum. examples 19200, speed 2970.49 words/sec, time elapsed 7661.39 sec\n","epoch 7, iter 20400, avg. loss 31.48, avg. ppl 5.91 cum. examples 25577, speed 3012.17 words/sec, time elapsed 7698.90 sec\n","epoch 7, iter 20500, avg. loss 30.73, avg. ppl 5.67 cum. examples 31977, speed 3021.17 words/sec, time elapsed 7736.41 sec\n","epoch 7, iter 20600, avg. loss 31.43, avg. ppl 5.80 cum. examples 38377, speed 3019.67 words/sec, time elapsed 7774.30 sec\n","epoch 7, iter 20700, avg. loss 30.71, avg. ppl 5.69 cum. examples 44777, speed 3061.01 words/sec, time elapsed 7811.25 sec\n","epoch 7, iter 20800, avg. loss 30.93, avg. ppl 5.81 cum. examples 51177, speed 3028.28 words/sec, time elapsed 7848.39 sec\n","epoch 7, iter 20900, avg. loss 31.35, avg. ppl 5.87 cum. examples 57577, speed 3012.19 words/sec, time elapsed 7886.03 sec\n","epoch 7, iter 21000, avg. loss 31.88, avg. ppl 6.11 cum. examples 63977, speed 2993.49 words/sec, time elapsed 7923.71 sec\n","epoch 7, iter 21100, avg. loss 31.71, avg. ppl 6.12 cum. examples 70377, speed 3016.09 words/sec, time elapsed 7960.85 sec\n","epoch 7, iter 21200, avg. loss 31.59, avg. ppl 6.09 cum. examples 76777, speed 2981.37 words/sec, time elapsed 7998.38 sec\n","epoch 7, iter 21300, avg. loss 32.37, avg. ppl 6.15 cum. examples 83177, speed 2969.52 words/sec, time elapsed 8036.77 sec\n","epoch 7, iter 21400, avg. loss 31.74, avg. ppl 6.12 cum. examples 89577, speed 3024.71 words/sec, time elapsed 8073.84 sec\n","epoch 7, iter 21500, avg. loss 32.76, avg. ppl 6.12 cum. examples 95977, speed 3057.61 words/sec, time elapsed 8111.70 sec\n","epoch 7, iter 21600, avg. loss 32.28, avg. ppl 6.29 cum. examples 102377, speed 2983.06 words/sec, time elapsed 8149.36 sec\n","epoch 7, iter 21700, avg. loss 32.38, avg. ppl 6.19 cum. examples 108777, speed 2963.03 words/sec, time elapsed 8187.72 sec\n","epoch 7, iter 21800, avg. loss 32.84, avg. ppl 6.38 cum. examples 115177, speed 3066.52 words/sec, time elapsed 8224.72 sec\n","epoch 7, iter 21900, avg. loss 32.92, avg. ppl 6.45 cum. examples 121577, speed 2970.92 words/sec, time elapsed 8262.76 sec\n","epoch 7, iter 22000, avg. loss 32.43, avg. ppl 6.30 cum. examples 127977, speed 2997.72 words/sec, time elapsed 8300.38 sec\n","epoch 7, iter 22000, cum. loss 32.36, cum. ppl 6.25 cum. examples 127977\n","begin validation ...\n","validation: iter 22000, dev. ppl 17.925898\n","hit patience 3\n","hit #1 trial\n","load previously best model and decay learning rate to 0.000500\n","restore parameters of the optimizers\n","epoch 7, iter 22100, avg. loss 33.41, avg. ppl 6.62 cum. examples 6400, speed 2872.61 words/sec, time elapsed 8339.76 sec\n","epoch 7, iter 22200, avg. loss 33.00, avg. ppl 6.45 cum. examples 12800, speed 2987.43 words/sec, time elapsed 8377.70 sec\n","epoch 7, iter 22300, avg. loss 33.33, avg. ppl 6.46 cum. examples 19200, speed 3067.42 words/sec, time elapsed 8414.96 sec\n","epoch 7, iter 22400, avg. loss 33.02, avg. ppl 6.47 cum. examples 25600, speed 3054.27 words/sec, time elapsed 8452.03 sec\n","epoch 7, iter 22500, avg. loss 32.55, avg. ppl 6.33 cum. examples 32000, speed 3023.52 words/sec, time elapsed 8489.37 sec\n","epoch 7, iter 22600, avg. loss 33.49, avg. ppl 6.63 cum. examples 38400, speed 3001.90 words/sec, time elapsed 8527.12 sec\n","epoch 7, iter 22700, avg. loss 33.12, avg. ppl 6.57 cum. examples 44800, speed 3038.05 words/sec, time elapsed 8564.17 sec\n","epoch 7, iter 22800, avg. loss 32.91, avg. ppl 6.51 cum. examples 51200, speed 3025.24 words/sec, time elapsed 8601.32 sec\n","epoch 7, iter 22900, avg. loss 33.66, avg. ppl 6.63 cum. examples 57600, speed 3028.16 words/sec, time elapsed 8638.91 sec\n","epoch 7, iter 23000, avg. loss 33.20, avg. ppl 6.49 cum. examples 64000, speed 2997.14 words/sec, time elapsed 8676.83 sec\n","epoch 7, iter 23100, avg. loss 33.15, avg. ppl 6.59 cum. examples 70400, speed 2962.65 words/sec, time elapsed 8714.83 sec\n","epoch 7, iter 23200, avg. loss 33.45, avg. ppl 6.58 cum. examples 76800, speed 2981.89 words/sec, time elapsed 8752.94 sec\n","epoch 7, iter 23300, avg. loss 33.04, avg. ppl 6.51 cum. examples 83200, speed 3036.81 words/sec, time elapsed 8790.11 sec\n","epoch 7, iter 23400, avg. loss 32.94, avg. ppl 6.58 cum. examples 89600, speed 2956.95 words/sec, time elapsed 8827.94 sec\n","epoch 7, iter 23500, avg. loss 33.04, avg. ppl 6.50 cum. examples 96000, speed 3009.03 words/sec, time elapsed 8865.47 sec\n","epoch 7, iter 23600, avg. loss 33.26, avg. ppl 6.64 cum. examples 102400, speed 3018.69 words/sec, time elapsed 8902.71 sec\n","epoch 8, iter 23700, avg. loss 32.89, avg. ppl 6.47 cum. examples 108777, speed 2946.25 words/sec, time elapsed 8940.84 sec\n","epoch 8, iter 23800, avg. loss 31.34, avg. ppl 5.87 cum. examples 115177, speed 3012.78 words/sec, time elapsed 8978.44 sec\n","epoch 8, iter 23900, avg. loss 31.37, avg. ppl 5.84 cum. examples 121577, speed 3002.48 words/sec, time elapsed 9016.33 sec\n","epoch 8, iter 24000, avg. loss 31.05, avg. ppl 5.90 cum. examples 127977, speed 3042.54 words/sec, time elapsed 9053.13 sec\n","epoch 8, iter 24000, cum. loss 32.86, cum. ppl 6.43 cum. examples 127977\n","begin validation ...\n","validation: iter 24000, dev. ppl 17.188937\n","save currently the best model to [NMT_model_mul_atten]\n","save model parameters to [NMT_model_mul_atten]\n","epoch 8, iter 24100, avg. loss 31.22, avg. ppl 5.87 cum. examples 6400, speed 2710.82 words/sec, time elapsed 9094.78 sec\n","epoch 8, iter 24200, avg. loss 31.46, avg. ppl 5.92 cum. examples 12800, speed 3018.79 words/sec, time elapsed 9132.27 sec\n","epoch 8, iter 24300, avg. loss 31.97, avg. ppl 6.07 cum. examples 19200, speed 2991.22 words/sec, time elapsed 9170.19 sec\n","epoch 8, iter 24400, avg. loss 31.83, avg. ppl 6.06 cum. examples 25600, speed 2993.85 words/sec, time elapsed 9207.97 sec\n","epoch 8, iter 24500, avg. loss 31.26, avg. ppl 5.91 cum. examples 32000, speed 3024.66 words/sec, time elapsed 9245.18 sec\n","epoch 8, iter 24600, avg. loss 32.10, avg. ppl 6.08 cum. examples 38400, speed 3014.24 words/sec, time elapsed 9282.95 sec\n","epoch 8, iter 24700, avg. loss 31.50, avg. ppl 6.02 cum. examples 44800, speed 3021.97 words/sec, time elapsed 9320.13 sec\n","epoch 8, iter 24800, avg. loss 32.37, avg. ppl 6.12 cum. examples 51200, speed 3020.64 words/sec, time elapsed 9358.00 sec\n","epoch 8, iter 24900, avg. loss 32.55, avg. ppl 6.22 cum. examples 57600, speed 3081.09 words/sec, time elapsed 9395.00 sec\n","epoch 8, iter 25000, avg. loss 32.19, avg. ppl 6.20 cum. examples 64000, speed 2977.61 words/sec, time elapsed 9432.91 sec\n","epoch 8, iter 25100, avg. loss 32.33, avg. ppl 6.21 cum. examples 70400, speed 3040.03 words/sec, time elapsed 9470.18 sec\n","epoch 8, iter 25200, avg. loss 31.44, avg. ppl 6.12 cum. examples 76800, speed 2977.86 words/sec, time elapsed 9507.48 sec\n","epoch 8, iter 25300, avg. loss 32.02, avg. ppl 6.07 cum. examples 83200, speed 3026.22 words/sec, time elapsed 9545.02 sec\n","epoch 8, iter 25400, avg. loss 32.01, avg. ppl 6.21 cum. examples 89600, speed 2970.67 words/sec, time elapsed 9582.79 sec\n","epoch 8, iter 25500, avg. loss 31.92, avg. ppl 6.23 cum. examples 96000, speed 2997.37 words/sec, time elapsed 9620.07 sec\n","epoch 8, iter 25600, avg. loss 32.72, avg. ppl 6.29 cum. examples 102400, speed 2996.71 words/sec, time elapsed 9658.07 sec\n","epoch 8, iter 25700, avg. loss 32.07, avg. ppl 6.24 cum. examples 108800, speed 2972.91 words/sec, time elapsed 9695.76 sec\n","epoch 8, iter 25800, avg. loss 31.98, avg. ppl 6.17 cum. examples 115200, speed 2967.00 words/sec, time elapsed 9733.69 sec\n","epoch 8, iter 25900, avg. loss 32.66, avg. ppl 6.29 cum. examples 121600, speed 3006.47 words/sec, time elapsed 9771.49 sec\n","epoch 8, iter 26000, avg. loss 32.89, avg. ppl 6.35 cum. examples 128000, speed 3027.31 words/sec, time elapsed 9809.10 sec\n","epoch 8, iter 26000, cum. loss 32.02, cum. ppl 6.13 cum. examples 128000\n","begin validation ...\n","validation: iter 26000, dev. ppl 16.932193\n","save currently the best model to [NMT_model_mul_atten]\n","save model parameters to [NMT_model_mul_atten]\n","epoch 8, iter 26100, avg. loss 32.80, avg. ppl 6.31 cum. examples 6400, speed 2765.73 words/sec, time elapsed 9850.31 sec\n","epoch 8, iter 26200, avg. loss 32.77, avg. ppl 6.26 cum. examples 12800, speed 2979.46 words/sec, time elapsed 9888.69 sec\n","epoch 8, iter 26300, avg. loss 32.85, avg. ppl 6.46 cum. examples 19200, speed 3029.63 words/sec, time elapsed 9925.88 sec\n","epoch 8, iter 26400, avg. loss 33.13, avg. ppl 6.35 cum. examples 25600, speed 3002.59 words/sec, time elapsed 9964.08 sec\n","epoch 8, iter 26500, avg. loss 31.99, avg. ppl 6.21 cum. examples 32000, speed 3030.37 words/sec, time elapsed 10001.06 sec\n","epoch 8, iter 26600, avg. loss 32.66, avg. ppl 6.30 cum. examples 38400, speed 3049.45 words/sec, time elapsed 10038.31 sec\n","epoch 8, iter 26700, avg. loss 32.15, avg. ppl 6.30 cum. examples 44800, speed 3020.60 words/sec, time elapsed 10075.32 sec\n","epoch 8, iter 26800, avg. loss 32.63, avg. ppl 6.30 cum. examples 51200, speed 2970.38 words/sec, time elapsed 10113.52 sec\n","epoch 8, iter 26900, avg. loss 32.63, avg. ppl 6.39 cum. examples 57600, speed 3014.61 words/sec, time elapsed 10150.86 sec\n","epoch 8, iter 27000, avg. loss 32.97, avg. ppl 6.35 cum. examples 64000, speed 2992.19 words/sec, time elapsed 10189.00 sec\n","epoch 9, iter 27100, avg. loss 31.47, avg. ppl 6.08 cum. examples 70377, speed 3018.15 words/sec, time elapsed 10225.85 sec\n","epoch 9, iter 27200, avg. loss 28.89, avg. ppl 5.26 cum. examples 76777, speed 2985.56 words/sec, time elapsed 10263.17 sec\n","epoch 9, iter 27300, avg. loss 29.39, avg. ppl 5.27 cum. examples 83177, speed 2960.69 words/sec, time elapsed 10301.42 sec\n","epoch 9, iter 27400, avg. loss 29.88, avg. ppl 5.30 cum. examples 89577, speed 3046.18 words/sec, time elapsed 10339.06 sec\n","epoch 9, iter 27500, avg. loss 29.24, avg. ppl 5.28 cum. examples 95977, speed 3008.55 words/sec, time elapsed 10376.46 sec\n","epoch 9, iter 27600, avg. loss 28.90, avg. ppl 5.22 cum. examples 102377, speed 3032.72 words/sec, time elapsed 10413.37 sec\n","epoch 9, iter 27700, avg. loss 29.87, avg. ppl 5.39 cum. examples 108777, speed 2989.42 words/sec, time elapsed 10451.32 sec\n","epoch 9, iter 27800, avg. loss 29.89, avg. ppl 5.40 cum. examples 115177, speed 3041.18 words/sec, time elapsed 10488.60 sec\n","epoch 9, iter 27900, avg. loss 29.85, avg. ppl 5.30 cum. examples 121577, speed 3054.60 words/sec, time elapsed 10526.12 sec\n","epoch 9, iter 28000, avg. loss 29.78, avg. ppl 5.45 cum. examples 127977, speed 2926.56 words/sec, time elapsed 10564.53 sec\n","epoch 9, iter 28000, cum. loss 31.19, cum. ppl 5.84 cum. examples 127977\n","begin validation ...\n","validation: iter 28000, dev. ppl 17.344305\n","hit patience 1\n","epoch 9, iter 28100, avg. loss 30.19, avg. ppl 5.44 cum. examples 6400, speed 2953.60 words/sec, time elapsed 10603.16 sec\n","epoch 9, iter 28200, avg. loss 29.96, avg. ppl 5.49 cum. examples 12800, speed 3061.65 words/sec, time elapsed 10639.94 sec\n","epoch 9, iter 28300, avg. loss 29.94, avg. ppl 5.48 cum. examples 19200, speed 3010.92 words/sec, time elapsed 10677.35 sec\n","epoch 9, iter 28400, avg. loss 29.87, avg. ppl 5.45 cum. examples 25600, speed 3030.14 words/sec, time elapsed 10714.55 sec\n","epoch 9, iter 28500, avg. loss 29.87, avg. ppl 5.44 cum. examples 32000, speed 3001.97 words/sec, time elapsed 10752.14 sec\n","epoch 9, iter 28600, avg. loss 30.04, avg. ppl 5.55 cum. examples 38400, speed 3048.96 words/sec, time elapsed 10788.95 sec\n","epoch 9, iter 28700, avg. loss 30.19, avg. ppl 5.58 cum. examples 44800, speed 3068.75 words/sec, time elapsed 10825.56 sec\n","epoch 9, iter 28800, avg. loss 30.02, avg. ppl 5.48 cum. examples 51200, speed 2980.43 words/sec, time elapsed 10863.48 sec\n","epoch 9, iter 28900, avg. loss 30.26, avg. ppl 5.52 cum. examples 57600, speed 3002.01 words/sec, time elapsed 10901.26 sec\n","epoch 9, iter 29000, avg. loss 30.92, avg. ppl 5.65 cum. examples 64000, speed 3083.31 words/sec, time elapsed 10938.34 sec\n","epoch 9, iter 29100, avg. loss 30.66, avg. ppl 5.62 cum. examples 70400, speed 3030.97 words/sec, time elapsed 10975.85 sec\n","epoch 9, iter 29200, avg. loss 30.71, avg. ppl 5.57 cum. examples 76800, speed 3005.85 words/sec, time elapsed 11013.94 sec\n","epoch 9, iter 29300, avg. loss 30.71, avg. ppl 5.67 cum. examples 83200, speed 3025.05 words/sec, time elapsed 11051.39 sec\n","epoch 9, iter 29400, avg. loss 31.07, avg. ppl 5.75 cum. examples 89600, speed 3040.01 words/sec, time elapsed 11088.76 sec\n","epoch 9, iter 29500, avg. loss 31.09, avg. ppl 5.77 cum. examples 96000, speed 3023.48 words/sec, time elapsed 11126.33 sec\n","epoch 9, iter 29600, avg. loss 30.77, avg. ppl 5.75 cum. examples 102400, speed 2978.31 words/sec, time elapsed 11164.13 sec\n","epoch 9, iter 29700, avg. loss 30.63, avg. ppl 5.71 cum. examples 108800, speed 2964.52 words/sec, time elapsed 11202.10 sec\n","epoch 9, iter 29800, avg. loss 30.54, avg. ppl 5.72 cum. examples 115200, speed 2954.84 words/sec, time elapsed 11240.03 sec\n","epoch 9, iter 29900, avg. loss 30.53, avg. ppl 5.72 cum. examples 121600, speed 2979.82 words/sec, time elapsed 11277.63 sec\n","epoch 9, iter 30000, avg. loss 30.88, avg. ppl 5.70 cum. examples 128000, speed 2996.00 words/sec, time elapsed 11315.55 sec\n","epoch 9, iter 30000, cum. loss 30.44, cum. ppl 5.60 cum. examples 128000\n","begin validation ...\n","validation: iter 30000, dev. ppl 17.008516\n","hit patience 2\n","epoch 9, iter 30100, avg. loss 30.87, avg. ppl 5.75 cum. examples 6400, speed 2867.06 words/sec, time elapsed 11354.95 sec\n","epoch 9, iter 30200, avg. loss 31.29, avg. ppl 5.82 cum. examples 12800, speed 3040.74 words/sec, time elapsed 11392.33 sec\n","epoch 9, iter 30300, avg. loss 31.25, avg. ppl 5.81 cum. examples 19200, speed 3040.64 words/sec, time elapsed 11429.71 sec\n","epoch 9, iter 30400, avg. loss 31.24, avg. ppl 5.80 cum. examples 25600, speed 3036.52 words/sec, time elapsed 11467.18 sec\n","epoch 10, iter 30500, avg. loss 29.80, avg. ppl 5.41 cum. examples 31977, speed 3012.92 words/sec, time elapsed 11504.53 sec\n","epoch 10, iter 30600, avg. loss 27.72, avg. ppl 4.80 cum. examples 38377, speed 3015.12 words/sec, time elapsed 11542.07 sec\n","epoch 10, iter 30700, avg. loss 27.35, avg. ppl 4.78 cum. examples 44777, speed 3044.63 words/sec, time elapsed 11578.84 sec\n","epoch 10, iter 30800, avg. loss 27.71, avg. ppl 4.77 cum. examples 51177, speed 3064.00 words/sec, time elapsed 11615.87 sec\n","epoch 10, iter 30900, avg. loss 27.93, avg. ppl 4.85 cum. examples 57577, speed 2969.06 words/sec, time elapsed 11654.02 sec\n","epoch 10, iter 31000, avg. loss 28.07, avg. ppl 4.82 cum. examples 63977, speed 3010.94 words/sec, time elapsed 11691.93 sec\n","epoch 10, iter 31100, avg. loss 27.87, avg. ppl 4.89 cum. examples 70377, speed 2963.95 words/sec, time elapsed 11729.85 sec\n","epoch 10, iter 31200, avg. loss 27.93, avg. ppl 4.85 cum. examples 76777, speed 3036.10 words/sec, time elapsed 11767.13 sec\n","epoch 10, iter 31300, avg. loss 28.33, avg. ppl 4.95 cum. examples 83177, speed 3041.06 words/sec, time elapsed 11804.40 sec\n","epoch 10, iter 31400, avg. loss 27.72, avg. ppl 4.89 cum. examples 89577, speed 3024.96 words/sec, time elapsed 11841.34 sec\n","epoch 10, iter 31500, avg. loss 28.09, avg. ppl 4.94 cum. examples 95977, speed 3002.79 words/sec, time elapsed 11878.82 sec\n","epoch 10, iter 31600, avg. loss 28.55, avg. ppl 5.04 cum. examples 102377, speed 3016.38 words/sec, time elapsed 11916.25 sec\n","epoch 10, iter 31700, avg. loss 29.02, avg. ppl 5.14 cum. examples 108777, speed 3006.22 words/sec, time elapsed 11953.99 sec\n","epoch 10, iter 31800, avg. loss 28.47, avg. ppl 4.98 cum. examples 115177, speed 3041.13 words/sec, time elapsed 11991.30 sec\n","epoch 10, iter 31900, avg. loss 28.70, avg. ppl 5.12 cum. examples 121577, speed 2976.59 words/sec, time elapsed 12029.06 sec\n","epoch 10, iter 32000, avg. loss 28.62, avg. ppl 5.12 cum. examples 127977, speed 2975.76 words/sec, time elapsed 12066.76 sec\n","epoch 10, iter 32000, cum. loss 28.83, cum. ppl 5.11 cum. examples 127977\n","begin validation ...\n","validation: iter 32000, dev. ppl 18.123811\n","hit patience 3\n","hit #2 trial\n","load previously best model and decay learning rate to 0.000250\n","restore parameters of the optimizers\n","epoch 10, iter 32100, avg. loss 29.63, avg. ppl 5.37 cum. examples 6400, speed 2848.17 words/sec, time elapsed 12106.37 sec\n","epoch 10, iter 32200, avg. loss 29.97, avg. ppl 5.38 cum. examples 12800, speed 3019.10 words/sec, time elapsed 12144.14 sec\n","epoch 10, iter 32300, avg. loss 29.37, avg. ppl 5.27 cum. examples 19200, speed 2997.16 words/sec, time elapsed 12181.89 sec\n","epoch 10, iter 32400, avg. loss 29.16, avg. ppl 5.23 cum. examples 25600, speed 2973.48 words/sec, time elapsed 12219.82 sec\n","epoch 10, iter 32500, avg. loss 29.61, avg. ppl 5.30 cum. examples 32000, speed 2973.26 words/sec, time elapsed 12258.05 sec\n","epoch 10, iter 32600, avg. loss 29.78, avg. ppl 5.38 cum. examples 38400, speed 2975.33 words/sec, time elapsed 12296.13 sec\n","epoch 10, iter 32700, avg. loss 29.38, avg. ppl 5.33 cum. examples 44800, speed 2978.72 words/sec, time elapsed 12333.86 sec\n","epoch 10, iter 32800, avg. loss 29.81, avg. ppl 5.38 cum. examples 51200, speed 3044.48 words/sec, time elapsed 12371.09 sec\n","epoch 10, iter 32900, avg. loss 30.13, avg. ppl 5.44 cum. examples 57600, speed 3069.25 words/sec, time elapsed 12408.19 sec\n","epoch 10, iter 33000, avg. loss 29.45, avg. ppl 5.29 cum. examples 64000, speed 3033.35 words/sec, time elapsed 12445.48 sec\n","epoch 10, iter 33100, avg. loss 29.86, avg. ppl 5.46 cum. examples 70400, speed 3036.97 words/sec, time elapsed 12482.56 sec\n","epoch 10, iter 33200, avg. loss 30.29, avg. ppl 5.47 cum. examples 76800, speed 3055.25 words/sec, time elapsed 12519.90 sec\n","epoch 10, iter 33300, avg. loss 29.87, avg. ppl 5.44 cum. examples 83200, speed 3000.77 words/sec, time elapsed 12557.51 sec\n","epoch 10, iter 33400, avg. loss 30.35, avg. ppl 5.52 cum. examples 89600, speed 3067.30 words/sec, time elapsed 12594.57 sec\n","epoch 10, iter 33500, avg. loss 29.57, avg. ppl 5.36 cum. examples 96000, speed 2978.80 words/sec, time elapsed 12632.41 sec\n","epoch 10, iter 33600, avg. loss 30.14, avg. ppl 5.50 cum. examples 102400, speed 2999.41 words/sec, time elapsed 12670.15 sec\n","epoch 10, iter 33700, avg. loss 30.05, avg. ppl 5.45 cum. examples 108800, speed 2979.06 words/sec, time elapsed 12708.24 sec\n","epoch 10, iter 33800, avg. loss 29.75, avg. ppl 5.38 cum. examples 115200, speed 2984.06 words/sec, time elapsed 12746.16 sec\n","epoch 11, iter 33900, avg. loss 29.05, avg. ppl 5.25 cum. examples 121577, speed 2969.37 words/sec, time elapsed 12783.80 sec\n","epoch 11, iter 34000, avg. loss 28.56, avg. ppl 5.02 cum. examples 127977, speed 3093.95 words/sec, time elapsed 12820.42 sec\n","epoch 11, iter 34000, cum. loss 29.69, cum. ppl 5.36 cum. examples 127977\n","begin validation ...\n","validation: iter 34000, dev. ppl 17.167306\n","hit patience 1\n","epoch 11, iter 34100, avg. loss 28.86, avg. ppl 5.10 cum. examples 6400, speed 2862.03 words/sec, time elapsed 12860.01 sec\n","epoch 11, iter 34200, avg. loss 29.04, avg. ppl 5.13 cum. examples 12800, speed 3029.85 words/sec, time elapsed 12897.55 sec\n","epoch 11, iter 34300, avg. loss 28.76, avg. ppl 5.15 cum. examples 19200, speed 3011.08 words/sec, time elapsed 12934.84 sec\n","epoch 11, iter 34400, avg. loss 28.43, avg. ppl 5.07 cum. examples 25600, speed 2978.42 words/sec, time elapsed 12972.48 sec\n","epoch 11, iter 34500, avg. loss 28.76, avg. ppl 5.12 cum. examples 32000, speed 3026.29 words/sec, time elapsed 13009.74 sec\n","epoch 11, iter 34600, avg. loss 28.36, avg. ppl 5.02 cum. examples 38400, speed 3062.28 words/sec, time elapsed 13046.48 sec\n","epoch 11, iter 34700, avg. loss 29.09, avg. ppl 5.19 cum. examples 44800, speed 3014.78 words/sec, time elapsed 13084.00 sec\n","epoch 11, iter 34800, avg. loss 29.01, avg. ppl 5.12 cum. examples 51200, speed 3051.87 words/sec, time elapsed 13121.25 sec\n","epoch 11, iter 34900, avg. loss 28.80, avg. ppl 5.14 cum. examples 57600, speed 2984.21 words/sec, time elapsed 13158.96 sec\n","epoch 11, iter 35000, avg. loss 28.85, avg. ppl 5.14 cum. examples 64000, speed 3017.97 words/sec, time elapsed 13196.33 sec\n","epoch 11, iter 35100, avg. loss 29.14, avg. ppl 5.22 cum. examples 70400, speed 3004.11 words/sec, time elapsed 13233.90 sec\n","epoch 11, iter 35200, avg. loss 29.09, avg. ppl 5.20 cum. examples 76800, speed 2996.82 words/sec, time elapsed 13271.59 sec\n","epoch 11, iter 35300, avg. loss 29.03, avg. ppl 5.18 cum. examples 83200, speed 3001.89 words/sec, time elapsed 13309.23 sec\n","epoch 11, iter 35400, avg. loss 29.07, avg. ppl 5.13 cum. examples 89600, speed 3016.84 words/sec, time elapsed 13346.93 sec\n","epoch 11, iter 35500, avg. loss 29.50, avg. ppl 5.33 cum. examples 96000, speed 3002.73 words/sec, time elapsed 13384.50 sec\n","epoch 11, iter 35600, avg. loss 29.10, avg. ppl 5.19 cum. examples 102400, speed 3014.29 words/sec, time elapsed 13422.01 sec\n","epoch 11, iter 35700, avg. loss 28.54, avg. ppl 5.17 cum. examples 108800, speed 3062.61 words/sec, time elapsed 13458.30 sec\n","epoch 11, iter 35800, avg. loss 29.64, avg. ppl 5.35 cum. examples 115200, speed 2994.04 words/sec, time elapsed 13496.07 sec\n","epoch 11, iter 35900, avg. loss 29.79, avg. ppl 5.27 cum. examples 121600, speed 3042.96 words/sec, time elapsed 13533.74 sec\n","epoch 11, iter 36000, avg. loss 29.69, avg. ppl 5.26 cum. examples 128000, speed 2990.56 words/sec, time elapsed 13572.04 sec\n","epoch 11, iter 36000, cum. loss 29.03, cum. ppl 5.17 cum. examples 128000\n","begin validation ...\n","validation: iter 36000, dev. ppl 17.014988\n","hit patience 2\n","epoch 11, iter 36100, avg. loss 29.42, avg. ppl 5.24 cum. examples 6400, speed 2897.71 words/sec, time elapsed 13611.28 sec\n","epoch 11, iter 36200, avg. loss 29.53, avg. ppl 5.22 cum. examples 12800, speed 3029.37 words/sec, time elapsed 13649.04 sec\n","epoch 11, iter 36300, avg. loss 29.62, avg. ppl 5.30 cum. examples 19200, speed 3012.98 words/sec, time elapsed 13686.76 sec\n","epoch 11, iter 36400, avg. loss 29.74, avg. ppl 5.30 cum. examples 25600, speed 3036.69 words/sec, time elapsed 13724.34 sec\n","epoch 11, iter 36500, avg. loss 29.24, avg. ppl 5.21 cum. examples 32000, speed 2968.15 words/sec, time elapsed 13762.53 sec\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-59140482d824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# clip gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"M6SoJjGPsher","colab_type":"text"},"source":["## Test"]},{"cell_type":"markdown","metadata":{"id":"vSdmzTZ4scw9","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"LTvAYbLFuqUd","colab_type":"code","colab":{}},"source":["def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n","    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n","    @param model (NMT): NMT Model\n","    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n","    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n","    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n","    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    hypotheses = []\n","    with torch.no_grad():\n","        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n","            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n","\n","            hypotheses.append(example_hyps)\n","\n","    if was_training: model.train(was_training)\n","\n","    return hypotheses"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGJbEn1Ew-S-","colab_type":"code","colab":{}},"source":["def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n","    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n","    @param references (List[List[str]]): a list of gold-standard reference target sentences\n","    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n","    @returns bleu_score: corpus-level BLEU score\n","    \"\"\"\n","    if references[0][0] == '<s>':\n","        references = [ref[1:-1] for ref in references]\n","    bleu_score = corpus_bleu([[ref] for ref in references],\n","                             [hyp.value for hyp in hypotheses])\n","    return bleu_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zW2HJYn0wXb5","colab_type":"code","colab":{}},"source":["##Test\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eW9oOGYXVdIa","colab_type":"code","colab":{}},"source":["def decode():\n","    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n","    If the target gold-standard sentences are given, the function also computes\n","    corpus-level BLEU score.\n","    @param args (Dict): args from cmd line\n","    \"\"\"\n","\n","    print(\"load test source sentences\", file=sys.stderr)\n","    test_data_src = read_corpus(test_es, source='src')\n","    \n","    \n","    print(\"load test target sentences\", file=sys.stderr)\n","    test_data_tgt = read_corpus(test_en, source='tgt')\n","\n","    print(\"load trained model\", file=sys.stderr)\n","    model = NMT.load(model_save_path)\n","\n","    #device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n","    if torch.cuda.device_count()>0:\n","        print(\"Transfer to cuda!!\")\n","        model = model.to(torch.device(\"cuda:0\"))\n","\n","    hypotheses = beam_search(model, test_data_src,\n","                             beam_size=5,\n","                             max_decoding_time_step=70)\n","\n","\n","    top_hypotheses = [hyps[0] for hyps in hypotheses]\n","    bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n","    print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n","\n","    with open('test_output.txt', 'w') as f:\n","        for src_sent, hyps in zip(test_data_src, hypotheses):\n","            top_hyp = hyps[0]\n","            hyp_sent = ' '.join(top_hyp.value)\n","            f.write(hyp_sent + '\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xexMtTi-Vcwg","colab_type":"code","outputId":"f937688d-a1e7-41ea-ba68-f73158f58ac1","executionInfo":{"status":"ok","timestamp":1558522986213,"user_tz":-480,"elapsed":698948,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["decode()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["load test source sentences\n","load test target sentences\n","load trained model\n"],"name":"stderr"},{"output_type":"stream","text":["Transfer to cuda!!\n","Decoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8064/8064 [11:32<00:00, 11.65it/s]\n"],"name":"stdout"},{"output_type":"stream","text":["Corpus BLEU: 22.08793926290774\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3XnkzmxaAo31","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ge7ElVDwXeP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pE6O6avwwXg-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DebG7DtyO8MF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jS4qJNbvO8OQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lB-e06ftO8Qz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"apPmV31QO8TS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zk_1TvRkSA2G","colab_type":"text"},"source":["## B"]},{"cell_type":"code","metadata":{"id":"tyHJgeMwwXjj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWF3Q5w1wXmS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfJd_QAPwXo6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLAJC9m4wXrX","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N9G69M1QwXuQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}