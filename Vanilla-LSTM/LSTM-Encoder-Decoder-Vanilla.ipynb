{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Encoder-Decoder-Vanilla.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"oqvWLwBtSZlP","colab_type":"text"},"source":["### Objective\n","\n","Build a machine translation model (Spainish --> English), using standard Encoder-Decoder architecture."]},{"cell_type":"markdown","metadata":{"id":"piJNnKlIK2DB","colab_type":"text"},"source":["### SetUP"]},{"cell_type":"code","metadata":{"id":"sVDW6zrCJAw0","colab_type":"code","outputId":"1871c391-a44b-43ac-cc07-4bdc0f744749","executionInfo":{"status":"ok","timestamp":1563859229659,"user_tz":-480,"elapsed":18196,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# connect to google drive\n","import os\n","import numpy as np\n","\n","# mount google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OnpTNdjCJAzS","colab_type":"code","outputId":"0718d951-5bd8-418f-bd5a-58a3546e931a","executionInfo":{"status":"ok","timestamp":1563859240734,"user_tz":-480,"elapsed":7367,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["# Check for GPU free memory\n","# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","#!pip install psutil\n","#!pip install humanize\n","import psutil\n","import humanize\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n","  process = psutil.Process(os.getpid())\n","  print('='*40)\n","  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n","  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","  print('='*40)\n","printm() "],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting gputil\n","  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n","Building wheels for collected packages: gputil\n","  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n","Successfully built gputil\n","Installing collected packages: gputil\n","Successfully installed gputil-1.4.0\n","========================================\n","Gen RAM Free: 12.9 GB  | Proc size: 156.3 MB\n","GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n","========================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bBIeBzHNJA1O","colab_type":"code","colab":{}},"source":["# change root directory such that models are saved in google drive during training\n","root_dir = \"/content/gdrive/My Drive/NLP/Git_MT\"\n","os.chdir(root_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZOJWXuhaJXWJ","colab_type":"text"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"kUYhCDHeJA3m","colab_type":"code","colab":{}},"source":["# insert the path for utility custom functions\n","import sys\n","sys.path.insert(0, os.path.join(root_dir, 'code_utils'))\n","\n","# custom python functions and classes\n","from utils import read_corpus, batch_iter, pad_sents\n","from vocab import Vocab, VocabEntry"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRT_jj_JJA5q","colab_type":"code","colab":{}},"source":["# basic packages\n","import math\n","import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from collections import Counter, namedtuple\n","from docopt import docopt\n","from itertools import chain\n","import json\n","from typing import List, Tuple, Dict, Set, Union\n","from docopt import docopt\n","\n","#pytorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.utils\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","\n","\n","\n","#others\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","from tqdm import tqdm\n","from IPython.core.debugger import set_trace"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lw20bMVrJA70","colab_type":"code","colab":{}},"source":["# Logger\n","\n","import logging\n","logger = logging.getLogger(\"Tensor\")\n","\n","file_handler = logging.FileHandler(\"Tensor_file.log\")\n","stream_handler = logging.StreamHandler()\n","formatter = logging.Formatter('%(asctime)s:%(name)s:%(message)s')\n","\n","file_handler.setFormatter(formatter)\n","stream_handler.setFormatter(formatter)\n","\n","logger.addHandler(file_handler)\n","\n","logger.addHandler(stream_handler)\n","\n","logger.setLevel(logging.DEBUG)\n","\n","# A helper function to check how tensor sizes change\n","def log_size(tsr: torch.Tensor, name: str):\n","    #cls = getclass()\n","    logger.debug(msg=f\"{name} ==> size={tsr.shape}\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ng8FWO6YRu6X","colab_type":"code","colab":{}},"source":["#! pip freeze > requirements.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JWItqVbOK-Si","colab_type":"text"},"source":["### Load Data: EDA"]},{"cell_type":"code","metadata":{"id":"5R85JYgFJBCp","colab_type":"code","outputId":"b4dd3717-cc24-40aa-b326-85ebcd385153","executionInfo":{"status":"ok","timestamp":1563859553488,"user_tz":-480,"elapsed":4798,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":612}},"source":["# load data\n","train_es = 'en_es_data/train.es'\n","train_en = 'en_es_data/train.en'\n","\n","dev_es = 'en_es_data/dev.es'\n","dev_en = 'en_es_data/dev.en'\n","\n","test_es = 'en_es_data/test.es'\n","test_en = 'en_es_data/test.en'\n","\n","\n","train_data_src = read_corpus(train_es, source='src')\n","train_data_tgt = read_corpus(train_en, source='tgt')\n","\n","dev_data_src = read_corpus(dev_es, source='src')\n","dev_data_tgt = read_corpus(dev_en, source='tgt')\n","\n","test_data_src = read_corpus(test_es, source='src')\n","test_data_tgt = read_corpus(test_en, source='tgt')\n","\n","train_data = list(zip(train_data_src,train_data_tgt))\n","dev_data = list(zip(dev_data_src,dev_data_tgt))\n","test_data = list(zip(test_data_src,test_data_tgt))\n","\n","#\n","print(\"==\"*40)\n","print(\"Number of examples in train: {}\".format(len(train_data)))\n","print(\"Number of examples in valid: {}\".format(len(dev_data)))\n","print(\"Number of examples in test: {}\".format(len(test_data)))\n","#\n","print(\"==\"*40)\n","print(\"Spanish --> English\")\n","es, en = next(iter(dev_data))\n","print(\"Sp: {}\".format(' '.join(es)))\n","print(\"En: {}\".format(' '.join(en)))\n","print(\"==\"*40)\n","\n","# Build Vocab with train set\n","\n","size = 50000\n","freq_cutoff= 2\n","vocab_file = 'en_es_data/vocab.json'\n","\n","vocab = Vocab.build(train_data_src, train_data_tgt, size, freq_cutoff)\n","print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n","\n","vocab.save(vocab_file)\n","print('vocabulary saved to %s' % vocab_file)\n","\n","#\n","print(\"==\"*40)\n","print('Note that the <s> and </s> tokens are added while vocab\\\n","      initialization.\\nThese tokens are also present in target\\\n","      top frequent words. \\nThat is why vocab size for target language is lesser by 2.')\n","print(\"==\"*40)\n","\n","\n","# Check tokenization process\n","print(\"==\"*40)\n","sents = [['I', 'asgjsssd', 'will', 'be', 'there', 'for', 'you.'], ['This', 'is', 'spartaaaaaaaa.']]\n","print(\"Tokenize:\\n {} \\n {}\\n\".format(' '.join(sents[0]), ' '.join(sents[1])))\n","\n","print(vocab.tgt.to_input_tensor(sents, \"cpu\"))\n","#\n","print(\"==\"*40)\n","print(\"Note that 3 and 0  are <unk> and <pad> tokens!\")\n","print(\"==\"*40)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["================================================================================\n","Number of examples in train: 216617\n","Number of examples in valid: 851\n","Number of examples in test: 8064\n","================================================================================\n","Spanish --> English\n","Sp: El ao pasado proyect estas dos diapositivas para demostrar que la capa de hielo rtico, que durante los ltimos tres millones de aos ha sido del tamao de los 48 estados, se ha reducido en un 40 por ciento.\n","En: <s> Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent. </s>\n","================================================================================\n","initialize source vocabulary ..\n","number of word types: 172418, number of word types w/ frequency >= 2: 80623\n","initialize target vocabulary ..\n","number of word types: 128873, number of word types w/ frequency >= 2: 64215\n","generated vocabulary, source 50004 words, target 50002 words\n","vocabulary saved to en_es_data/vocab.json\n","================================================================================\n","Note that the <s> and </s> tokens are added while vocab      initialization.\n","These tokens are also present in target      top frequent words. \n","That is why vocab size for target language is lesser by 2.\n","================================================================================\n","================================================================================\n","Tokenize:\n"," I asgjsssd will be there for you. \n"," This is spartaaaaaaaa.\n","\n","tensor([[ 11,  76],\n","        [  3,  12],\n","        [ 88,   3],\n","        [ 29,   0],\n","        [ 67,   0],\n","        [ 19,   0],\n","        [165,   0]])\n","================================================================================\n","Note that 3 and 0  are <unk> and <pad> tokens!\n","================================================================================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HSrj7xNZS9xs","colab_type":"text"},"source":["### Encoder-decoder model parts"]},{"cell_type":"markdown","metadata":{"id":"IJGpHy18MYUE","colab_type":"text"},"source":["#### Model Embedding"]},{"cell_type":"code","metadata":{"id":"i02Y5boSJBHS","colab_type":"code","colab":{}},"source":["class ModelEmbeddings(nn.Module): \n","    \"\"\"\n","    Class that converts input words to their embeddings.\n","    \"\"\"\n","    def __init__(self, embed_size, vocab):\n","        \"\"\"\n","        Init the Embedding layers.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py in code_utils.\n","        \"\"\"\n","        super(ModelEmbeddings, self).__init__()\n","        self.embed_size = embed_size\n","\n","        # default values\n","        self.source = None\n","        self.target = None\n","\n","        src_pad_token_idx = vocab.src['<pad>']\n","        tgt_pad_token_idx = vocab.tgt['<pad>']\n","\n","        \n","        self.source = nn.Embedding(len(vocab.src), embed_size, padding_idx=src_pad_token_idx)\n","        self.target = nn.Embedding(len(vocab.tgt), embed_size, padding_idx=tgt_pad_token_idx)\n","      \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"br26Jq49Mc-V","colab_type":"text"},"source":["#### Encoder "]},{"cell_type":"code","metadata":{"id":"XFM-KDhdJBJ5","colab_type":"code","colab":{}},"source":["## Encoder Block \n","class Encoder(nn.Module):\n","  \n","  \"\"\" Class that encodes given sentences to finite dimensional vectors \n","  \"\"\"\n","\n","  def __init__(self, embed_size, hidden_size, word_mat, n_layers=1, dropout_rate=0.2):\n","  \n","    \"\"\" Initialize the encoder block \n","    @ param embed_size: embedding dimension\n","    @ param hidden_size: # of hidden units for LSTM\n","    @ word_mat:  ModelEmbeddings object\n","    @ param n_layers: # of LSTM layers\n","    @ dropout_rate: dropout for regularization\n","    \"\"\"  \n","\n","    super(Encoder, self).__init__()\n","    self.embed_size = embed_size\n","    self.hidden_size = hidden_size\n","    self.n_layers = n_layers\n","    self.dropout_rate = dropout_rate\n","    self.embed_mat = word_mat.source # to get embeddings of words from source language\n","\n","    # Layers\n","    self.seq_src = nn.LSTM(embed_size, hidden_size, n_layers)\n","    self.dropout = nn.Dropout(dropout_rate)\n","\n","  def forward(self, source_padded, source_lengths):\n","    \n","    \"\"\" Encodes the source sentence\n","        @param: source_padded ==> Tensor (T,b) #T = max src length, b= batch size \n","        @param: sorce_lengths ==> List (b) : length of source sentences\n","        @return last_hidden ==> Tensor (n_layers*n_directions,b,h) # h = hidden_size\n","        @return last_cell => Tensor (n_layers*n_directions,b,h) # h = hidden_size\n","    \"\"\"\n","    #set_trace()\n","    X = self.dropout(self.embed_mat(source_padded))\n","    X = pack_padded_sequence(X, source_lengths) # for faster computation\n","    _, (last_hidden, last_cell) = self.seq_src(X) \n","\n","    return last_hidden, last_cell\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H1kJWbQvOqjV","colab_type":"text"},"source":["#### Decoder"]},{"cell_type":"code","metadata":{"id":"y0oh2LkbJBMV","colab_type":"code","colab":{}},"source":["# Decoder block\n","class Decoder(nn.Module):\n","  \n","  \"\"\" Class that decodes source sentences\n","  \"\"\"\n","\n","  def __init__(self, embed_size, hidden_size, word_mat, n_layers=1, dropout_rate =0.2):\n","    \n","    super(Decoder, self).__init__()\n","    \n","    self.embed_size = embed_size\n","    self.hidden_size = hidden_size\n","    self.n_layers = n_layers\n","    self.dropout_rate = dropout_rate\n","    self.embed_mat = word_mat.target # embedding layer\n","    self.vocab_size = self.embed_mat.weight.size(0)\n","    \n","    # Layers\n","    self.seq_tgt = nn.LSTM(embed_size, hidden_size, n_layers)\n","    self.dropout = nn.Dropout(dropout_rate)\n","    self.out = nn.Linear(hidden_size, self.vocab_size)\n","\n","  def forward(self, Y_in, dec_state):\n","    \n","    \"\"\" Takes input of time step t and predicts conditional \n","    probability of words at time t+1\n","    \n","    @ param: Y_embed: embedding for input token at time t\n","    @ param: decoder state: hidden unit activations at time t\n","    @ returns updated decoder state and next time step word predictions\n","    \"\"\"\n","    \n","    Y_embed = self.embed_mat(Y_in) #(1,b,embed_size)\n","    d_out, dec_state = self.seq_tgt(Y_embed, dec_state)\n","    d_out = self.dropout(torch.squeeze(d_out,dim=0)) #(b,hidden_size)\n","    dec_pred = self.out(d_out)\n","    \n","    return dec_pred, dec_state"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzKHJFYOOsax","colab_type":"text"},"source":["### Seq2Seq Model: Encoder-Decoder"]},{"cell_type":"code","metadata":{"id":"NVuKaFnwJBOl","colab_type":"code","colab":{}},"source":["class NMT(nn.Module):\n","  def __init__(self, vocab, encoder, decoder):\n","    super(NMT, self).__init__()\n","    \n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.vocab = vocab\n","    \n","    assert encoder.hidden_size == decoder.hidden_size, \\\n","    \"Hidden dimensions of encoder and decoder must be equal!\"\n","    \n","    assert encoder.embed_size == decoder.embed_size, \\\n","    \"Embedding dimensions of encoder and decoder must be equal!\"\n","    \n","    assert encoder.n_layers == decoder.n_layers, \\\n","    \"Encoder and decoder must have equal number of layers!\"\n","    \n","    self.embed_size = encoder.embed_size\n","    self.hidden_size = encoder.hidden_size\n","    self.n_layers = encoder.n_layers\n","    \n","    \n","    # different layers  \n","    #self.model_embeddings = ModelEmbeddings(self.embed_size, vocab)\n","    \n","  def forward(self, source, target):\n","    # Compute sentence lengths\n","    source_lengths = [len(s) for s in source]\n","      \n","    # Convert list of lists into tensors\n","    source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n","    target_padded_raw = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n","    \n","    # Encode source padded --> decoder initial hidden state\n","    dec_hidden, dec_cell = self.encoder(source_padded, source_lengths)\n","    dec_state = (dec_hidden, dec_cell)\n","    \n","    # Chop of the <END> token for max length sentences.\n","    target_padded = target_padded_raw[:-1] #(tgt_len-1,b) ==> Tensor\n","    \n","    # Get target sentence length \n","    tgt_len = target_padded.size(0)\n","    \n","    \n","    # split Y for input to each time step\n","    Y_splited = torch.split(target_padded,1, dim=0)\n","    \n","    # Genearte outputs with decoder\n","    outputs = [] # placeholder\n","    \n","    for i in range(tgt_len):\n","      \n","      Y_t = Y_splited[i] # Tensor (1,b)\n","      \n","      dec_pred, dec_state = self.decoder(Y_t, dec_state) #dec_pred ==> (b, tgt_vocab_len)\n","      \n","      outputs.append(dec_pred) \n","    \n","    # stack  \n","    outputs = torch.stack(outputs) #(tgt_len,b,tgt_vocab_len)\n","    \n","    # apply softmax\n","    P = F.log_softmax(outputs, dim = -1) # P ==> (tgt_len,b,tgt_vocab_len)\n","    \n","    # Zero out, probabilities for which we have nothing in the target text\n","    target_masks = (target_padded_raw != self.vocab.tgt['<pad>']).float()\n","    \n","    # Compute log probability of generating true target words\n","    target_gold_words_log_prob = torch.gather(P, index=target_padded_raw[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n","    scores = target_gold_words_log_prob.sum(dim=0)\n","      \n","    return scores   \n","  \n","  @staticmethod\n","  def load(model_path: str):\n","      \"\"\" Load the model from a file.\n","      @param model_path (str): path to model\n","      \"\"\"\n","      params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","      args = params['args']\n","      model = NMT_base(vocab=params['vocab'], **args)\n","      model.load_state_dict(params['state_dict'])\n","\n","      return model\n","    \n","    \n","  def save(self, path: str):\n","      \"\"\" Save the odel to a file.\n","      @param path (str): path to the model\n","      \"\"\"\n","      print('save model parameters to [%s]' % path, file=sys.stderr)\n","\n","      params = {\n","          'args': dict(embed_size=self.encoder.embed_size, hidden_size=self.hidden_size, n_layers=self.n_layers),\n","          'vocab': self.vocab,\n","          'state_dict': self.state_dict()\n","      }\n","\n","      torch.save(params, path)\n","    \n","  @property\n","  def device(self) -> torch.device:\n","      \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","      \"\"\"\n","      return self.encoder.embed_mat.weight.device\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JNyyiTmUNsO","colab_type":"text"},"source":["### Build Model"]},{"cell_type":"code","metadata":{"id":"bODKv8CqltFB","colab_type":"code","colab":{}},"source":["# Hyperparameters\n","\n","embed_size = 256\n","hidden_size = 512\n","n_layers = 1\n","enc_dropout = 0.2\n","dec_dropout = 0.2\n","\n","# Build model\n","# Create model embedding object\n","word_mat = ModelEmbeddings(embed_size, vocab)\n","# Create Encoder\n","enc = Encoder(embed_size, hidden_size, word_mat, n_layers, enc_dropout)\n","# Create Decoder\n","dec = Decoder(embed_size, hidden_size, word_mat, n_layers, dec_dropout)\n","# Build Seq2Seq Model\n","model = NMT(vocab,enc, dec)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_4zMorIn-No","colab_type":"text"},"source":["#### Initialize parameters and optimizer"]},{"cell_type":"code","metadata":{"id":"fkFw5UNsJBfV","colab_type":"code","outputId":"c80f4d89-73be-489d-8913-a9d35583b21c","executionInfo":{"status":"ok","timestamp":1563861301661,"user_tz":-480,"elapsed":8651,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["uniform_init = 0.1\n","\n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -uniform_init, uniform_init)\n","        \n","model.apply(init_weights)\n","\n","# Count total parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')\n","\n","# Use Adam Optimizaer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# transfer model to cuda if available\n","device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n","print('use device: %s' % device)\n","model = model.to(device)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["The model has 54,406,482 trainable parameters\n","use device: cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ngINUmQtp1C8","colab_type":"text"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"3oo6ly6spkzN","colab_type":"text"},"source":["#### Perplexity (PPL)"]},{"cell_type":"code","metadata":{"id":"ostSvyOPJBpE","colab_type":"code","colab":{}},"source":["## Compute Perplexity to keep track of training\n","\n","def evaluate_ppl(model, dev_data, batch_size=32):\n","    \"\"\" Evaluate perplexity on dev sentences\n","    @param model (NMT): NMT Model\n","    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (batch size)\n","    @returns ppl (perplixty on dev sentences)\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    cum_loss = 0.\n","    cum_tgt_words = 0.\n","\n","    # no_grad() signals backend to throw away all gradients\n","    with torch.no_grad():\n","        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n","            loss = -model(src_sents, tgt_sents).sum()\n","\n","            cum_loss += loss.item()\n","            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","            cum_tgt_words += tgt_word_num_to_predict\n","\n","        ppl = np.exp(cum_loss / cum_tgt_words)\n","\n","    if was_training:\n","        model.train()\n","\n","    return ppl"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KjyIc6ReZwm7","colab_type":"text"},"source":["#### Model training function"]},{"cell_type":"code","metadata":{"id":"_x_P11IZJBty","colab_type":"code","colab":{}},"source":["######## Train Model ########\n","\n","model_save_path = 'NMT_LSTM_seq2seq_one_layer'\n","\n","##\n","def train_model(model, optimizer, clip_grad =5.0, max_epoch =30, max_patience = 3, max_trial = 3, lr_decay = 0.5, train_batch_size = 128, log_every = 100, valid_niter = 1000):\n","  \n","  \n","  print('Training begins...')\n","  ## Temp variables\n","  num_trial = 0\n","  train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","  cum_examples = report_examples  = valid_num = 0\n","  hist_valid_scores = []\n","  train_time = begin_time = time.time()\n","  \n","  # put the model in training mode\n","  model.train()\n","  \n","  \n","  # iterate over the epochs\n","  for epoch in range(max_epoch):\n","    for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n","        \n","        train_iter += 1\n","        optimizer.zero_grad()\n","        batch_size = len(src_sents)\n","        \n","        example_losses = -model(src_sents, tgt_sents)\n","        batch_loss = example_losses.sum()\n","        loss = batch_loss/batch_size\n","        loss.backward() # autograd\n","        \n","        # Clip gradient\n","        grad_norn = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n","        optimizer.step() # update parameters\n","        \n","        batch_losses_val = batch_loss.item()\n","        report_loss += batch_losses_val\n","        cum_loss += batch_losses_val\n","        \n","        tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","        report_tgt_words += tgt_words_num_to_predict\n","        cum_tgt_words += tgt_words_num_to_predict\n","        report_examples += batch_size\n","        cum_examples += batch_size\n","        \n","        # print interim report about training\n","        \n","        if train_iter % log_every == 0:\n","            #set_trace()\n","            print('| Epoch %d, Iter %d| Avg Loss = %.2f| Avg. ppl = %.2f| Speed %.2f words/sec| Time %.2f min|' % (epoch+1, train_iter, report_loss / report_examples, math.exp(report_loss / report_tgt_words),\n","                                                                                     report_tgt_words / (time.time() - train_time), (time.time() - begin_time)/60.0))\n","\n","            train_time = time.time()\n","            report_loss = report_tgt_words = report_examples = 0.\n","        \n","        # validation\n","        if train_iter % valid_niter == 0:\n","            \n","            print('| <Train Summary> | Epoch %d, Iter %d| Cum. loss = %.2f| Cum. ppl = %.2f|' % (epoch+1, train_iter, cum_loss / cum_examples, np.exp(cum_loss / cum_tgt_words)))\n","\n","            cum_loss = cum_examples = cum_tgt_words = 0.\n","            valid_num += 1\n","\n","            print('Report on validation set:', file=sys.stderr)\n","\n","            # compute dev. ppl and bleu\n","            dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n","            valid_metric = -dev_ppl\n","\n","            print('Validation:  Dev. ppl = %f' % (dev_ppl), file=sys.stderr)\n","\n","            \n","            # learning rate scheduling\n","            \n","            is_better = (len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores))\n","            hist_valid_scores.append(valid_metric)\n","\n","            if is_better:\n","                patience = 0\n","                print('Save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n","                model.save(model_save_path)\n","\n","                # also save the optimizers' state\n","                torch.save(optimizer.state_dict(), model_save_path + '.optim')\n","                \n","            elif patience < int(max_patience):\n","                patience += 1\n","                print('Hit patience %d' % patience, file=sys.stderr)\n","\n","                if patience == int(max_patience):\n","                    num_trial += 1\n","                    print('Hit #%d trial' % num_trial, file=sys.stderr)\n","                    \n","                    if num_trial == int(max_trial):\n","                        print('early stop!', file=sys.stderr)\n","                        return\n","\n","                    # decay lr, and restore from previously best checkpoint\n","                    lr = optimizer.param_groups[0]['lr'] * float(lr_decay)\n","                    print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n","\n","                    # load model\n","                    params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n","                    model.load_state_dict(params['state_dict'])\n","                    model = model.to(device)\n","\n","                    print('restore parameters of the optimizers', file=sys.stderr)\n","                    optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n","\n","                    # set new lr\n","                    for param_group in optimizer.param_groups:\n","                        param_group['lr'] = lr\n","\n","                    # reset patience\n","                    patience = 0\n","\n","            if epoch +1 == int(max_epoch):\n","                print('Training stopped <-> Reached maximum number of epochs!', file=sys.stderr)\n","                return"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ykmYWMkBqxez","colab_type":"code","outputId":"c32fe70a-1bfa-4130-86bd-f1a359bb2ddc","executionInfo":{"status":"ok","timestamp":1563873342563,"user_tz":-480,"elapsed":11394588,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# train parameters\n","max_epoch =30\n","train_batch_size = 128\n","\n","# train the model\n","train_model(model, optimizer, max_epoch =max_epoch, train_batch_size = train_batch_size)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Training begins...\n","| Epoch 1, Iter 100| Avg Loss = 125.55| Avg. ppl = 1234.10| Speed 3880.12 words/sec| Time 0.97 min|\n","| Epoch 1, Iter 200| Avg Loss = 110.54| Avg. ppl = 506.61| Speed 3871.41 words/sec| Time 1.95 min|\n","| Epoch 1, Iter 300| Avg Loss = 104.44| Avg. ppl = 362.79| Speed 3818.79 words/sec| Time 2.94 min|\n","| Epoch 1, Iter 400| Avg Loss = 101.38| Avg. ppl = 305.26| Speed 3865.46 words/sec| Time 3.92 min|\n","| Epoch 1, Iter 500| Avg Loss = 97.59| Avg. ppl = 263.03| Speed 3797.05 words/sec| Time 4.90 min|\n","| Epoch 1, Iter 600| Avg Loss = 97.35| Avg. ppl = 237.80| Speed 3837.94 words/sec| Time 5.89 min|\n","| Epoch 1, Iter 700| Avg Loss = 95.62| Avg. ppl = 218.89| Speed 3817.28 words/sec| Time 6.88 min|\n","| Epoch 1, Iter 800| Avg Loss = 92.71| Avg. ppl = 195.04| Speed 3801.30 words/sec| Time 7.87 min|\n","| Epoch 1, Iter 900| Avg Loss = 91.98| Avg. ppl = 180.72| Speed 3830.55 words/sec| Time 8.85 min|\n","| Epoch 1, Iter 1000| Avg Loss = 89.82| Avg. ppl = 167.01| Speed 3805.90 words/sec| Time 9.84 min|\n","| <Train Summary> | Epoch 1, Iter 1000| Cum. loss = 100.70| Cum. ppl = 298.43|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 172.725205\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 1, Iter 1100| Avg Loss = 89.57| Avg. ppl = 156.58| Speed 3344.71 words/sec| Time 10.97 min|\n","| Epoch 1, Iter 1200| Avg Loss = 88.29| Avg. ppl = 147.12| Speed 3789.53 words/sec| Time 11.96 min|\n","| Epoch 1, Iter 1300| Avg Loss = 86.49| Avg. ppl = 137.82| Speed 3776.01 words/sec| Time 12.95 min|\n","| Epoch 1, Iter 1400| Avg Loss = 86.73| Avg. ppl = 132.59| Speed 3821.52 words/sec| Time 13.95 min|\n","| Epoch 1, Iter 1500| Avg Loss = 85.06| Avg. ppl = 123.91| Speed 3783.49 words/sec| Time 14.94 min|\n","| Epoch 1, Iter 1600| Avg Loss = 83.95| Avg. ppl = 119.23| Speed 3786.74 words/sec| Time 15.93 min|\n","| Epoch 2, Iter 1700| Avg Loss = 84.20| Avg. ppl = 113.32| Speed 3817.81 words/sec| Time 16.92 min|\n","| Epoch 2, Iter 1800| Avg Loss = 80.31| Avg. ppl = 92.75| Speed 3802.06 words/sec| Time 17.91 min|\n","| Epoch 2, Iter 1900| Avg Loss = 79.83| Avg. ppl = 90.46| Speed 3778.86 words/sec| Time 18.91 min|\n","| Epoch 2, Iter 2000| Avg Loss = 78.82| Avg. ppl = 87.68| Speed 3794.05 words/sec| Time 19.90 min|\n","| <Train Summary> | Epoch 2, Iter 2000| Cum. loss = 84.33| Cum. ppl = 117.89|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 115.323163\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 2, Iter 2100| Avg Loss = 78.50| Avg. ppl = 85.78| Speed 3509.21 words/sec| Time 20.98 min|\n","| Epoch 2, Iter 2200| Avg Loss = 77.82| Avg. ppl = 84.01| Speed 3776.84 words/sec| Time 21.97 min|\n","| Epoch 2, Iter 2300| Avg Loss = 77.73| Avg. ppl = 81.99| Speed 3779.82 words/sec| Time 22.96 min|\n","| Epoch 2, Iter 2400| Avg Loss = 77.91| Avg. ppl = 80.33| Speed 3806.67 words/sec| Time 23.96 min|\n","| Epoch 2, Iter 2500| Avg Loss = 77.22| Avg. ppl = 79.77| Speed 3793.40 words/sec| Time 24.95 min|\n","| Epoch 2, Iter 2600| Avg Loss = 75.93| Avg. ppl = 77.60| Speed 3790.54 words/sec| Time 25.93 min|\n","| Epoch 2, Iter 2700| Avg Loss = 77.82| Avg. ppl = 76.69| Speed 3908.90 words/sec| Time 26.91 min|\n","| Epoch 2, Iter 2800| Avg Loss = 76.23| Avg. ppl = 74.13| Speed 3786.46 words/sec| Time 27.91 min|\n","| Epoch 2, Iter 2900| Avg Loss = 75.55| Avg. ppl = 73.04| Speed 3775.67 words/sec| Time 28.90 min|\n","| Epoch 2, Iter 3000| Avg Loss = 75.78| Avg. ppl = 71.36| Speed 3792.27 words/sec| Time 29.90 min|\n","| <Train Summary> | Epoch 2, Iter 3000| Cum. loss = 77.05| Cum. ppl = 78.33|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 90.633073\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 2, Iter 3100| Avg Loss = 75.19| Avg. ppl = 69.37| Speed 3567.82 words/sec| Time 30.96 min|\n","| Epoch 2, Iter 3200| Avg Loss = 75.06| Avg. ppl = 69.35| Speed 3793.65 words/sec| Time 31.96 min|\n","| Epoch 2, Iter 3300| Avg Loss = 74.06| Avg. ppl = 67.56| Speed 3753.56 words/sec| Time 32.96 min|\n","| Epoch 3, Iter 3400| Avg Loss = 73.22| Avg. ppl = 63.65| Speed 3768.91 words/sec| Time 33.95 min|\n","| Epoch 3, Iter 3500| Avg Loss = 68.67| Avg. ppl = 49.30| Speed 3793.41 words/sec| Time 34.94 min|\n","| Epoch 3, Iter 3600| Avg Loss = 68.47| Avg. ppl = 48.23| Speed 3762.97 words/sec| Time 35.94 min|\n","| Epoch 3, Iter 3700| Avg Loss = 68.56| Avg. ppl = 48.82| Speed 3778.25 words/sec| Time 36.94 min|\n","| Epoch 3, Iter 3800| Avg Loss = 68.92| Avg. ppl = 48.38| Speed 3830.30 words/sec| Time 37.93 min|\n","| Epoch 3, Iter 3900| Avg Loss = 69.12| Avg. ppl = 49.19| Speed 3798.76 words/sec| Time 38.92 min|\n","| Epoch 3, Iter 4000| Avg Loss = 68.65| Avg. ppl = 48.46| Speed 3752.62 words/sec| Time 39.93 min|\n","| <Train Summary> | Epoch 3, Iter 4000| Cum. loss = 70.99| Cum. ppl = 55.48|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 79.608680\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 3, Iter 4100| Avg Loss = 68.07| Avg. ppl = 48.22| Speed 3565.40 words/sec| Time 40.98 min|\n","| Epoch 3, Iter 4200| Avg Loss = 67.45| Avg. ppl = 47.18| Speed 3778.46 words/sec| Time 41.97 min|\n","| Epoch 3, Iter 4300| Avg Loss = 68.08| Avg. ppl = 47.25| Speed 3802.94 words/sec| Time 42.96 min|\n","| Epoch 3, Iter 4400| Avg Loss = 68.39| Avg. ppl = 47.23| Speed 3798.49 words/sec| Time 43.95 min|\n","| Epoch 3, Iter 4500| Avg Loss = 67.85| Avg. ppl = 47.17| Speed 3793.67 words/sec| Time 44.94 min|\n","| Epoch 3, Iter 4600| Avg Loss = 68.18| Avg. ppl = 47.25| Speed 3803.24 words/sec| Time 45.94 min|\n","| Epoch 3, Iter 4700| Avg Loss = 68.22| Avg. ppl = 46.66| Speed 3800.02 words/sec| Time 46.93 min|\n","| Epoch 3, Iter 4800| Avg Loss = 68.10| Avg. ppl = 47.03| Speed 3776.98 words/sec| Time 47.93 min|\n","| Epoch 3, Iter 4900| Avg Loss = 67.19| Avg. ppl = 45.48| Speed 3776.16 words/sec| Time 48.93 min|\n","| Epoch 3, Iter 5000| Avg Loss = 67.72| Avg. ppl = 45.85| Speed 3802.74 words/sec| Time 49.92 min|\n","| <Train Summary> | Epoch 3, Iter 5000| Cum. loss = 67.93| Cum. ppl = 46.93|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 69.057400\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 4, Iter 5100| Avg Loss = 66.71| Avg. ppl = 42.53| Speed 3587.61 words/sec| Time 50.97 min|\n","| Epoch 4, Iter 5200| Avg Loss = 61.72| Avg. ppl = 32.01| Speed 3818.34 words/sec| Time 51.96 min|\n","| Epoch 4, Iter 5300| Avg Loss = 61.76| Avg. ppl = 32.33| Speed 3793.95 words/sec| Time 52.96 min|\n","| Epoch 4, Iter 5400| Avg Loss = 60.92| Avg. ppl = 31.89| Speed 3786.19 words/sec| Time 53.96 min|\n","| Epoch 4, Iter 5500| Avg Loss = 60.44| Avg. ppl = 31.85| Speed 3769.08 words/sec| Time 54.94 min|\n","| Epoch 4, Iter 5600| Avg Loss = 61.78| Avg. ppl = 32.89| Speed 3775.65 words/sec| Time 55.94 min|\n","| Epoch 4, Iter 5700| Avg Loss = 61.85| Avg. ppl = 33.00| Speed 3807.76 words/sec| Time 56.93 min|\n","| Epoch 4, Iter 5800| Avg Loss = 61.98| Avg. ppl = 32.86| Speed 3779.67 words/sec| Time 57.94 min|\n","| Epoch 4, Iter 5900| Avg Loss = 62.35| Avg. ppl = 33.41| Speed 3783.59 words/sec| Time 58.94 min|\n","| Epoch 4, Iter 6000| Avg Loss = 61.29| Avg. ppl = 33.37| Speed 3750.74 words/sec| Time 59.93 min|\n","| <Train Summary> | Epoch 4, Iter 6000| Cum. loss = 62.08| Cum. ppl = 33.50|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 65.485592\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 4, Iter 6100| Avg Loss = 61.77| Avg. ppl = 33.23| Speed 3519.23 words/sec| Time 61.00 min|\n","| Epoch 4, Iter 6200| Avg Loss = 61.92| Avg. ppl = 33.32| Speed 3808.43 words/sec| Time 61.99 min|\n","| Epoch 4, Iter 6300| Avg Loss = 61.63| Avg. ppl = 33.08| Speed 3801.95 words/sec| Time 62.98 min|\n","| Epoch 4, Iter 6400| Avg Loss = 61.86| Avg. ppl = 33.30| Speed 3799.51 words/sec| Time 63.97 min|\n","| Epoch 4, Iter 6500| Avg Loss = 62.21| Avg. ppl = 33.51| Speed 3774.69 words/sec| Time 64.97 min|\n","| Epoch 4, Iter 6600| Avg Loss = 62.04| Avg. ppl = 33.10| Speed 3770.49 words/sec| Time 65.97 min|\n","| Epoch 4, Iter 6700| Avg Loss = 61.99| Avg. ppl = 33.27| Speed 3774.30 words/sec| Time 66.97 min|\n","| Epoch 5, Iter 6800| Avg Loss = 60.20| Avg. ppl = 30.01| Speed 3740.10 words/sec| Time 67.98 min|\n","| Epoch 5, Iter 6900| Avg Loss = 54.92| Avg. ppl = 22.72| Speed 3732.60 words/sec| Time 68.98 min|\n","| Epoch 5, Iter 7000| Avg Loss = 55.72| Avg. ppl = 23.17| Speed 3813.42 words/sec| Time 69.97 min|\n","| <Train Summary> | Epoch 5, Iter 7000| Cum. loss = 60.43| Cum. ppl = 30.56|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 63.082473\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 5, Iter 7100| Avg Loss = 55.23| Avg. ppl = 23.40| Speed 3503.91 words/sec| Time 71.04 min|\n","| Epoch 5, Iter 7200| Avg Loss = 55.18| Avg. ppl = 23.19| Speed 3758.23 words/sec| Time 72.04 min|\n","| Epoch 5, Iter 7300| Avg Loss = 56.18| Avg. ppl = 24.07| Speed 3815.81 words/sec| Time 73.02 min|\n","| Epoch 5, Iter 7400| Avg Loss = 56.69| Avg. ppl = 24.35| Speed 3801.62 words/sec| Time 74.02 min|\n","| Epoch 5, Iter 7500| Avg Loss = 56.45| Avg. ppl = 24.30| Speed 3809.65 words/sec| Time 75.01 min|\n","| Epoch 5, Iter 7600| Avg Loss = 56.21| Avg. ppl = 24.08| Speed 3788.80 words/sec| Time 76.00 min|\n","| Epoch 5, Iter 7700| Avg Loss = 56.92| Avg. ppl = 24.85| Speed 3773.89 words/sec| Time 77.01 min|\n","| Epoch 5, Iter 7800| Avg Loss = 56.74| Avg. ppl = 24.85| Speed 3761.00 words/sec| Time 78.01 min|\n","| Epoch 5, Iter 7900| Avg Loss = 57.07| Avg. ppl = 25.38| Speed 3760.11 words/sec| Time 79.01 min|\n","| Epoch 5, Iter 8000| Avg Loss = 57.30| Avg. ppl = 25.22| Speed 3756.44 words/sec| Time 80.02 min|\n","| <Train Summary> | Epoch 5, Iter 8000| Cum. loss = 56.40| Cum. ppl = 24.36|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 60.287420\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 5, Iter 8100| Avg Loss = 57.08| Avg. ppl = 25.53| Speed 3540.30 words/sec| Time 81.08 min|\n","| Epoch 5, Iter 8200| Avg Loss = 56.99| Avg. ppl = 25.31| Speed 3779.62 words/sec| Time 82.07 min|\n","| Epoch 5, Iter 8300| Avg Loss = 56.89| Avg. ppl = 25.25| Speed 3795.74 words/sec| Time 83.06 min|\n","| Epoch 5, Iter 8400| Avg Loss = 57.70| Avg. ppl = 25.48| Speed 3811.60 words/sec| Time 84.06 min|\n","| Epoch 6, Iter 8500| Avg Loss = 55.21| Avg. ppl = 22.12| Speed 3827.24 words/sec| Time 85.05 min|\n","| Epoch 6, Iter 8600| Avg Loss = 50.21| Avg. ppl = 17.33| Speed 3808.75 words/sec| Time 86.03 min|\n","| Epoch 6, Iter 8700| Avg Loss = 50.80| Avg. ppl = 17.69| Speed 3765.93 words/sec| Time 87.04 min|\n","| Epoch 6, Iter 8800| Avg Loss = 51.28| Avg. ppl = 18.02| Speed 3784.44 words/sec| Time 88.04 min|\n","| Epoch 6, Iter 8900| Avg Loss = 51.27| Avg. ppl = 17.99| Speed 3771.10 words/sec| Time 89.04 min|\n","| Epoch 6, Iter 9000| Avg Loss = 50.61| Avg. ppl = 18.19| Speed 3749.54 words/sec| Time 90.03 min|\n","| <Train Summary> | Epoch 6, Iter 9000| Cum. loss = 53.80| Cum. ppl = 21.00|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 60.620972\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 6, Iter 9100| Avg Loss = 51.87| Avg. ppl = 18.68| Speed 3669.60 words/sec| Time 91.06 min|\n","| Epoch 6, Iter 9200| Avg Loss = 52.44| Avg. ppl = 19.06| Speed 3803.54 words/sec| Time 92.06 min|\n","| Epoch 6, Iter 9300| Avg Loss = 52.05| Avg. ppl = 19.04| Speed 3796.01 words/sec| Time 93.05 min|\n","| Epoch 6, Iter 9400| Avg Loss = 52.60| Avg. ppl = 19.21| Speed 3836.90 words/sec| Time 94.04 min|\n","| Epoch 6, Iter 9500| Avg Loss = 52.80| Avg. ppl = 19.61| Speed 3819.09 words/sec| Time 95.03 min|\n","| Epoch 6, Iter 9600| Avg Loss = 52.61| Avg. ppl = 19.66| Speed 3791.43 words/sec| Time 96.03 min|\n","| Epoch 6, Iter 9700| Avg Loss = 51.97| Avg. ppl = 19.48| Speed 3777.34 words/sec| Time 97.02 min|\n","| Epoch 6, Iter 9800| Avg Loss = 52.72| Avg. ppl = 20.03| Speed 3778.02 words/sec| Time 98.01 min|\n","| Epoch 6, Iter 9900| Avg Loss = 53.45| Avg. ppl = 20.28| Speed 3765.44 words/sec| Time 99.02 min|\n","| Epoch 6, Iter 10000| Avg Loss = 52.82| Avg. ppl = 20.10| Speed 3788.95 words/sec| Time 100.01 min|\n","| <Train Summary> | Epoch 6, Iter 10000| Cum. loss = 52.53| Cum. ppl = 19.51|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 57.787351\n","Save currently the best model to [NMT_LSTM_seq2seq_one_layer]\n","save model parameters to [NMT_LSTM_seq2seq_one_layer]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 6, Iter 10100| Avg Loss = 53.15| Avg. ppl = 20.25| Speed 3521.81 words/sec| Time 101.08 min|\n","| Epoch 7, Iter 10200| Avg Loss = 50.77| Avg. ppl = 17.34| Speed 3808.45 words/sec| Time 102.07 min|\n","| Epoch 7, Iter 10300| Avg Loss = 45.90| Avg. ppl = 13.65| Speed 3787.57 words/sec| Time 103.06 min|\n","| Epoch 7, Iter 10400| Avg Loss = 46.73| Avg. ppl = 14.13| Speed 3777.20 words/sec| Time 104.05 min|\n","| Epoch 7, Iter 10500| Avg Loss = 46.92| Avg. ppl = 14.31| Speed 3768.31 words/sec| Time 105.05 min|\n","| Epoch 7, Iter 10600| Avg Loss = 47.59| Avg. ppl = 14.70| Speed 3779.62 words/sec| Time 106.05 min|\n","| Epoch 7, Iter 10700| Avg Loss = 47.58| Avg. ppl = 14.86| Speed 3758.23 words/sec| Time 107.05 min|\n","| Epoch 7, Iter 10800| Avg Loss = 48.18| Avg. ppl = 15.12| Speed 3814.19 words/sec| Time 108.04 min|\n","| Epoch 7, Iter 10900| Avg Loss = 48.56| Avg. ppl = 15.32| Speed 3831.17 words/sec| Time 109.03 min|\n","| Epoch 7, Iter 11000| Avg Loss = 48.34| Avg. ppl = 15.35| Speed 3782.04 words/sec| Time 110.03 min|\n","| <Train Summary> | Epoch 7, Iter 11000| Cum. loss = 48.37| Cum. ppl = 15.41|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 60.569942\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 7, Iter 11100| Avg Loss = 48.23| Avg. ppl = 15.65| Speed 3686.66 words/sec| Time 111.05 min|\n","| Epoch 7, Iter 11200| Avg Loss = 48.59| Avg. ppl = 15.80| Speed 3763.13 words/sec| Time 112.04 min|\n","| Epoch 7, Iter 11300| Avg Loss = 49.02| Avg. ppl = 15.79| Speed 3772.84 words/sec| Time 113.05 min|\n","| Epoch 7, Iter 11400| Avg Loss = 49.19| Avg. ppl = 16.23| Speed 3785.01 words/sec| Time 114.04 min|\n","| Epoch 7, Iter 11500| Avg Loss = 49.29| Avg. ppl = 16.21| Speed 3814.51 words/sec| Time 115.03 min|\n","| Epoch 7, Iter 11600| Avg Loss = 49.03| Avg. ppl = 16.27| Speed 3739.11 words/sec| Time 116.04 min|\n","| Epoch 7, Iter 11700| Avg Loss = 49.98| Avg. ppl = 16.55| Speed 3836.27 words/sec| Time 117.03 min|\n","| Epoch 7, Iter 11800| Avg Loss = 49.40| Avg. ppl = 16.52| Speed 3729.89 words/sec| Time 118.03 min|\n","| Epoch 8, Iter 11900| Avg Loss = 45.97| Avg. ppl = 13.64| Speed 3762.92 words/sec| Time 119.02 min|\n","| Epoch 8, Iter 12000| Avg Loss = 42.87| Avg. ppl = 11.31| Speed 3785.45 words/sec| Time 120.02 min|\n","| <Train Summary> | Epoch 8, Iter 12000| Cum. loss = 48.16| Cum. ppl = 15.31|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 61.450189\n","Hit patience 2\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 8, Iter 12100| Avg Loss = 44.44| Avg. ppl = 11.79| Speed 3729.56 words/sec| Time 121.05 min|\n","| Epoch 8, Iter 12200| Avg Loss = 43.90| Avg. ppl = 11.97| Speed 3766.15 words/sec| Time 122.05 min|\n","| Epoch 8, Iter 12300| Avg Loss = 43.55| Avg. ppl = 12.01| Speed 3769.17 words/sec| Time 123.04 min|\n","| Epoch 8, Iter 12400| Avg Loss = 44.24| Avg. ppl = 12.26| Speed 3754.70 words/sec| Time 124.05 min|\n","| Epoch 8, Iter 12500| Avg Loss = 44.55| Avg. ppl = 12.54| Speed 3785.62 words/sec| Time 125.04 min|\n","| Epoch 8, Iter 12600| Avg Loss = 44.72| Avg. ppl = 12.63| Speed 3744.76 words/sec| Time 126.04 min|\n","| Epoch 8, Iter 12700| Avg Loss = 45.11| Avg. ppl = 12.88| Speed 3765.05 words/sec| Time 127.04 min|\n","| Epoch 8, Iter 12800| Avg Loss = 45.76| Avg. ppl = 13.19| Speed 3792.66 words/sec| Time 128.04 min|\n","| Epoch 8, Iter 12900| Avg Loss = 46.31| Avg. ppl = 13.45| Speed 3818.25 words/sec| Time 129.04 min|\n","| Epoch 8, Iter 13000| Avg Loss = 45.96| Avg. ppl = 13.43| Speed 3841.74 words/sec| Time 130.02 min|\n","| <Train Summary> | Epoch 8, Iter 13000| Cum. loss = 44.85| Cum. ppl = 12.60|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 63.497742\n","Hit patience 3\n","Hit #1 trial\n","load previously best model and decay learning rate to 0.000500\n","restore parameters of the optimizers\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 8, Iter 13100| Avg Loss = 45.63| Avg. ppl = 13.46| Speed 3638.64 words/sec| Time 131.05 min|\n","| Epoch 8, Iter 13200| Avg Loss = 45.51| Avg. ppl = 13.36| Speed 3771.21 words/sec| Time 132.04 min|\n","| Epoch 8, Iter 13300| Avg Loss = 46.20| Avg. ppl = 13.53| Speed 3818.44 words/sec| Time 133.03 min|\n","| Epoch 8, Iter 13400| Avg Loss = 45.22| Avg. ppl = 13.24| Speed 3724.21 words/sec| Time 134.04 min|\n","| Epoch 8, Iter 13500| Avg Loss = 46.24| Avg. ppl = 13.69| Speed 3799.08 words/sec| Time 135.03 min|\n","| Epoch 9, Iter 13600| Avg Loss = 45.93| Avg. ppl = 13.32| Speed 3779.04 words/sec| Time 136.02 min|\n","| Epoch 9, Iter 13700| Avg Loss = 44.72| Avg. ppl = 12.62| Speed 3774.29 words/sec| Time 137.02 min|\n","| Epoch 9, Iter 13800| Avg Loss = 45.20| Avg. ppl = 12.67| Speed 3777.70 words/sec| Time 138.03 min|\n","| Epoch 9, Iter 13900| Avg Loss = 45.13| Avg. ppl = 12.82| Speed 3813.21 words/sec| Time 139.02 min|\n","| Epoch 9, Iter 14000| Avg Loss = 45.17| Avg. ppl = 12.94| Speed 3809.76 words/sec| Time 140.00 min|\n","| <Train Summary> | Epoch 9, Iter 14000| Cum. loss = 45.50| Cum. ppl = 13.16|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 60.553490\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 9, Iter 14100| Avg Loss = 45.09| Avg. ppl = 13.16| Speed 3675.48 words/sec| Time 141.02 min|\n","| Epoch 9, Iter 14200| Avg Loss = 45.79| Avg. ppl = 13.20| Speed 3829.65 words/sec| Time 142.01 min|\n","| Epoch 9, Iter 14300| Avg Loss = 45.80| Avg. ppl = 13.20| Speed 3833.06 words/sec| Time 143.00 min|\n","| Epoch 9, Iter 14400| Avg Loss = 45.88| Avg. ppl = 13.44| Speed 3776.05 words/sec| Time 143.99 min|\n","| Epoch 9, Iter 14500| Avg Loss = 45.91| Avg. ppl = 13.49| Speed 3769.79 words/sec| Time 144.99 min|\n","| Epoch 9, Iter 14600| Avg Loss = 46.16| Avg. ppl = 13.48| Speed 3795.70 words/sec| Time 145.99 min|\n","| Epoch 9, Iter 14700| Avg Loss = 45.95| Avg. ppl = 13.65| Speed 3771.23 words/sec| Time 146.98 min|\n","| Epoch 9, Iter 14800| Avg Loss = 46.37| Avg. ppl = 13.77| Speed 3791.44 words/sec| Time 147.98 min|\n","| Epoch 9, Iter 14900| Avg Loss = 46.68| Avg. ppl = 13.93| Speed 3797.17 words/sec| Time 148.97 min|\n","| Epoch 9, Iter 15000| Avg Loss = 45.59| Avg. ppl = 13.66| Speed 3748.64 words/sec| Time 149.97 min|\n","| <Train Summary> | Epoch 9, Iter 15000| Cum. loss = 45.92| Cum. ppl = 13.50|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 58.929521\n","Hit patience 2\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 9, Iter 15100| Avg Loss = 46.71| Avg. ppl = 13.88| Speed 3693.55 words/sec| Time 150.99 min|\n","| Epoch 9, Iter 15200| Avg Loss = 46.80| Avg. ppl = 14.04| Speed 3786.05 words/sec| Time 151.99 min|\n","| Epoch 10, Iter 15300| Avg Loss = 44.39| Avg. ppl = 12.10| Speed 3764.40 words/sec| Time 152.99 min|\n","| Epoch 10, Iter 15400| Avg Loss = 42.42| Avg. ppl = 10.97| Speed 3786.97 words/sec| Time 153.99 min|\n","| Epoch 10, Iter 15500| Avg Loss = 42.65| Avg. ppl = 11.07| Speed 3797.83 words/sec| Time 154.99 min|\n","| Epoch 10, Iter 15600| Avg Loss = 43.12| Avg. ppl = 11.35| Speed 3776.51 words/sec| Time 155.99 min|\n","| Epoch 10, Iter 15700| Avg Loss = 43.14| Avg. ppl = 11.42| Speed 3795.80 words/sec| Time 156.98 min|\n","| Epoch 10, Iter 15800| Avg Loss = 43.27| Avg. ppl = 11.56| Speed 3802.02 words/sec| Time 157.98 min|\n","| Epoch 10, Iter 15900| Avg Loss = 43.24| Avg. ppl = 11.59| Speed 3818.12 words/sec| Time 158.96 min|\n","| Epoch 10, Iter 16000| Avg Loss = 43.08| Avg. ppl = 11.62| Speed 3768.80 words/sec| Time 159.96 min|\n","| <Train Summary> | Epoch 10, Iter 16000| Cum. loss = 43.88| Cum. ppl = 11.92|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 61.331199\n","Hit patience 3\n","Hit #2 trial\n","load previously best model and decay learning rate to 0.000250\n","restore parameters of the optimizers\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 10, Iter 16100| Avg Loss = 46.23| Avg. ppl = 13.56| Speed 3613.82 words/sec| Time 161.00 min|\n","| Epoch 10, Iter 16200| Avg Loss = 46.40| Avg. ppl = 13.38| Speed 3823.79 words/sec| Time 162.00 min|\n","| Epoch 10, Iter 16300| Avg Loss = 45.45| Avg. ppl = 13.18| Speed 3832.26 words/sec| Time 162.98 min|\n","| Epoch 10, Iter 16400| Avg Loss = 45.42| Avg. ppl = 13.27| Speed 3786.55 words/sec| Time 163.97 min|\n","| Epoch 10, Iter 16500| Avg Loss = 45.33| Avg. ppl = 13.12| Speed 3773.90 words/sec| Time 164.97 min|\n","| Epoch 10, Iter 16600| Avg Loss = 44.86| Avg. ppl = 13.04| Speed 3695.79 words/sec| Time 165.98 min|\n","| Epoch 10, Iter 16700| Avg Loss = 45.45| Avg. ppl = 13.17| Speed 3785.41 words/sec| Time 166.97 min|\n","| Epoch 10, Iter 16800| Avg Loss = 45.78| Avg. ppl = 13.30| Speed 3799.03 words/sec| Time 167.96 min|\n","| Epoch 10, Iter 16900| Avg Loss = 45.68| Avg. ppl = 13.37| Speed 3808.72 words/sec| Time 168.95 min|\n","| Epoch 11, Iter 17000| Avg Loss = 44.67| Avg. ppl = 12.52| Speed 3820.42 words/sec| Time 169.93 min|\n","| <Train Summary> | Epoch 11, Iter 17000| Cum. loss = 45.53| Cum. ppl = 13.19|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 59.717359\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 11, Iter 17100| Avg Loss = 44.43| Avg. ppl = 12.25| Speed 3737.12 words/sec| Time 170.94 min|\n","| Epoch 11, Iter 17200| Avg Loss = 44.81| Avg. ppl = 12.39| Speed 3847.83 words/sec| Time 171.93 min|\n","| Epoch 11, Iter 17300| Avg Loss = 44.03| Avg. ppl = 12.22| Speed 3747.72 words/sec| Time 172.93 min|\n","| Epoch 11, Iter 17400| Avg Loss = 44.69| Avg. ppl = 12.43| Speed 3799.12 words/sec| Time 173.93 min|\n","| Epoch 11, Iter 17500| Avg Loss = 44.36| Avg. ppl = 12.40| Speed 3794.83 words/sec| Time 174.92 min|\n","| Epoch 11, Iter 17600| Avg Loss = 44.77| Avg. ppl = 12.56| Speed 3759.54 words/sec| Time 175.92 min|\n","| Epoch 11, Iter 17700| Avg Loss = 44.71| Avg. ppl = 12.50| Speed 3792.94 words/sec| Time 176.92 min|\n","| Epoch 11, Iter 17800| Avg Loss = 44.83| Avg. ppl = 12.54| Speed 3806.05 words/sec| Time 177.91 min|\n","| Epoch 11, Iter 17900| Avg Loss = 44.76| Avg. ppl = 12.54| Speed 3751.90 words/sec| Time 178.92 min|\n","| Epoch 11, Iter 18000| Avg Loss = 44.57| Avg. ppl = 12.58| Speed 3768.41 words/sec| Time 179.91 min|\n","| <Train Summary> | Epoch 11, Iter 18000| Cum. loss = 44.60| Cum. ppl = 12.44|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 59.130569\n","Hit patience 2\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 11, Iter 18100| Avg Loss = 44.53| Avg. ppl = 12.67| Speed 3685.93 words/sec| Time 180.93 min|\n","| Epoch 11, Iter 18200| Avg Loss = 45.55| Avg. ppl = 12.85| Speed 3831.64 words/sec| Time 181.92 min|\n","| Epoch 11, Iter 18300| Avg Loss = 44.75| Avg. ppl = 12.71| Speed 3754.95 words/sec| Time 182.92 min|\n","| Epoch 11, Iter 18400| Avg Loss = 44.67| Avg. ppl = 12.73| Speed 3746.10 words/sec| Time 183.92 min|\n","| Epoch 11, Iter 18500| Avg Loss = 45.02| Avg. ppl = 12.80| Speed 3779.21 words/sec| Time 184.92 min|\n","| Epoch 11, Iter 18600| Avg Loss = 45.00| Avg. ppl = 12.90| Speed 3801.50 words/sec| Time 185.91 min|\n","| Epoch 12, Iter 18700| Avg Loss = 43.26| Avg. ppl = 11.48| Speed 3786.23 words/sec| Time 186.90 min|\n","| Epoch 12, Iter 18800| Avg Loss = 42.50| Avg. ppl = 11.16| Speed 3770.47 words/sec| Time 187.89 min|\n","| Epoch 12, Iter 18900| Avg Loss = 42.38| Avg. ppl = 11.17| Speed 3784.07 words/sec| Time 188.88 min|\n","| Epoch 12, Iter 19000| Avg Loss = 43.19| Avg. ppl = 11.45| Speed 3797.08 words/sec| Time 189.88 min|\n","| <Train Summary> | Epoch 12, Iter 19000| Cum. loss = 44.08| Cum. ppl = 12.17|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 60.063730\n","Hit patience 3\n","Hit #3 trial\n","early stop!\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"7H_8ByGncjml","colab_type":"text"},"source":["### Evaluation"]},{"cell_type":"markdown","metadata":{"id":"tWHTEiynxhCC","colab_type":"text"},"source":["#### Beam Search"]},{"cell_type":"code","metadata":{"id":"sJh7-W9SyZOZ","colab_type":"code","colab":{}},"source":["# Create Hypothesis tuple for beam search\n","Hypothesis = namedtuple('Hypothesis', ['value', 'score'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SGzbD5CVqxjM","colab_type":"code","colab":{}},"source":["def beam_search(model, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n","    \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n","    @param src_sent (List[str]): a single source sentence (words)\n","    @param beam_size (int): beam size\n","    @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n","    @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n","            value: List[str]: the decoded target sentence, represented as a list of words\n","            score: float: the log-likelihood of the target sentence\n","    \"\"\"\n","    src_sents_var = model.vocab.src.to_input_tensor([src_sent], model.device)\n","\n","    dec_init_vec = model.encoder(src_sents_var, [len(src_sent)])\n","    #src_encodings_att_linear = self.att_projection(src_encodings)\n","\n","    h_tm1 = dec_init_vec\n","    #att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n","\n","    eos_id = model.vocab.tgt['</s>']\n","\n","    hypotheses = [['<s>']]\n","    hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=model.device)\n","    completed_hypotheses = []\n","\n","    t = 0\n","    while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n","        t += 1\n","        hyp_num = len(hypotheses)\n","        #set_trace()\n","        y_tm1 = torch.tensor([model.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=model.device)\n","        \n","        #y_tm1 = y_tm1.transpose(0,1)\n","        #set_trace()\n","        #y_t_embed = model.model_embeddings.target(y_tm1)\n","        y_tm1 = torch.unsqueeze(y_tm1,dim=0)\n","        #y_t_embed = torch.unsqueeze(y_t_embed, dim=0)\n","        \n","        w_t, (h_t, cell_t) = model.decoder(y_tm1, dec_init_vec) # w_t --> (1, batch, hidden_size)\n","        \n","        #w_t = w_t.squeeze(dim=0)\n","        \n","        #output_t = model.target_vocab_projection(w_t)\n","        \n","        log_p_t = F.log_softmax(w_t, dim=-1)\n","        #set_trace()\n","\n","        live_hyp_num = beam_size - len(completed_hypotheses)\n","        contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n","        top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n","\n","        prev_hyp_ids = top_cand_hyp_pos / len(model.vocab.tgt)\n","        hyp_word_ids = top_cand_hyp_pos % len(model.vocab.tgt)\n","\n","        new_hypotheses = []\n","        live_hyp_ids = []\n","        new_hyp_scores = []\n","\n","        for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n","            prev_hyp_id = prev_hyp_id.item()\n","            hyp_word_id = hyp_word_id.item()\n","            cand_new_hyp_score = cand_new_hyp_score.item()\n","\n","            hyp_word = model.vocab.tgt.id2word[hyp_word_id]\n","            new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n","            if hyp_word == '</s>':\n","                completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n","                                                       score=cand_new_hyp_score))\n","            else:\n","                new_hypotheses.append(new_hyp_sent)\n","                live_hyp_ids.append(prev_hyp_id)\n","                new_hyp_scores.append(cand_new_hyp_score)\n","\n","        if len(completed_hypotheses) == beam_size:\n","            break\n","        #set_trace()\n","        live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=model.device)\n","        dec_init_vec = (h_t[:,live_hyp_ids], cell_t[:,live_hyp_ids])\n","        #att_tm1 = att_t[live_hyp_ids]\n","        \n","\n","        hypotheses = new_hypotheses\n","        hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=model.device)\n","        #set_trace()\n","\n","    if len(completed_hypotheses) == 0:\n","        completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n","                                               score=hyp_scores[0].item()))\n","\n","    completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n","\n","    return completed_hypotheses  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmlHEwbe6kp8","colab_type":"text"},"source":["### BLEU\n","Evaluate Model BLEU score"]},{"cell_type":"code","metadata":{"id":"P47GEADPqxp5","colab_type":"code","colab":{}},"source":["def test_beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n","    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n","    @param model (NMT): NMT Model\n","    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n","    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n","    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n","    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    hypotheses = []\n","    with torch.no_grad():\n","        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n","            example_hyps = beam_search(model, src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n","\n","            hypotheses.append(example_hyps)\n","\n","    if was_training: model.train(was_training)\n","\n","    return hypotheses\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6MQm-nMqxsE","colab_type":"code","colab":{}},"source":["def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n","    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n","    @param references (List[List[str]]): a list of gold-standard reference target sentences\n","    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n","    @returns bleu_score: corpus-level BLEU score\n","    \"\"\"\n","    if references[0][0] == '<s>':\n","        references = [ref[1:-1] for ref in references]\n","    bleu_score = corpus_bleu([[ref] for ref in references],\n","                             [hyp.value for hyp in hypotheses])\n","    return bleu_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-x9TKQ20qxuc","colab_type":"code","colab":{}},"source":["def decode(model):\n","    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n","    If the target gold-standard sentences are given, the function also computes\n","    corpus-level BLEU score.\n","    @param args (Dict): args from cmd line\n","    \"\"\"\n","\n","    print(\"load test source sentences\", file=sys.stderr)\n","    test_data_src = read_corpus(test_es, source='src')\n","    \n","    \n","    print(\"load test target sentences\", file=sys.stderr)\n","    test_data_tgt = read_corpus(test_en, source='tgt')\n","\n","    #print(\"load trained model\", file=sys.stderr)\n","    #model = NMT_base.load(model_save_path)\n","\n","    #device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n","    if torch.cuda.is_available():\n","        print(\"Transfer to cuda!!\")\n","        model = model.to(torch.device(\"cuda:0\"))\n","\n","    hypotheses = test_beam_search(model, test_data_src,\n","                             beam_size=5,\n","                             max_decoding_time_step=70)\n","\n","\n","    top_hypotheses = [hyps[0] for hyps in hypotheses]\n","    bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n","    print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n","\n","    with open('test_output.txt', 'w') as f:\n","        for src_sent, hyps in zip(test_data_src, hypotheses):\n","            top_hyp = hyps[0]\n","            hyp_sent = ' '.join(top_hyp.value)\n","            f.write(hyp_sent + '\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtOBS_Ub7KfV","colab_type":"code","outputId":"e5fb9ca8-08ac-4b5b-85f9-4f3cc4f3d409","executionInfo":{"status":"ok","timestamp":1563876337608,"user_tz":-480,"elapsed":423497,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["decode(model)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Transfer to cuda!!\n","\rDecoding:   0%|          | 0/8064 [00:00<?, ?it/s]"],"name":"stdout"},{"output_type":"stream","text":["load test source sentences\n","load test target sentences\n"],"name":"stderr"},{"output_type":"stream","text":["Decoding: 100%|██████████| 8064/8064 [07:01<00:00, 19.15it/s]\n"],"name":"stdout"},{"output_type":"stream","text":["Corpus BLEU: 8.029102353804074\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"chG-bGiHaJNr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zo5hrMP4ctbd","colab_type":"text"},"source":["#### Inference"]},{"cell_type":"code","metadata":{"id":"5l4uPLcycvLf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"031e7d25-f33a-42ee-d561-fe222b606954","executionInfo":{"status":"ok","timestamp":1563876345474,"user_tz":-480,"elapsed":703,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/-pyhp3YfJmhI/AAAAAAAAAAI/AAAAAAAAAQk/B_Dez3ha7_M/s64/photo.jpg","userId":"15930704027125169268"}}},"source":["####\n","src_sent =['También', 'tenemos', 'que', 'tener', 'cuidado', 'con', 'el', 'hielo', 'se', 'resbala', 'fácilmente', 'en', 'él.']\n","en_ref = ['We', 'also', 'have', 'to', 'be', 'careful', 'with', 'the', 'ice,','it', 'slides', 'easily', 'on', 'it.']\n","en_hat = beam_search(model,src_sent,5,70)\n","print(\"==\"*40)\n","print(\"Model Translation:\\n\")\n","print('{}'.format(' '.join(en_hat[0].value)))\n","print(\"\\n\")\n","print(\"Human Reference:\\n\")\n","print('{}'.format(' '.join(en_ref)))\n","print(\"==\"*40)"],"execution_count":52,"outputs":[{"output_type":"stream","text":["================================================================================\n","Model Translation:\n","\n","<unk> we have to be able to cross the ice course of <unk> <unk>\n","\n","\n","Human Reference:\n","\n","We also have to be careful with the ice, it slides easily on it.\n","================================================================================\n"],"name":"stdout"}]}]}